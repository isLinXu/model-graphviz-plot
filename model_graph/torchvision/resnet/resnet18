digraph {
	graph [size="59.55,59.55"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	140621284736592 [label="
 (1, 1000)" fillcolor=darkolivegreen1]
	140621284813552 [label=AddmmBackward0]
	140621284813360 -> 140621284813552
	140621798537072 [label="fc.bias
 (1000)" fillcolor=lightblue]
	140621798537072 -> 140621284813360
	140621284813360 [label=AccumulateGrad]
	140621284813408 -> 140621284813552
	140621284813408 [label=ReshapeAliasBackward0]
	140621284814032 -> 140621284813408
	140621284814032 [label=MeanBackward1]
	140621284813024 -> 140621284814032
	140621284813024 [label=ReluBackward0]
	140621284812928 -> 140621284813024
	140621284812928 [label=AddBackward0]
	140621284812736 -> 140621284812928
	140621284812736 [label=NativeBatchNormBackward0]
	140621284812496 -> 140621284812736
	140621284812496 [label=ConvolutionBackward0]
	140621284812112 -> 140621284812496
	140621284812112 [label=ReluBackward0]
	140621284811872 -> 140621284812112
	140621284811872 [label=NativeBatchNormBackward0]
	140621284811680 -> 140621284811872
	140621284811680 [label=ConvolutionBackward0]
	140621284812880 -> 140621284811680
	140621284812880 [label=ReluBackward0]
	140621284811200 -> 140621284812880
	140621284811200 [label=AddBackward0]
	140621284811008 -> 140621284811200
	140621284811008 [label=NativeBatchNormBackward0]
	140621284810816 -> 140621284811008
	140621284810816 [label=ConvolutionBackward0]
	140621284781648 -> 140621284810816
	140621284781648 [label=ReluBackward0]
	140621284781408 -> 140621284781648
	140621284781408 [label=NativeBatchNormBackward0]
	140621284781216 -> 140621284781408
	140621284781216 [label=ConvolutionBackward0]
	140621284780832 -> 140621284781216
	140621284780832 [label=ReluBackward0]
	140621284780688 -> 140621284780832
	140621284780688 [label=AddBackward0]
	140621284780496 -> 140621284780688
	140621284780496 [label=NativeBatchNormBackward0]
	140621284780160 -> 140621284780496
	140621284780160 [label=ConvolutionBackward0]
	140621284779872 -> 140621284780160
	140621284779872 [label=ReluBackward0]
	140621284779632 -> 140621284779872
	140621284779632 [label=NativeBatchNormBackward0]
	140621284779440 -> 140621284779632
	140621284779440 [label=ConvolutionBackward0]
	140621284780544 -> 140621284779440
	140621284780544 [label=ReluBackward0]
	140621284778960 -> 140621284780544
	140621284778960 [label=AddBackward0]
	140621284778768 -> 140621284778960
	140621284778768 [label=NativeBatchNormBackward0]
	140621284778432 -> 140621284778768
	140621284778432 [label=ConvolutionBackward0]
	140621284778144 -> 140621284778432
	140621284778144 [label=ReluBackward0]
	140621284778096 -> 140621284778144
	140621284778096 [label=NativeBatchNormBackward0]
	140621284744880 -> 140621284778096
	140621284744880 [label=ConvolutionBackward0]
	140621284744496 -> 140621284744880
	140621284744496 [label=ReluBackward0]
	140621284744256 -> 140621284744496
	140621284744256 [label=AddBackward0]
	140621284744064 -> 140621284744256
	140621284744064 [label=NativeBatchNormBackward0]
	140621284743824 -> 140621284744064
	140621284743824 [label=ConvolutionBackward0]
	140621284743536 -> 140621284743824
	140621284743536 [label=ReluBackward0]
	140621284743200 -> 140621284743536
	140621284743200 [label=NativeBatchNormBackward0]
	140621284743008 -> 140621284743200
	140621284743008 [label=ConvolutionBackward0]
	140621284744208 -> 140621284743008
	140621284744208 [label=ReluBackward0]
	140621284741904 -> 140621284744208
	140621284741904 [label=AddBackward0]
	140621284741184 -> 140621284741904
	140621284741184 [label=NativeBatchNormBackward0]
	140621284741232 -> 140621284741184
	140621284741232 [label=ConvolutionBackward0]
	140621284747584 -> 140621284741232
	140621284747584 [label=ReluBackward0]
	140621284746096 -> 140621284747584
	140621284746096 [label=NativeBatchNormBackward0]
	140621284745520 -> 140621284746096
	140621284745520 [label=ConvolutionBackward0]
	140621284731968 -> 140621284745520
	140621284731968 [label=ReluBackward0]
	140621284731344 -> 140621284731968
	140621284731344 [label=AddBackward0]
	140621284730768 -> 140621284731344
	140621284730768 [label=NativeBatchNormBackward0]
	140621284729664 -> 140621284730768
	140621284729664 [label=ConvolutionBackward0]
	140621284728896 -> 140621284729664
	140621284728896 [label=ReluBackward0]
	140621284724112 -> 140621284728896
	140621284724112 [label=NativeBatchNormBackward0]
	140621284723680 -> 140621284724112
	140621284723680 [label=ConvolutionBackward0]
	140621284730672 -> 140621284723680
	140621284730672 [label=ReluBackward0]
	140621284722432 -> 140621284730672
	140621284722432 [label=AddBackward0]
	140621284722000 -> 140621284722432
	140621284722000 [label=NativeBatchNormBackward0]
	140621284720944 -> 140621284722000
	140621284720944 [label=ConvolutionBackward0]
	140621798484144 -> 140621284720944
	140621798484144 [label=ReluBackward0]
	140621798484432 -> 140621798484144
	140621798484432 [label=NativeBatchNormBackward0]
	140621798483280 -> 140621798484432
	140621798483280 [label=ConvolutionBackward0]
	140621284721952 -> 140621798483280
	140621284721952 [label=MaxPool2DWithIndicesBackward0]
	140621284613520 -> 140621284721952
	140621284613520 [label=ReluBackward0]
	140621284841888 -> 140621284613520
	140621284841888 [label=NativeBatchNormBackward0]
	140621284841648 -> 140621284841888
	140621284841648 [label=ConvolutionBackward0]
	140621284841264 -> 140621284841648
	140620602026480 [label="conv1.weight
 (64, 3, 7, 7)" fillcolor=lightblue]
	140620602026480 -> 140621284841264
	140621284841264 [label=AccumulateGrad]
	140621284841696 -> 140621284841888
	140620602026560 [label="bn1.weight
 (64)" fillcolor=lightblue]
	140620602026560 -> 140621284841696
	140621284841696 [label=AccumulateGrad]
	140621284841936 -> 140621284841888
	140620602026640 [label="bn1.bias
 (64)" fillcolor=lightblue]
	140620602026640 -> 140621284841936
	140621284841936 [label=AccumulateGrad]
	140621284612992 -> 140621798483280
	140620602027360 [label="layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	140620602027360 -> 140621284612992
	140621284612992 [label=AccumulateGrad]
	140621798484912 -> 140621798484432
	140620602027280 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	140620602027280 -> 140621798484912
	140621798484912 [label=AccumulateGrad]
	140621284613376 -> 140621798484432
	140620602027440 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	140620602027440 -> 140621284613376
	140621284613376 [label=AccumulateGrad]
	140621798484576 -> 140621284720944
	140620602027920 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	140620602027920 -> 140621798484576
	140621798484576 [label=AccumulateGrad]
	140621284721520 -> 140621284722000
	140620602027840 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	140620602027840 -> 140621284721520
	140621284721520 [label=AccumulateGrad]
	140621284721472 -> 140621284722000
	140620602064960 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	140620602064960 -> 140621284721472
	140621284721472 [label=AccumulateGrad]
	140621284721952 -> 140621284722432
	140621284722960 -> 140621284723680
	140620602065360 [label="layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	140620602065360 -> 140621284722960
	140621284722960 [label=AccumulateGrad]
	140621284723728 -> 140621284724112
	140620602065280 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	140620602065280 -> 140621284723728
	140621284723728 [label=AccumulateGrad]
	140621284724640 -> 140621284724112
	140620602065440 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	140620602065440 -> 140621284724640
	140621284724640 [label=AccumulateGrad]
	140621284729328 -> 140621284729664
	140620602065920 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	140620602065920 -> 140621284729328
	140621284729328 [label=AccumulateGrad]
	140621284730288 -> 140621284730768
	140620602065840 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	140620602065840 -> 140621284730288
	140621284730288 [label=AccumulateGrad]
	140621284730240 -> 140621284730768
	140620602066000 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	140620602066000 -> 140621284730240
	140621284730240 [label=AccumulateGrad]
	140621284730672 -> 140621284731344
	140621284732640 -> 140621284745520
	140620602067120 [label="layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	140620602067120 -> 140621284732640
	140621284732640 [label=AccumulateGrad]
	140621284746144 -> 140621284746096
	140620602067040 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	140620602067040 -> 140621284746144
	140621284746144 [label=AccumulateGrad]
	140621284746816 -> 140621284746096
	140620602067200 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	140620602067200 -> 140621284746816
	140621284746816 [label=AccumulateGrad]
	140621284747536 -> 140621284741232
	140620602067680 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	140620602067680 -> 140621284747536
	140621284747536 [label=AccumulateGrad]
	140621284748784 -> 140621284741184
	140620602067600 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	140620602067600 -> 140621284748784
	140621284748784 [label=AccumulateGrad]
	140621284748736 -> 140621284741184
	140620602067760 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	140620602067760 -> 140621284748736
	140621284748736 [label=AccumulateGrad]
	140621284741952 -> 140621284741904
	140621284741952 [label=NativeBatchNormBackward0]
	140621284745568 -> 140621284741952
	140621284745568 [label=ConvolutionBackward0]
	140621284731968 -> 140621284745568
	140621284732016 -> 140621284745568
	140620602066400 [label="layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	140620602066400 -> 140621284732016
	140621284732016 [label=AccumulateGrad]
	140621284748208 -> 140621284741952
	140620602066480 [label="layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	140620602066480 -> 140621284748208
	140621284748208 [label=AccumulateGrad]
	140621284748160 -> 140621284741952
	140620602066560 [label="layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	140620602066560 -> 140621284748160
	140621284748160 [label=AccumulateGrad]
	140621284742624 -> 140621284743008
	140620602068160 [label="layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	140620602068160 -> 140621284742624
	140621284742624 [label=AccumulateGrad]
	140621284743152 -> 140621284743200
	140620602068080 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	140620602068080 -> 140621284743152
	140621284743152 [label=AccumulateGrad]
	140621284743392 -> 140621284743200
	140620602068240 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	140620602068240 -> 140621284743392
	140621284743392 [label=AccumulateGrad]
	140621284743584 -> 140621284743824
	140620602068720 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	140620602068720 -> 140621284743584
	140621284743584 [label=AccumulateGrad]
	140621284743872 -> 140621284744064
	140620602068640 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	140620602068640 -> 140621284743872
	140621284743872 [label=AccumulateGrad]
	140621284744016 -> 140621284744064
	140620602068800 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	140620602068800 -> 140621284744016
	140621284744016 [label=AccumulateGrad]
	140621284744208 -> 140621284744256
	140621284744544 -> 140621284744880
	140621798441984 [label="layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	140621798441984 -> 140621284744544
	140621284744544 [label=AccumulateGrad]
	140621284744928 -> 140621284778096
	140621798441904 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	140621798441904 -> 140621284744928
	140621284744928 [label=AccumulateGrad]
	140621284745120 -> 140621284778096
	140621798442064 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	140621798442064 -> 140621284745120
	140621284745120 [label=AccumulateGrad]
	140621284778192 -> 140621284778432
	140621798442544 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	140621798442544 -> 140621284778192
	140621284778192 [label=AccumulateGrad]
	140621284778576 -> 140621284778768
	140621798442464 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	140621798442464 -> 140621284778576
	140621284778576 [label=AccumulateGrad]
	140621284778624 -> 140621284778768
	140621798442624 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	140621798442624 -> 140621284778624
	140621284778624 [label=AccumulateGrad]
	140621284778816 -> 140621284778960
	140621284778816 [label=NativeBatchNormBackward0]
	140621284746864 -> 140621284778816
	140621284746864 [label=ConvolutionBackward0]
	140621284744496 -> 140621284746864
	140621284744448 -> 140621284746864
	140621798441264 [label="layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	140621798441264 -> 140621284744448
	140621284744448 [label=AccumulateGrad]
	140621284778240 -> 140621284778816
	140621798441344 [label="layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	140621798441344 -> 140621284778240
	140621284778240 [label=AccumulateGrad]
	140621284778384 -> 140621284778816
	140621798441424 [label="layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	140621798441424 -> 140621284778384
	140621284778384 [label=AccumulateGrad]
	140621284779056 -> 140621284779440
	140621798443024 [label="layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	140621798443024 -> 140621284779056
	140621284779056 [label=AccumulateGrad]
	140621284779488 -> 140621284779632
	140621798442944 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	140621798442944 -> 140621284779488
	140621284779488 [label=AccumulateGrad]
	140621284779824 -> 140621284779632
	140621798443104 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	140621798443104 -> 140621284779824
	140621284779824 [label=AccumulateGrad]
	140621284779920 -> 140621284780160
	140621798443584 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	140621798443584 -> 140621284779920
	140621284779920 [label=AccumulateGrad]
	140621284780304 -> 140621284780496
	140621798443504 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	140621798443504 -> 140621284780304
	140621284780304 [label=AccumulateGrad]
	140621284780352 -> 140621284780496
	140621798443664 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	140621798443664 -> 140621284780352
	140621284780352 [label=AccumulateGrad]
	140621284780544 -> 140621284780688
	140621284780976 -> 140621284781216
	140621798444784 [label="layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	140621798444784 -> 140621284780976
	140621284780976 [label=AccumulateGrad]
	140621284781360 -> 140621284781408
	140621798444704 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	140621798444704 -> 140621284781360
	140621284781360 [label=AccumulateGrad]
	140621284781600 -> 140621284781408
	140621798444864 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	140621798444864 -> 140621284781600
	140621284781600 [label=AccumulateGrad]
	140621284781696 -> 140621284810816
	140621798535552 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	140621798535552 -> 140621284781696
	140621284781696 [label=AccumulateGrad]
	140621284810960 -> 140621284811008
	140621798535472 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	140621798535472 -> 140621284810960
	140621284810960 [label=AccumulateGrad]
	140621284782032 -> 140621284811008
	140621798535632 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	140621798535632 -> 140621284782032
	140621284782032 [label=AccumulateGrad]
	140621284811152 -> 140621284811200
	140621284811152 [label=NativeBatchNormBackward0]
	140621284781024 -> 140621284811152
	140621284781024 [label=ConvolutionBackward0]
	140621284780832 -> 140621284781024
	140621284780784 -> 140621284781024
	140621798444064 [label="layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	140621798444064 -> 140621284780784
	140621284780784 [label=AccumulateGrad]
	140621284781840 -> 140621284811152
	140621798444144 [label="layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	140621798444144 -> 140621284781840
	140621284781840 [label=AccumulateGrad]
	140621284781888 -> 140621284811152
	140621798444224 [label="layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	140621798444224 -> 140621284781888
	140621284781888 [label=AccumulateGrad]
	140621284811296 -> 140621284811680
	140621798536032 [label="layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	140621798536032 -> 140621284811296
	140621284811296 [label=AccumulateGrad]
	140621284811824 -> 140621284811872
	140621798535952 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	140621798535952 -> 140621284811824
	140621284811824 [label=AccumulateGrad]
	140621284812064 -> 140621284811872
	140621798536112 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	140621798536112 -> 140621284812064
	140621284812064 [label=AccumulateGrad]
	140621284812160 -> 140621284812496
	140621798536592 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	140621798536592 -> 140621284812160
	140621284812160 [label=AccumulateGrad]
	140621284812544 -> 140621284812736
	140621798536512 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	140621798536512 -> 140621284812544
	140621284812544 [label=AccumulateGrad]
	140621284812688 -> 140621284812736
	140621798536672 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	140621798536672 -> 140621284812688
	140621284812688 [label=AccumulateGrad]
	140621284812880 -> 140621284812928
	140621284814080 -> 140621284813552
	140621284814080 [label=TBackward0]
	140621284812976 -> 140621284814080
	140621798536992 [label="fc.weight
 (1000, 512)" fillcolor=lightblue]
	140621798536992 -> 140621284812976
	140621284812976 [label=AccumulateGrad]
	140621284813552 -> 140621284736592
}
