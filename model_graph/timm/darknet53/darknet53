digraph {
	graph [size="153.29999999999998,153.29999999999998"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	140455160366640 [label="
 (1, 1000)" fillcolor=darkolivegreen1]
	140455160119504 [label=AddmmBackward0]
	140455160122432 -> 140455160119504
	140455367965968 [label="head.fc.bias
 (1000)" fillcolor=lightblue]
	140455367965968 -> 140455160122432
	140455160122432 [label=AccumulateGrad]
	140455160120320 -> 140455160119504
	140455160120320 [label=ReshapeAliasBackward0]
	140455160120512 -> 140455160120320
	140455160120512 [label=MeanBackward1]
	140455160122480 -> 140455160120512
	140455160122480 [label=AddBackward0]
	140455160122240 -> 140455160122480
	140455160122240 [label=LeakyReluBackward1]
	140455160119456 -> 140455160122240
	140455160119456 [label=NativeBatchNormBackward0]
	140455160122864 -> 140455160119456
	140455160122864 [label=ConvolutionBackward0]
	140455160122912 -> 140455160122864
	140455160122912 [label=LeakyReluBackward1]
	140455160119744 -> 140455160122912
	140455160119744 [label=NativeBatchNormBackward0]
	140455160123008 -> 140455160119744
	140455160123008 [label=ConvolutionBackward0]
	140455160119408 -> 140455160123008
	140455160119408 [label=AddBackward0]
	140455160123296 -> 140455160119408
	140455160123296 [label=LeakyReluBackward1]
	140455160123344 -> 140455160123296
	140455160123344 [label=NativeBatchNormBackward0]
	140455160328400 -> 140455160123344
	140455160328400 [label=ConvolutionBackward0]
	140455160328592 -> 140455160328400
	140455160328592 [label=LeakyReluBackward1]
	140455160328736 -> 140455160328592
	140455160328736 [label=NativeBatchNormBackward0]
	140455160328832 -> 140455160328736
	140455160328832 [label=ConvolutionBackward0]
	140455160123248 -> 140455160328832
	140455160123248 [label=AddBackward0]
	140455160329120 -> 140455160123248
	140455160329120 [label=LeakyReluBackward1]
	140455160329264 -> 140455160329120
	140455160329264 [label=NativeBatchNormBackward0]
	140455160329360 -> 140455160329264
	140455160329360 [label=ConvolutionBackward0]
	140455160329552 -> 140455160329360
	140455160329552 [label=LeakyReluBackward1]
	140455160329696 -> 140455160329552
	140455160329696 [label=NativeBatchNormBackward0]
	140455160329792 -> 140455160329696
	140455160329792 [label=ConvolutionBackward0]
	140455160329072 -> 140455160329792
	140455160329072 [label=AddBackward0]
	140455160330080 -> 140455160329072
	140455160330080 [label=LeakyReluBackward1]
	140455160330224 -> 140455160330080
	140455160330224 [label=NativeBatchNormBackward0]
	140455160330320 -> 140455160330224
	140455160330320 [label=ConvolutionBackward0]
	140455160330512 -> 140455160330320
	140455160330512 [label=LeakyReluBackward1]
	140455160330656 -> 140455160330512
	140455160330656 [label=NativeBatchNormBackward0]
	140455160330752 -> 140455160330656
	140455160330752 [label=ConvolutionBackward0]
	140455160330032 -> 140455160330752
	140455160330032 [label=LeakyReluBackward1]
	140455160331040 -> 140455160330032
	140455160331040 [label=NativeBatchNormBackward0]
	140455160331136 -> 140455160331040
	140455160331136 [label=ConvolutionBackward0]
	140455160331328 -> 140455160331136
	140455160331328 [label=AddBackward0]
	140455160331472 -> 140455160331328
	140455160331472 [label=LeakyReluBackward1]
	140455160331616 -> 140455160331472
	140455160331616 [label=NativeBatchNormBackward0]
	140455160331712 -> 140455160331616
	140455160331712 [label=ConvolutionBackward0]
	140455160331904 -> 140455160331712
	140455160331904 [label=LeakyReluBackward1]
	140455160332048 -> 140455160331904
	140455160332048 [label=NativeBatchNormBackward0]
	140455160332144 -> 140455160332048
	140455160332144 [label=ConvolutionBackward0]
	140455160331424 -> 140455160332144
	140455160331424 [label=AddBackward0]
	140455160320208 -> 140455160331424
	140455160320208 [label=LeakyReluBackward1]
	140455160320352 -> 140455160320208
	140455160320352 [label=NativeBatchNormBackward0]
	140455160320448 -> 140455160320352
	140455160320448 [label=ConvolutionBackward0]
	140455160320640 -> 140455160320448
	140455160320640 [label=LeakyReluBackward1]
	140455160320784 -> 140455160320640
	140455160320784 [label=NativeBatchNormBackward0]
	140455160320880 -> 140455160320784
	140455160320880 [label=ConvolutionBackward0]
	140455160320160 -> 140455160320880
	140455160320160 [label=AddBackward0]
	140455160321168 -> 140455160320160
	140455160321168 [label=LeakyReluBackward1]
	140455160321312 -> 140455160321168
	140455160321312 [label=NativeBatchNormBackward0]
	140455160321408 -> 140455160321312
	140455160321408 [label=ConvolutionBackward0]
	140455160321600 -> 140455160321408
	140455160321600 [label=LeakyReluBackward1]
	140455160321744 -> 140455160321600
	140455160321744 [label=NativeBatchNormBackward0]
	140455160321840 -> 140455160321744
	140455160321840 [label=ConvolutionBackward0]
	140455160321120 -> 140455160321840
	140455160321120 [label=AddBackward0]
	140455160322128 -> 140455160321120
	140455160322128 [label=LeakyReluBackward1]
	140455160322272 -> 140455160322128
	140455160322272 [label=NativeBatchNormBackward0]
	140455160322320 -> 140455160322272
	140455160322320 [label=ConvolutionBackward0]
	140455160322608 -> 140455160322320
	140455160322608 [label=LeakyReluBackward1]
	140455160322752 -> 140455160322608
	140455160322752 [label=NativeBatchNormBackward0]
	140455160322800 -> 140455160322752
	140455160322800 [label=ConvolutionBackward0]
	140455160322080 -> 140455160322800
	140455160322080 [label=AddBackward0]
	140455160323184 -> 140455160322080
	140455160323184 [label=LeakyReluBackward1]
	140455160323328 -> 140455160323184
	140455160323328 [label=NativeBatchNormBackward0]
	140455160323376 -> 140455160323328
	140455160323376 [label=ConvolutionBackward0]
	140455160323664 -> 140455160323376
	140455160323664 [label=LeakyReluBackward1]
	140455160323808 -> 140455160323664
	140455160323808 [label=NativeBatchNormBackward0]
	140455160323856 -> 140455160323808
	140455160323856 [label=ConvolutionBackward0]
	140455160323136 -> 140455160323856
	140455160323136 [label=AddBackward0]
	140455160389840 -> 140455160323136
	140455160389840 [label=LeakyReluBackward1]
	140455160389984 -> 140455160389840
	140455160389984 [label=NativeBatchNormBackward0]
	140455160390032 -> 140455160389984
	140455160390032 [label=ConvolutionBackward0]
	140455160390320 -> 140455160390032
	140455160390320 [label=LeakyReluBackward1]
	140455160390464 -> 140455160390320
	140455160390464 [label=NativeBatchNormBackward0]
	140455160390512 -> 140455160390464
	140455160390512 [label=ConvolutionBackward0]
	140455160389792 -> 140455160390512
	140455160389792 [label=AddBackward0]
	140455160390896 -> 140455160389792
	140455160390896 [label=LeakyReluBackward1]
	140455160391040 -> 140455160390896
	140455160391040 [label=NativeBatchNormBackward0]
	140455160391088 -> 140455160391040
	140455160391088 [label=ConvolutionBackward0]
	140455160391376 -> 140455160391088
	140455160391376 [label=LeakyReluBackward1]
	140455160391520 -> 140455160391376
	140455160391520 [label=NativeBatchNormBackward0]
	140455160391568 -> 140455160391520
	140455160391568 [label=ConvolutionBackward0]
	140455160390848 -> 140455160391568
	140455160390848 [label=AddBackward0]
	140455160391952 -> 140455160390848
	140455160391952 [label=LeakyReluBackward1]
	140455160392096 -> 140455160391952
	140455160392096 [label=NativeBatchNormBackward0]
	140455160392144 -> 140455160392096
	140455160392144 [label=ConvolutionBackward0]
	140455160392432 -> 140455160392144
	140455160392432 [label=LeakyReluBackward1]
	140455160392576 -> 140455160392432
	140455160392576 [label=NativeBatchNormBackward0]
	140455160392624 -> 140455160392576
	140455160392624 [label=ConvolutionBackward0]
	140455160391904 -> 140455160392624
	140455160391904 [label=LeakyReluBackward1]
	140455160393008 -> 140455160391904
	140455160393008 [label=NativeBatchNormBackward0]
	140455160393056 -> 140455160393008
	140455160393056 [label=ConvolutionBackward0]
	140455160393344 -> 140455160393056
	140455160393344 [label=AddBackward0]
	140455160393488 -> 140455160393344
	140455160393488 [label=LeakyReluBackward1]
	140455160393632 -> 140455160393488
	140455160393632 [label=NativeBatchNormBackward0]
	140455160393536 -> 140455160393632
	140455160393536 [label=ConvolutionBackward0]
	140455160422704 -> 140455160393536
	140455160422704 [label=LeakyReluBackward1]
	140455160422848 -> 140455160422704
	140455160422848 [label=NativeBatchNormBackward0]
	140455160422896 -> 140455160422848
	140455160422896 [label=ConvolutionBackward0]
	140455160393440 -> 140455160422896
	140455160393440 [label=AddBackward0]
	140455160423280 -> 140455160393440
	140455160423280 [label=LeakyReluBackward1]
	140455160423424 -> 140455160423280
	140455160423424 [label=NativeBatchNormBackward0]
	140455160423472 -> 140455160423424
	140455160423472 [label=ConvolutionBackward0]
	140455160423760 -> 140455160423472
	140455160423760 [label=LeakyReluBackward1]
	140455160423904 -> 140455160423760
	140455160423904 [label=NativeBatchNormBackward0]
	140455160423952 -> 140455160423904
	140455160423952 [label=ConvolutionBackward0]
	140455160423232 -> 140455160423952
	140455160423232 [label=AddBackward0]
	140455160424336 -> 140455160423232
	140455160424336 [label=LeakyReluBackward1]
	140455160424480 -> 140455160424336
	140455160424480 [label=NativeBatchNormBackward0]
	140455160424528 -> 140455160424480
	140455160424528 [label=ConvolutionBackward0]
	140455160424816 -> 140455160424528
	140455160424816 [label=LeakyReluBackward1]
	140455160424960 -> 140455160424816
	140455160424960 [label=NativeBatchNormBackward0]
	140455160425008 -> 140455160424960
	140455160425008 [label=ConvolutionBackward0]
	140455160424288 -> 140455160425008
	140455160424288 [label=AddBackward0]
	140455160425392 -> 140455160424288
	140455160425392 [label=LeakyReluBackward1]
	140455160425536 -> 140455160425392
	140455160425536 [label=NativeBatchNormBackward0]
	140455160425584 -> 140455160425536
	140455160425584 [label=ConvolutionBackward0]
	140455160425872 -> 140455160425584
	140455160425872 [label=LeakyReluBackward1]
	140455160426016 -> 140455160425872
	140455160426016 [label=NativeBatchNormBackward0]
	140455160426064 -> 140455160426016
	140455160426064 [label=ConvolutionBackward0]
	140455160425344 -> 140455160426064
	140455160425344 [label=AddBackward0]
	140455160426448 -> 140455160425344
	140455160426448 [label=LeakyReluBackward1]
	140455160434848 -> 140455160426448
	140455160434848 [label=NativeBatchNormBackward0]
	140455160434896 -> 140455160434848
	140455160434896 [label=ConvolutionBackward0]
	140455160435184 -> 140455160434896
	140455160435184 [label=LeakyReluBackward1]
	140455160435328 -> 140455160435184
	140455160435328 [label=NativeBatchNormBackward0]
	140455160435376 -> 140455160435328
	140455160435376 [label=ConvolutionBackward0]
	140455160426400 -> 140455160435376
	140455160426400 [label=AddBackward0]
	140455160435760 -> 140455160426400
	140455160435760 [label=LeakyReluBackward1]
	140455160435904 -> 140455160435760
	140455160435904 [label=NativeBatchNormBackward0]
	140455160435952 -> 140455160435904
	140455160435952 [label=ConvolutionBackward0]
	140455160436240 -> 140455160435952
	140455160436240 [label=LeakyReluBackward1]
	140455160436384 -> 140455160436240
	140455160436384 [label=NativeBatchNormBackward0]
	140455160436432 -> 140455160436384
	140455160436432 [label=ConvolutionBackward0]
	140455160435712 -> 140455160436432
	140455160435712 [label=AddBackward0]
	140455160436816 -> 140455160435712
	140455160436816 [label=LeakyReluBackward1]
	140455160436960 -> 140455160436816
	140455160436960 [label=NativeBatchNormBackward0]
	140455160437008 -> 140455160436960
	140455160437008 [label=ConvolutionBackward0]
	140455160437296 -> 140455160437008
	140455160437296 [label=LeakyReluBackward1]
	140455160437440 -> 140455160437296
	140455160437440 [label=NativeBatchNormBackward0]
	140455160437488 -> 140455160437440
	140455160437488 [label=ConvolutionBackward0]
	140455160436768 -> 140455160437488
	140455160436768 [label=AddBackward0]
	140455160437872 -> 140455160436768
	140455160437872 [label=LeakyReluBackward1]
	140455160438016 -> 140455160437872
	140455160438016 [label=NativeBatchNormBackward0]
	140455160438064 -> 140455160438016
	140455160438064 [label=ConvolutionBackward0]
	140455160438352 -> 140455160438064
	140455160438352 [label=LeakyReluBackward1]
	140455160438496 -> 140455160438352
	140455160438496 [label=NativeBatchNormBackward0]
	140455160438544 -> 140455160438496
	140455160438544 [label=ConvolutionBackward0]
	140455160437824 -> 140455160438544
	140455160437824 [label=LeakyReluBackward1]
	140455160447184 -> 140455160437824
	140455160447184 [label=NativeBatchNormBackward0]
	140455160447232 -> 140455160447184
	140455160447232 [label=ConvolutionBackward0]
	140455160447520 -> 140455160447232
	140455160447520 [label=AddBackward0]
	140455160447664 -> 140455160447520
	140455160447664 [label=LeakyReluBackward1]
	140455160447808 -> 140455160447664
	140455160447808 [label=NativeBatchNormBackward0]
	140455160447856 -> 140455160447808
	140455160447856 [label=ConvolutionBackward0]
	140455160448144 -> 140455160447856
	140455160448144 [label=LeakyReluBackward1]
	140455160448288 -> 140455160448144
	140455160448288 [label=NativeBatchNormBackward0]
	140455160448336 -> 140455160448288
	140455160448336 [label=ConvolutionBackward0]
	140455160447616 -> 140455160448336
	140455160447616 [label=AddBackward0]
	140455160448720 -> 140455160447616
	140455160448720 [label=LeakyReluBackward1]
	140455160448864 -> 140455160448720
	140455160448864 [label=NativeBatchNormBackward0]
	140455160448912 -> 140455160448864
	140455160448912 [label=ConvolutionBackward0]
	140455160449200 -> 140455160448912
	140455160449200 [label=LeakyReluBackward1]
	140455160449344 -> 140455160449200
	140455160449344 [label=NativeBatchNormBackward0]
	140455160449392 -> 140455160449344
	140455160449392 [label=ConvolutionBackward0]
	140455160448672 -> 140455160449392
	140455160448672 [label=LeakyReluBackward1]
	140455160449776 -> 140455160448672
	140455160449776 [label=NativeBatchNormBackward0]
	140455160449824 -> 140455160449776
	140455160449824 [label=ConvolutionBackward0]
	140455160450112 -> 140455160449824
	140455160450112 [label=AddBackward0]
	140455160450256 -> 140455160450112
	140455160450256 [label=LeakyReluBackward1]
	140455160450400 -> 140455160450256
	140455160450400 [label=NativeBatchNormBackward0]
	140455160450448 -> 140455160450400
	140455160450448 [label=ConvolutionBackward0]
	140455160450736 -> 140455160450448
	140455160450736 [label=LeakyReluBackward1]
	140455160450880 -> 140455160450736
	140455160450880 [label=NativeBatchNormBackward0]
	140455160450928 -> 140455160450880
	140455160450928 [label=ConvolutionBackward0]
	140455160450208 -> 140455160450928
	140455160450208 [label=LeakyReluBackward1]
	140455160459568 -> 140455160450208
	140455160459568 [label=NativeBatchNormBackward0]
	140455160459616 -> 140455160459568
	140455160459616 [label=ConvolutionBackward0]
	140455160459904 -> 140455160459616
	140455160459904 [label=LeakyReluBackward1]
	140455160460048 -> 140455160459904
	140455160460048 [label=NativeBatchNormBackward0]
	140455160460096 -> 140455160460048
	140455160460096 [label=ConvolutionBackward0]
	140455160460384 -> 140455160460096
	140455147259040 [label="stem.conv1.conv.weight
 (32, 3, 3, 3)" fillcolor=lightblue]
	140455147259040 -> 140455160460384
	140455160460384 [label=AccumulateGrad]
	140455160459952 -> 140455160460048
	140455147259120 [label="stem.conv1.bn.weight
 (32)" fillcolor=lightblue]
	140455147259120 -> 140455160459952
	140455160459952 [label=AccumulateGrad]
	140455160460192 -> 140455160460048
	140455147259200 [label="stem.conv1.bn.bias
 (32)" fillcolor=lightblue]
	140455147259200 -> 140455160460192
	140455160460192 [label=AccumulateGrad]
	140455160459856 -> 140455160459616
	140455147260720 [label="stages.0.conv_down.conv.weight
 (64, 32, 3, 3)" fillcolor=lightblue]
	140455147260720 -> 140455160459856
	140455160459856 [label=AccumulateGrad]
	140455160459376 -> 140455160459568
	140455147260800 [label="stages.0.conv_down.bn.weight
 (64)" fillcolor=lightblue]
	140455147260800 -> 140455160459376
	140455160459376 [label=AccumulateGrad]
	140455160459712 -> 140455160459568
	140455147260880 [label="stages.0.conv_down.bn.bias
 (64)" fillcolor=lightblue]
	140455147260880 -> 140455160459712
	140455160459712 [label=AccumulateGrad]
	140455160459472 -> 140455160450928
	140455147261280 [label="stages.0.blocks.0.conv1.conv.weight
 (32, 64, 1, 1)" fillcolor=lightblue]
	140455147261280 -> 140455160459472
	140455160459472 [label=AccumulateGrad]
	140455160450784 -> 140455160450880
	140455147261360 [label="stages.0.blocks.0.conv1.bn.weight
 (32)" fillcolor=lightblue]
	140455147261360 -> 140455160450784
	140455160450784 [label=AccumulateGrad]
	140455160451024 -> 140455160450880
	140455147261440 [label="stages.0.blocks.0.conv1.bn.bias
 (32)" fillcolor=lightblue]
	140455147261440 -> 140455160451024
	140455160451024 [label=AccumulateGrad]
	140455160450688 -> 140455160450448
	140455147348032 [label="stages.0.blocks.0.conv2.conv.weight
 (64, 32, 3, 3)" fillcolor=lightblue]
	140455147348032 -> 140455160450688
	140455160450688 [label=AccumulateGrad]
	140455160450304 -> 140455160450400
	140455147348112 [label="stages.0.blocks.0.conv2.bn.weight
 (64)" fillcolor=lightblue]
	140455147348112 -> 140455160450304
	140455160450304 [label=AccumulateGrad]
	140455160450544 -> 140455160450400
	140455147348192 [label="stages.0.blocks.0.conv2.bn.bias
 (64)" fillcolor=lightblue]
	140455147348192 -> 140455160450544
	140455160450544 [label=AccumulateGrad]
	140455160450208 -> 140455160450112
	140455160450064 -> 140455160449824
	140455147348512 [label="stages.1.conv_down.conv.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	140455147348512 -> 140455160450064
	140455160450064 [label=AccumulateGrad]
	140455160449584 -> 140455160449776
	140455147348592 [label="stages.1.conv_down.bn.weight
 (128)" fillcolor=lightblue]
	140455147348592 -> 140455160449584
	140455160449584 [label=AccumulateGrad]
	140455160449920 -> 140455160449776
	140455147348672 [label="stages.1.conv_down.bn.bias
 (128)" fillcolor=lightblue]
	140455147348672 -> 140455160449920
	140455160449920 [label=AccumulateGrad]
	140455160449680 -> 140455160449392
	140455147349072 [label="stages.1.blocks.0.conv1.conv.weight
 (64, 128, 1, 1)" fillcolor=lightblue]
	140455147349072 -> 140455160449680
	140455160449680 [label=AccumulateGrad]
	140455160449248 -> 140455160449344
	140455147349152 [label="stages.1.blocks.0.conv1.bn.weight
 (64)" fillcolor=lightblue]
	140455147349152 -> 140455160449248
	140455160449248 [label=AccumulateGrad]
	140455160449488 -> 140455160449344
	140455147349232 [label="stages.1.blocks.0.conv1.bn.bias
 (64)" fillcolor=lightblue]
	140455147349232 -> 140455160449488
	140455160449488 [label=AccumulateGrad]
	140455160449152 -> 140455160448912
	140455147349712 [label="stages.1.blocks.0.conv2.conv.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	140455147349712 -> 140455160449152
	140455160449152 [label=AccumulateGrad]
	140455160448768 -> 140455160448864
	140455147349792 [label="stages.1.blocks.0.conv2.bn.weight
 (128)" fillcolor=lightblue]
	140455147349792 -> 140455160448768
	140455160448768 [label=AccumulateGrad]
	140455160449008 -> 140455160448864
	140455147349872 [label="stages.1.blocks.0.conv2.bn.bias
 (128)" fillcolor=lightblue]
	140455147349872 -> 140455160449008
	140455160449008 [label=AccumulateGrad]
	140455160448672 -> 140455160447616
	140455160448624 -> 140455160448336
	140455147350192 [label="stages.1.blocks.1.conv1.conv.weight
 (64, 128, 1, 1)" fillcolor=lightblue]
	140455147350192 -> 140455160448624
	140455160448624 [label=AccumulateGrad]
	140455160448192 -> 140455160448288
	140455147350272 [label="stages.1.blocks.1.conv1.bn.weight
 (64)" fillcolor=lightblue]
	140455147350272 -> 140455160448192
	140455160448192 [label=AccumulateGrad]
	140455160448432 -> 140455160448288
	140455147350352 [label="stages.1.blocks.1.conv1.bn.bias
 (64)" fillcolor=lightblue]
	140455147350352 -> 140455160448432
	140455160448432 [label=AccumulateGrad]
	140455160448096 -> 140455160447856
	140455147350832 [label="stages.1.blocks.1.conv2.conv.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	140455147350832 -> 140455160448096
	140455160448096 [label=AccumulateGrad]
	140455160447712 -> 140455160447808
	140455147350912 [label="stages.1.blocks.1.conv2.bn.weight
 (128)" fillcolor=lightblue]
	140455147350912 -> 140455160447712
	140455160447712 [label=AccumulateGrad]
	140455160447952 -> 140455160447808
	140455147350992 [label="stages.1.blocks.1.conv2.bn.bias
 (128)" fillcolor=lightblue]
	140455147350992 -> 140455160447952
	140455160447952 [label=AccumulateGrad]
	140455160447616 -> 140455160447520
	140455160447472 -> 140455160447232
	140455147351312 [label="stages.2.conv_down.conv.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	140455147351312 -> 140455160447472
	140455160447472 [label=AccumulateGrad]
	140455160447040 -> 140455160447184
	140455147351392 [label="stages.2.conv_down.bn.weight
 (256)" fillcolor=lightblue]
	140455147351392 -> 140455160447040
	140455160447040 [label=AccumulateGrad]
	140455160447328 -> 140455160447184
	140455147351472 [label="stages.2.conv_down.bn.bias
 (256)" fillcolor=lightblue]
	140455147351472 -> 140455160447328
	140455160447328 [label=AccumulateGrad]
	140455160438736 -> 140455160438544
	140455147351872 [label="stages.2.blocks.0.conv1.conv.weight
 (128, 256, 1, 1)" fillcolor=lightblue]
	140455147351872 -> 140455160438736
	140455160438736 [label=AccumulateGrad]
	140455160438400 -> 140455160438496
	140455147351952 [label="stages.2.blocks.0.conv1.bn.weight
 (128)" fillcolor=lightblue]
	140455147351952 -> 140455160438400
	140455160438400 [label=AccumulateGrad]
	140455160438640 -> 140455160438496
	140455147532352 [label="stages.2.blocks.0.conv1.bn.bias
 (128)" fillcolor=lightblue]
	140455147532352 -> 140455160438640
	140455160438640 [label=AccumulateGrad]
	140455160438304 -> 140455160438064
	140455147532832 [label="stages.2.blocks.0.conv2.conv.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	140455147532832 -> 140455160438304
	140455160438304 [label=AccumulateGrad]
	140455160437920 -> 140455160438016
	140455147532912 [label="stages.2.blocks.0.conv2.bn.weight
 (256)" fillcolor=lightblue]
	140455147532912 -> 140455160437920
	140455160437920 [label=AccumulateGrad]
	140455160438160 -> 140455160438016
	140455147532992 [label="stages.2.blocks.0.conv2.bn.bias
 (256)" fillcolor=lightblue]
	140455147532992 -> 140455160438160
	140455160438160 [label=AccumulateGrad]
	140455160437824 -> 140455160436768
	140455160437776 -> 140455160437488
	140455147533312 [label="stages.2.blocks.1.conv1.conv.weight
 (128, 256, 1, 1)" fillcolor=lightblue]
	140455147533312 -> 140455160437776
	140455160437776 [label=AccumulateGrad]
	140455160437344 -> 140455160437440
	140455147533392 [label="stages.2.blocks.1.conv1.bn.weight
 (128)" fillcolor=lightblue]
	140455147533392 -> 140455160437344
	140455160437344 [label=AccumulateGrad]
	140455160437584 -> 140455160437440
	140455147533472 [label="stages.2.blocks.1.conv1.bn.bias
 (128)" fillcolor=lightblue]
	140455147533472 -> 140455160437584
	140455160437584 [label=AccumulateGrad]
	140455160437248 -> 140455160437008
	140455147533952 [label="stages.2.blocks.1.conv2.conv.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	140455147533952 -> 140455160437248
	140455160437248 [label=AccumulateGrad]
	140455160436864 -> 140455160436960
	140455147534032 [label="stages.2.blocks.1.conv2.bn.weight
 (256)" fillcolor=lightblue]
	140455147534032 -> 140455160436864
	140455160436864 [label=AccumulateGrad]
	140455160437104 -> 140455160436960
	140455147534112 [label="stages.2.blocks.1.conv2.bn.bias
 (256)" fillcolor=lightblue]
	140455147534112 -> 140455160437104
	140455160437104 [label=AccumulateGrad]
	140455160436768 -> 140455160435712
	140455160436720 -> 140455160436432
	140455147534432 [label="stages.2.blocks.2.conv1.conv.weight
 (128, 256, 1, 1)" fillcolor=lightblue]
	140455147534432 -> 140455160436720
	140455160436720 [label=AccumulateGrad]
	140455160436288 -> 140455160436384
	140455147534512 [label="stages.2.blocks.2.conv1.bn.weight
 (128)" fillcolor=lightblue]
	140455147534512 -> 140455160436288
	140455160436288 [label=AccumulateGrad]
	140455160436528 -> 140455160436384
	140455147534592 [label="stages.2.blocks.2.conv1.bn.bias
 (128)" fillcolor=lightblue]
	140455147534592 -> 140455160436528
	140455160436528 [label=AccumulateGrad]
	140455160436192 -> 140455160435952
	140455147535072 [label="stages.2.blocks.2.conv2.conv.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	140455147535072 -> 140455160436192
	140455160436192 [label=AccumulateGrad]
	140455160435808 -> 140455160435904
	140455147535152 [label="stages.2.blocks.2.conv2.bn.weight
 (256)" fillcolor=lightblue]
	140455147535152 -> 140455160435808
	140455160435808 [label=AccumulateGrad]
	140455160436048 -> 140455160435904
	140455147535232 [label="stages.2.blocks.2.conv2.bn.bias
 (256)" fillcolor=lightblue]
	140455147535232 -> 140455160436048
	140455160436048 [label=AccumulateGrad]
	140455160435712 -> 140455160426400
	140455160435664 -> 140455160435376
	140455147535552 [label="stages.2.blocks.3.conv1.conv.weight
 (128, 256, 1, 1)" fillcolor=lightblue]
	140455147535552 -> 140455160435664
	140455160435664 [label=AccumulateGrad]
	140455160435232 -> 140455160435328
	140455147535632 [label="stages.2.blocks.3.conv1.bn.weight
 (128)" fillcolor=lightblue]
	140455147535632 -> 140455160435232
	140455160435232 [label=AccumulateGrad]
	140455160435472 -> 140455160435328
	140455147535712 [label="stages.2.blocks.3.conv1.bn.bias
 (128)" fillcolor=lightblue]
	140455147535712 -> 140455160435472
	140455160435472 [label=AccumulateGrad]
	140455160435136 -> 140455160434896
	140455147536192 [label="stages.2.blocks.3.conv2.conv.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	140455147536192 -> 140455160435136
	140455160435136 [label=AccumulateGrad]
	140455160434752 -> 140455160434848
	140455147536272 [label="stages.2.blocks.3.conv2.bn.weight
 (256)" fillcolor=lightblue]
	140455147536272 -> 140455160434752
	140455160434752 [label=AccumulateGrad]
	140455160434992 -> 140455160434848
	140455903322176 [label="stages.2.blocks.3.conv2.bn.bias
 (256)" fillcolor=lightblue]
	140455903322176 -> 140455160434992
	140455160434992 [label=AccumulateGrad]
	140455160426400 -> 140455160425344
	140455160426352 -> 140455160426064
	140455903322496 [label="stages.2.blocks.4.conv1.conv.weight
 (128, 256, 1, 1)" fillcolor=lightblue]
	140455903322496 -> 140455160426352
	140455160426352 [label=AccumulateGrad]
	140455160425920 -> 140455160426016
	140455903322576 [label="stages.2.blocks.4.conv1.bn.weight
 (128)" fillcolor=lightblue]
	140455903322576 -> 140455160425920
	140455160425920 [label=AccumulateGrad]
	140455160426160 -> 140455160426016
	140455903322656 [label="stages.2.blocks.4.conv1.bn.bias
 (128)" fillcolor=lightblue]
	140455903322656 -> 140455160426160
	140455160426160 [label=AccumulateGrad]
	140455160425824 -> 140455160425584
	140455903323136 [label="stages.2.blocks.4.conv2.conv.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	140455903323136 -> 140455160425824
	140455160425824 [label=AccumulateGrad]
	140455160425440 -> 140455160425536
	140455903323216 [label="stages.2.blocks.4.conv2.bn.weight
 (256)" fillcolor=lightblue]
	140455903323216 -> 140455160425440
	140455160425440 [label=AccumulateGrad]
	140455160425680 -> 140455160425536
	140455903323296 [label="stages.2.blocks.4.conv2.bn.bias
 (256)" fillcolor=lightblue]
	140455903323296 -> 140455160425680
	140455160425680 [label=AccumulateGrad]
	140455160425344 -> 140455160424288
	140455160425296 -> 140455160425008
	140455903323616 [label="stages.2.blocks.5.conv1.conv.weight
 (128, 256, 1, 1)" fillcolor=lightblue]
	140455903323616 -> 140455160425296
	140455160425296 [label=AccumulateGrad]
	140455160424864 -> 140455160424960
	140455903323696 [label="stages.2.blocks.5.conv1.bn.weight
 (128)" fillcolor=lightblue]
	140455903323696 -> 140455160424864
	140455160424864 [label=AccumulateGrad]
	140455160425104 -> 140455160424960
	140455903323776 [label="stages.2.blocks.5.conv1.bn.bias
 (128)" fillcolor=lightblue]
	140455903323776 -> 140455160425104
	140455160425104 [label=AccumulateGrad]
	140455160424768 -> 140455160424528
	140455903324256 [label="stages.2.blocks.5.conv2.conv.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	140455903324256 -> 140455160424768
	140455160424768 [label=AccumulateGrad]
	140455160424384 -> 140455160424480
	140455903324336 [label="stages.2.blocks.5.conv2.bn.weight
 (256)" fillcolor=lightblue]
	140455903324336 -> 140455160424384
	140455160424384 [label=AccumulateGrad]
	140455160424624 -> 140455160424480
	140455903324416 [label="stages.2.blocks.5.conv2.bn.bias
 (256)" fillcolor=lightblue]
	140455903324416 -> 140455160424624
	140455160424624 [label=AccumulateGrad]
	140455160424288 -> 140455160423232
	140455160424240 -> 140455160423952
	140455903324736 [label="stages.2.blocks.6.conv1.conv.weight
 (128, 256, 1, 1)" fillcolor=lightblue]
	140455903324736 -> 140455160424240
	140455160424240 [label=AccumulateGrad]
	140455160423808 -> 140455160423904
	140455903324816 [label="stages.2.blocks.6.conv1.bn.weight
 (128)" fillcolor=lightblue]
	140455903324816 -> 140455160423808
	140455160423808 [label=AccumulateGrad]
	140455160424048 -> 140455160423904
	140455903324896 [label="stages.2.blocks.6.conv1.bn.bias
 (128)" fillcolor=lightblue]
	140455903324896 -> 140455160424048
	140455160424048 [label=AccumulateGrad]
	140455160423712 -> 140455160423472
	140455903325376 [label="stages.2.blocks.6.conv2.conv.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	140455903325376 -> 140455160423712
	140455160423712 [label=AccumulateGrad]
	140455160423328 -> 140455160423424
	140455903325456 [label="stages.2.blocks.6.conv2.bn.weight
 (256)" fillcolor=lightblue]
	140455903325456 -> 140455160423328
	140455160423328 [label=AccumulateGrad]
	140455160423568 -> 140455160423424
	140455903325536 [label="stages.2.blocks.6.conv2.bn.bias
 (256)" fillcolor=lightblue]
	140455903325536 -> 140455160423568
	140455160423568 [label=AccumulateGrad]
	140455160423232 -> 140455160393440
	140455160423184 -> 140455160422896
	140455903325856 [label="stages.2.blocks.7.conv1.conv.weight
 (128, 256, 1, 1)" fillcolor=lightblue]
	140455903325856 -> 140455160423184
	140455160423184 [label=AccumulateGrad]
	140455160422752 -> 140455160422848
	140455903325936 [label="stages.2.blocks.7.conv1.bn.weight
 (128)" fillcolor=lightblue]
	140455903325936 -> 140455160422752
	140455160422752 [label=AccumulateGrad]
	140455160422992 -> 140455160422848
	140455903326016 [label="stages.2.blocks.7.conv1.bn.bias
 (128)" fillcolor=lightblue]
	140455903326016 -> 140455160422992
	140455160422992 [label=AccumulateGrad]
	140455160422656 -> 140455160393536
	140455648784768 [label="stages.2.blocks.7.conv2.conv.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	140455648784768 -> 140455160422656
	140455160422656 [label=AccumulateGrad]
	140455160422464 -> 140455160393632
	140455648784848 [label="stages.2.blocks.7.conv2.bn.weight
 (256)" fillcolor=lightblue]
	140455648784848 -> 140455160422464
	140455160422464 [label=AccumulateGrad]
	140455160422512 -> 140455160393632
	140455648784928 [label="stages.2.blocks.7.conv2.bn.bias
 (256)" fillcolor=lightblue]
	140455648784928 -> 140455160422512
	140455160422512 [label=AccumulateGrad]
	140455160393440 -> 140455160393344
	140455160393296 -> 140455160393056
	140455648785248 [label="stages.3.conv_down.conv.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	140455648785248 -> 140455160393296
	140455160393296 [label=AccumulateGrad]
	140455160392816 -> 140455160393008
	140455648785328 [label="stages.3.conv_down.bn.weight
 (512)" fillcolor=lightblue]
	140455648785328 -> 140455160392816
	140455160392816 [label=AccumulateGrad]
	140455160393152 -> 140455160393008
	140455648785408 [label="stages.3.conv_down.bn.bias
 (512)" fillcolor=lightblue]
	140455648785408 -> 140455160393152
	140455160393152 [label=AccumulateGrad]
	140455160392912 -> 140455160392624
	140455648785808 [label="stages.3.blocks.0.conv1.conv.weight
 (256, 512, 1, 1)" fillcolor=lightblue]
	140455648785808 -> 140455160392912
	140455160392912 [label=AccumulateGrad]
	140455160392480 -> 140455160392576
	140455648785888 [label="stages.3.blocks.0.conv1.bn.weight
 (256)" fillcolor=lightblue]
	140455648785888 -> 140455160392480
	140455160392480 [label=AccumulateGrad]
	140455160392720 -> 140455160392576
	140455648785968 [label="stages.3.blocks.0.conv1.bn.bias
 (256)" fillcolor=lightblue]
	140455648785968 -> 140455160392720
	140455160392720 [label=AccumulateGrad]
	140455160392384 -> 140455160392144
	140455648786448 [label="stages.3.blocks.0.conv2.conv.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	140455648786448 -> 140455160392384
	140455160392384 [label=AccumulateGrad]
	140455160392000 -> 140455160392096
	140455648786528 [label="stages.3.blocks.0.conv2.bn.weight
 (512)" fillcolor=lightblue]
	140455648786528 -> 140455160392000
	140455160392000 [label=AccumulateGrad]
	140455160392240 -> 140455160392096
	140455648786608 [label="stages.3.blocks.0.conv2.bn.bias
 (512)" fillcolor=lightblue]
	140455648786608 -> 140455160392240
	140455160392240 [label=AccumulateGrad]
	140455160391904 -> 140455160390848
	140455160391856 -> 140455160391568
	140455648786928 [label="stages.3.blocks.1.conv1.conv.weight
 (256, 512, 1, 1)" fillcolor=lightblue]
	140455648786928 -> 140455160391856
	140455160391856 [label=AccumulateGrad]
	140455160391424 -> 140455160391520
	140455648787008 [label="stages.3.blocks.1.conv1.bn.weight
 (256)" fillcolor=lightblue]
	140455648787008 -> 140455160391424
	140455160391424 [label=AccumulateGrad]
	140455160391664 -> 140455160391520
	140455648787088 [label="stages.3.blocks.1.conv1.bn.bias
 (256)" fillcolor=lightblue]
	140455648787088 -> 140455160391664
	140455160391664 [label=AccumulateGrad]
	140455160391328 -> 140455160391088
	140455648787568 [label="stages.3.blocks.1.conv2.conv.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	140455648787568 -> 140455160391328
	140455160391328 [label=AccumulateGrad]
	140455160390944 -> 140455160391040
	140455648787648 [label="stages.3.blocks.1.conv2.bn.weight
 (512)" fillcolor=lightblue]
	140455648787648 -> 140455160390944
	140455160390944 [label=AccumulateGrad]
	140455160391184 -> 140455160391040
	140455648787728 [label="stages.3.blocks.1.conv2.bn.bias
 (512)" fillcolor=lightblue]
	140455648787728 -> 140455160391184
	140455160391184 [label=AccumulateGrad]
	140455160390848 -> 140455160389792
	140455160390800 -> 140455160390512
	140455648788048 [label="stages.3.blocks.2.conv1.conv.weight
 (256, 512, 1, 1)" fillcolor=lightblue]
	140455648788048 -> 140455160390800
	140455160390800 [label=AccumulateGrad]
	140455160390368 -> 140455160390464
	140455648788128 [label="stages.3.blocks.2.conv1.bn.weight
 (256)" fillcolor=lightblue]
	140455648788128 -> 140455160390368
	140455160390368 [label=AccumulateGrad]
	140455160390608 -> 140455160390464
	140455648788208 [label="stages.3.blocks.2.conv1.bn.bias
 (256)" fillcolor=lightblue]
	140455648788208 -> 140455160390608
	140455160390608 [label=AccumulateGrad]
	140455160390272 -> 140455160390032
	140455648969008 [label="stages.3.blocks.2.conv2.conv.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	140455648969008 -> 140455160390272
	140455160390272 [label=AccumulateGrad]
	140455160389888 -> 140455160389984
	140455648969088 [label="stages.3.blocks.2.conv2.bn.weight
 (512)" fillcolor=lightblue]
	140455648969088 -> 140455160389888
	140455160389888 [label=AccumulateGrad]
	140455160390128 -> 140455160389984
	140455648969168 [label="stages.3.blocks.2.conv2.bn.bias
 (512)" fillcolor=lightblue]
	140455648969168 -> 140455160390128
	140455160390128 [label=AccumulateGrad]
	140455160389792 -> 140455160323136
	140455160324048 -> 140455160323856
	140455648969488 [label="stages.3.blocks.3.conv1.conv.weight
 (256, 512, 1, 1)" fillcolor=lightblue]
	140455648969488 -> 140455160324048
	140455160324048 [label=AccumulateGrad]
	140455160323712 -> 140455160323808
	140455648969568 [label="stages.3.blocks.3.conv1.bn.weight
 (256)" fillcolor=lightblue]
	140455648969568 -> 140455160323712
	140455160323712 [label=AccumulateGrad]
	140455160323952 -> 140455160323808
	140455648969648 [label="stages.3.blocks.3.conv1.bn.bias
 (256)" fillcolor=lightblue]
	140455648969648 -> 140455160323952
	140455160323952 [label=AccumulateGrad]
	140455160323616 -> 140455160323376
	140455648970128 [label="stages.3.blocks.3.conv2.conv.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	140455648970128 -> 140455160323616
	140455160323616 [label=AccumulateGrad]
	140455160323232 -> 140455160323328
	140455648970208 [label="stages.3.blocks.3.conv2.bn.weight
 (512)" fillcolor=lightblue]
	140455648970208 -> 140455160323232
	140455160323232 [label=AccumulateGrad]
	140455160323472 -> 140455160323328
	140455648970288 [label="stages.3.blocks.3.conv2.bn.bias
 (512)" fillcolor=lightblue]
	140455648970288 -> 140455160323472
	140455160323472 [label=AccumulateGrad]
	140455160323136 -> 140455160322080
	140455160323088 -> 140455160322800
	140455648970608 [label="stages.3.blocks.4.conv1.conv.weight
 (256, 512, 1, 1)" fillcolor=lightblue]
	140455648970608 -> 140455160323088
	140455160323088 [label=AccumulateGrad]
	140455160322656 -> 140455160322752
	140455648970688 [label="stages.3.blocks.4.conv1.bn.weight
 (256)" fillcolor=lightblue]
	140455648970688 -> 140455160322656
	140455160322656 [label=AccumulateGrad]
	140455160322896 -> 140455160322752
	140455648970768 [label="stages.3.blocks.4.conv1.bn.bias
 (256)" fillcolor=lightblue]
	140455648970768 -> 140455160322896
	140455160322896 [label=AccumulateGrad]
	140455160322560 -> 140455160322320
	140455648971248 [label="stages.3.blocks.4.conv2.conv.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	140455648971248 -> 140455160322560
	140455160322560 [label=AccumulateGrad]
	140455160322176 -> 140455160322272
	140455648971328 [label="stages.3.blocks.4.conv2.bn.weight
 (512)" fillcolor=lightblue]
	140455648971328 -> 140455160322176
	140455160322176 [label=AccumulateGrad]
	140455160322416 -> 140455160322272
	140455648971408 [label="stages.3.blocks.4.conv2.bn.bias
 (512)" fillcolor=lightblue]
	140455648971408 -> 140455160322416
	140455160322416 [label=AccumulateGrad]
	140455160322080 -> 140455160321120
	140455160322032 -> 140455160321840
	140455648971728 [label="stages.3.blocks.5.conv1.conv.weight
 (256, 512, 1, 1)" fillcolor=lightblue]
	140455648971728 -> 140455160322032
	140455160322032 [label=AccumulateGrad]
	140455160321792 -> 140455160321744
	140455648971808 [label="stages.3.blocks.5.conv1.bn.weight
 (256)" fillcolor=lightblue]
	140455648971808 -> 140455160321792
	140455160321792 [label=AccumulateGrad]
	140455160321648 -> 140455160321744
	140455648971888 [label="stages.3.blocks.5.conv1.bn.bias
 (256)" fillcolor=lightblue]
	140455648971888 -> 140455160321648
	140455160321648 [label=AccumulateGrad]
	140455160321552 -> 140455160321408
	140455648972368 [label="stages.3.blocks.5.conv2.conv.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	140455648972368 -> 140455160321552
	140455160321552 [label=AccumulateGrad]
	140455160321360 -> 140455160321312
	140455648972448 [label="stages.3.blocks.5.conv2.bn.weight
 (512)" fillcolor=lightblue]
	140455648972448 -> 140455160321360
	140455160321360 [label=AccumulateGrad]
	140455160321216 -> 140455160321312
	140455648972528 [label="stages.3.blocks.5.conv2.bn.bias
 (512)" fillcolor=lightblue]
	140455648972528 -> 140455160321216
	140455160321216 [label=AccumulateGrad]
	140455160321120 -> 140455160320160
	140455160321072 -> 140455160320880
	140455661592720 [label="stages.3.blocks.6.conv1.conv.weight
 (256, 512, 1, 1)" fillcolor=lightblue]
	140455661592720 -> 140455160321072
	140455160321072 [label=AccumulateGrad]
	140455160320832 -> 140455160320784
	140455661592800 [label="stages.3.blocks.6.conv1.bn.weight
 (256)" fillcolor=lightblue]
	140455661592800 -> 140455160320832
	140455160320832 [label=AccumulateGrad]
	140455160320688 -> 140455160320784
	140455661592880 [label="stages.3.blocks.6.conv1.bn.bias
 (256)" fillcolor=lightblue]
	140455661592880 -> 140455160320688
	140455160320688 [label=AccumulateGrad]
	140455160320592 -> 140455160320448
	140455661593360 [label="stages.3.blocks.6.conv2.conv.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	140455661593360 -> 140455160320592
	140455160320592 [label=AccumulateGrad]
	140455160320400 -> 140455160320352
	140455661593440 [label="stages.3.blocks.6.conv2.bn.weight
 (512)" fillcolor=lightblue]
	140455661593440 -> 140455160320400
	140455160320400 [label=AccumulateGrad]
	140455160320256 -> 140455160320352
	140455661593520 [label="stages.3.blocks.6.conv2.bn.bias
 (512)" fillcolor=lightblue]
	140455661593520 -> 140455160320256
	140455160320256 [label=AccumulateGrad]
	140455160320160 -> 140455160331424
	140455160332240 -> 140455160332144
	140455661593840 [label="stages.3.blocks.7.conv1.conv.weight
 (256, 512, 1, 1)" fillcolor=lightblue]
	140455661593840 -> 140455160332240
	140455160332240 [label=AccumulateGrad]
	140455160332096 -> 140455160332048
	140455661593920 [label="stages.3.blocks.7.conv1.bn.weight
 (256)" fillcolor=lightblue]
	140455661593920 -> 140455160332096
	140455160332096 [label=AccumulateGrad]
	140455160331952 -> 140455160332048
	140455661594000 [label="stages.3.blocks.7.conv1.bn.bias
 (256)" fillcolor=lightblue]
	140455661594000 -> 140455160331952
	140455160331952 [label=AccumulateGrad]
	140455160331856 -> 140455160331712
	140455661594480 [label="stages.3.blocks.7.conv2.conv.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	140455661594480 -> 140455160331856
	140455160331856 [label=AccumulateGrad]
	140455160331664 -> 140455160331616
	140455661594560 [label="stages.3.blocks.7.conv2.bn.weight
 (512)" fillcolor=lightblue]
	140455661594560 -> 140455160331664
	140455160331664 [label=AccumulateGrad]
	140455160331520 -> 140455160331616
	140455661594640 [label="stages.3.blocks.7.conv2.bn.bias
 (512)" fillcolor=lightblue]
	140455661594640 -> 140455160331520
	140455160331520 [label=AccumulateGrad]
	140455160331424 -> 140455160331328
	140455160331280 -> 140455160331136
	140455661594960 [label="stages.4.conv_down.conv.weight
 (1024, 512, 3, 3)" fillcolor=lightblue]
	140455661594960 -> 140455160331280
	140455160331280 [label=AccumulateGrad]
	140455160331088 -> 140455160331040
	140455661595040 [label="stages.4.conv_down.bn.weight
 (1024)" fillcolor=lightblue]
	140455661595040 -> 140455160331088
	140455160331088 [label=AccumulateGrad]
	140455160330848 -> 140455160331040
	140455661595120 [label="stages.4.conv_down.bn.bias
 (1024)" fillcolor=lightblue]
	140455661595120 -> 140455160330848
	140455160330848 [label=AccumulateGrad]
	140455160330944 -> 140455160330752
	140455661595520 [label="stages.4.blocks.0.conv1.conv.weight
 (512, 1024, 1, 1)" fillcolor=lightblue]
	140455661595520 -> 140455160330944
	140455160330944 [label=AccumulateGrad]
	140455160330704 -> 140455160330656
	140455661595600 [label="stages.4.blocks.0.conv1.bn.weight
 (512)" fillcolor=lightblue]
	140455661595600 -> 140455160330704
	140455160330704 [label=AccumulateGrad]
	140455160330560 -> 140455160330656
	140455661595680 [label="stages.4.blocks.0.conv1.bn.bias
 (512)" fillcolor=lightblue]
	140455661595680 -> 140455160330560
	140455160330560 [label=AccumulateGrad]
	140455160330464 -> 140455160330320
	140455661596160 [label="stages.4.blocks.0.conv2.conv.weight
 (1024, 512, 3, 3)" fillcolor=lightblue]
	140455661596160 -> 140455160330464
	140455160330464 [label=AccumulateGrad]
	140455160330272 -> 140455160330224
	140455661596240 [label="stages.4.blocks.0.conv2.bn.weight
 (1024)" fillcolor=lightblue]
	140455661596240 -> 140455160330272
	140455160330272 [label=AccumulateGrad]
	140455160330128 -> 140455160330224
	140455661596320 [label="stages.4.blocks.0.conv2.bn.bias
 (1024)" fillcolor=lightblue]
	140455661596320 -> 140455160330128
	140455160330128 [label=AccumulateGrad]
	140455160330032 -> 140455160329072
	140455160329984 -> 140455160329792
	140455367962688 [label="stages.4.blocks.1.conv1.conv.weight
 (512, 1024, 1, 1)" fillcolor=lightblue]
	140455367962688 -> 140455160329984
	140455160329984 [label=AccumulateGrad]
	140455160329744 -> 140455160329696
	140455367962768 [label="stages.4.blocks.1.conv1.bn.weight
 (512)" fillcolor=lightblue]
	140455367962768 -> 140455160329744
	140455160329744 [label=AccumulateGrad]
	140455160329600 -> 140455160329696
	140455367962848 [label="stages.4.blocks.1.conv1.bn.bias
 (512)" fillcolor=lightblue]
	140455367962848 -> 140455160329600
	140455160329600 [label=AccumulateGrad]
	140455160329504 -> 140455160329360
	140455367963328 [label="stages.4.blocks.1.conv2.conv.weight
 (1024, 512, 3, 3)" fillcolor=lightblue]
	140455367963328 -> 140455160329504
	140455160329504 [label=AccumulateGrad]
	140455160329312 -> 140455160329264
	140455367963408 [label="stages.4.blocks.1.conv2.bn.weight
 (1024)" fillcolor=lightblue]
	140455367963408 -> 140455160329312
	140455160329312 [label=AccumulateGrad]
	140455160329168 -> 140455160329264
	140455367963488 [label="stages.4.blocks.1.conv2.bn.bias
 (1024)" fillcolor=lightblue]
	140455367963488 -> 140455160329168
	140455160329168 [label=AccumulateGrad]
	140455160329072 -> 140455160123248
	140455160329024 -> 140455160328832
	140455367963808 [label="stages.4.blocks.2.conv1.conv.weight
 (512, 1024, 1, 1)" fillcolor=lightblue]
	140455367963808 -> 140455160329024
	140455160329024 [label=AccumulateGrad]
	140455160328784 -> 140455160328736
	140455367963888 [label="stages.4.blocks.2.conv1.bn.weight
 (512)" fillcolor=lightblue]
	140455367963888 -> 140455160328784
	140455160328784 [label=AccumulateGrad]
	140455160328640 -> 140455160328736
	140455367963968 [label="stages.4.blocks.2.conv1.bn.bias
 (512)" fillcolor=lightblue]
	140455367963968 -> 140455160328640
	140455160328640 [label=AccumulateGrad]
	140455160328544 -> 140455160328400
	140455367964448 [label="stages.4.blocks.2.conv2.conv.weight
 (1024, 512, 3, 3)" fillcolor=lightblue]
	140455367964448 -> 140455160328544
	140455160328544 [label=AccumulateGrad]
	140455160328352 -> 140455160123344
	140455367964528 [label="stages.4.blocks.2.conv2.bn.weight
 (1024)" fillcolor=lightblue]
	140455367964528 -> 140455160328352
	140455160328352 [label=AccumulateGrad]
	140455160328256 -> 140455160123344
	140455367964608 [label="stages.4.blocks.2.conv2.bn.bias
 (1024)" fillcolor=lightblue]
	140455367964608 -> 140455160328256
	140455160328256 [label=AccumulateGrad]
	140455160123248 -> 140455160119408
	140455160123200 -> 140455160123008
	140455367964928 [label="stages.4.blocks.3.conv1.conv.weight
 (512, 1024, 1, 1)" fillcolor=lightblue]
	140455367964928 -> 140455160123200
	140455160123200 [label=AccumulateGrad]
	140455160122960 -> 140455160119744
	140455367965008 [label="stages.4.blocks.3.conv1.bn.weight
 (512)" fillcolor=lightblue]
	140455367965008 -> 140455160122960
	140455160122960 [label=AccumulateGrad]
	140455160122576 -> 140455160119744
	140455367965088 [label="stages.4.blocks.3.conv1.bn.bias
 (512)" fillcolor=lightblue]
	140455367965088 -> 140455160122576
	140455160122576 [label=AccumulateGrad]
	140455160120464 -> 140455160122864
	140455367965568 [label="stages.4.blocks.3.conv2.conv.weight
 (1024, 512, 3, 3)" fillcolor=lightblue]
	140455367965568 -> 140455160120464
	140455160120464 [label=AccumulateGrad]
	140455160121856 -> 140455160119456
	140455367965648 [label="stages.4.blocks.3.conv2.bn.weight
 (1024)" fillcolor=lightblue]
	140455367965648 -> 140455160121856
	140455160121856 [label=AccumulateGrad]
	140455160122768 -> 140455160119456
	140455367965728 [label="stages.4.blocks.3.conv2.bn.bias
 (1024)" fillcolor=lightblue]
	140455367965728 -> 140455160122768
	140455160122768 [label=AccumulateGrad]
	140455160119408 -> 140455160122480
	140455160120560 -> 140455160119504
	140455160120560 [label=TBackward0]
	140455160122528 -> 140455160120560
	140455367965328 [label="head.fc.weight
 (1000, 1024)" fillcolor=lightblue]
	140455367965328 -> 140455160122528
	140455160122528 [label=AccumulateGrad]
	140455160119504 -> 140455160366640
}
