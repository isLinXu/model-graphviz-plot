digraph {
	graph [size="276.45,276.45"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	140492159210096 [label="
 (1, 1000)" fillcolor=darkolivegreen1]
	140492154507760 [label=AddmmBackward0]
	140492154510304 -> 140492154507760
	140492154450224 [label="head.fc.bias
 (1000)" fillcolor=lightblue]
	140492154450224 -> 140492154510304
	140492154510304 [label=AccumulateGrad]
	140492154507808 -> 140492154507760
	140492154507808 [label=ReshapeAliasBackward0]
	140492154508624 -> 140492154507808
	140492154508624 [label=PermuteBackward0]
	140492154510208 -> 140492154508624
	140492154510208 [label=NativeLayerNormBackward0]
	140492154510400 -> 140492154510208
	140492154510400 [label=PermuteBackward0]
	140492154510592 -> 140492154510400
	140492154510592 [label=AsStridedBackward1]
	140492154510688 -> 140492154510592
	140492154510688 [label=MeanBackward1]
	140492154510784 -> 140492154510688
	140492154510784 [label=AddBackward0]
	140492154510880 -> 140492154510784
	140492154510880 [label=AddBackward0]
	140492154511024 -> 140492154510880
	140492154511024 [label=AddBackward0]
	140492154511168 -> 140492154511024
	140492154511168 [label=ConvolutionBackward0]
	140492154511312 -> 140492154511168
	140492154511312 [label=PermuteBackward0]
	140492159275216 -> 140492154511312
	140492159275216 [label=NativeLayerNormBackward0]
	140492159275312 -> 140492159275216
	140492159275312 [label=PermuteBackward0]
	140492159275504 -> 140492159275312
	140492159275504 [label=AddBackward0]
	140492159275600 -> 140492159275504
	140492159275600 [label=AddBackward0]
	140492159275744 -> 140492159275600
	140492159275744 [label=AddBackward0]
	140492159275888 -> 140492159275744
	140492159275888 [label=AddBackward0]
	140492159276032 -> 140492159275888
	140492159276032 [label=AddBackward0]
	140492159276176 -> 140492159276032
	140492159276176 [label=AddBackward0]
	140492159276320 -> 140492159276176
	140492159276320 [label=AddBackward0]
	140492159276464 -> 140492159276320
	140492159276464 [label=AddBackward0]
	140492159276608 -> 140492159276464
	140492159276608 [label=AddBackward0]
	140492159276752 -> 140492159276608
	140492159276752 [label=ConvolutionBackward0]
	140492159276896 -> 140492159276752
	140492159276896 [label=PermuteBackward0]
	140492159277088 -> 140492159276896
	140492159277088 [label=NativeLayerNormBackward0]
	140492159277184 -> 140492159277088
	140492159277184 [label=PermuteBackward0]
	140492159277376 -> 140492159277184
	140492159277376 [label=AddBackward0]
	140492159277472 -> 140492159277376
	140492159277472 [label=AddBackward0]
	140492159277616 -> 140492159277472
	140492159277616 [label=AddBackward0]
	140492159277760 -> 140492159277616
	140492159277760 [label=ConvolutionBackward0]
	140492159277904 -> 140492159277760
	140492159277904 [label=PermuteBackward0]
	140492159278096 -> 140492159277904
	140492159278096 [label=NativeLayerNormBackward0]
	140492159278192 -> 140492159278096
	140492159278192 [label=PermuteBackward0]
	140492159278384 -> 140492159278192
	140492159278384 [label=AddBackward0]
	140492159278480 -> 140492159278384
	140492159278480 [label=AddBackward0]
	140492159278624 -> 140492159278480
	140492159278624 [label=AddBackward0]
	140492159278768 -> 140492159278624
	140492159278768 [label=PermuteBackward0]
	140492159278912 -> 140492159278768
	140492159278912 [label=NativeLayerNormBackward0]
	140492159278960 -> 140492159278912
	140492159278960 [label=PermuteBackward0]
	140492159283408 -> 140492159278960
	140492159283408 [label=ConvolutionBackward0]
	140492159283456 -> 140492159283408
	140492151305056 [label="stem.0.weight
 (48, 3, 4, 4)" fillcolor=lightblue]
	140492151305056 -> 140492159283456
	140492159283456 [label=AccumulateGrad]
	140492159283312 -> 140492159283408
	140492151305136 [label="stem.0.bias
 (48)" fillcolor=lightblue]
	140492151305136 -> 140492159283312
	140492159283312 [label=AccumulateGrad]
	140492159278816 -> 140492159278912
	140492945659360 [label="stem.1.weight
 (48)" fillcolor=lightblue]
	140492945659360 -> 140492159278816
	140492159278816 [label=AccumulateGrad]
	140492159279056 -> 140492159278912
	140493217616896 [label="stem.1.bias
 (48)" fillcolor=lightblue]
	140493217616896 -> 140492159279056
	140492159279056 [label=AccumulateGrad]
	140492159278720 -> 140492159278624
	140492159278720 [label=PermuteBackward0]
	140492159278864 -> 140492159278720
	140492159278864 [label=MulBackward0]
	140492159283744 -> 140492159278864
	140492433381232 [label="stages.0.blocks.0.gamma
 (48)" fillcolor=lightblue]
	140492433381232 -> 140492159283744
	140492159283744 [label=AccumulateGrad]
	140492159283552 -> 140492159278864
	140492159283552 [label=AddBackward0]
	140492159283792 -> 140492159283552
	140492159283792 [label=UnsafeViewBackward0]
	140492159284032 -> 140492159283792
	140492159284032 [label=MmBackward0]
	140492159284128 -> 140492159284032
	140492159284128 [label=ReshapeAliasBackward0]
	140492159284272 -> 140492159284128
	140492159284272 [label=GeluBackward0]
	140492159284368 -> 140492159284272
	140492159284368 [label=AddBackward0]
	140492159284464 -> 140492159284368
	140492159284464 [label=UnsafeViewBackward0]
	140492159284608 -> 140492159284464
	140492159284608 [label=MmBackward0]
	140492159284704 -> 140492159284608
	140492159284704 [label=ReshapeAliasBackward0]
	140492159284848 -> 140492159284704
	140492159284848 [label=NativeLayerNormBackward0]
	140492159284896 -> 140492159284848
	140492159284896 [label=PermuteBackward0]
	140492159285184 -> 140492159284896
	140492159285184 [label=ConvolutionBackward0]
	140492159278768 -> 140492159285184
	140492159285232 -> 140492159285184
	140492151305376 [label="stages.0.blocks.0.conv_dw.weight
 (48, 1, 3, 3)" fillcolor=lightblue]
	140492151305376 -> 140492159285232
	140492159285232 [label=AccumulateGrad]
	140492159285088 -> 140492159285184
	140492151304976 [label="stages.0.blocks.0.conv_dw.bias
 (48)" fillcolor=lightblue]
	140492151304976 -> 140492159285088
	140492159285088 [label=AccumulateGrad]
	140492159284752 -> 140492159284848
	140492151305216 [label="stages.0.blocks.0.norm.weight
 (48)" fillcolor=lightblue]
	140492151305216 -> 140492159284752
	140492159284752 [label=AccumulateGrad]
	140492159284992 -> 140492159284848
	140492151305296 [label="stages.0.blocks.0.norm.bias
 (48)" fillcolor=lightblue]
	140492151305296 -> 140492159284992
	140492159284992 [label=AccumulateGrad]
	140492159284656 -> 140492159284608
	140492159284656 [label=TBackward0]
	140492159285136 -> 140492159284656
	140492433381312 [label="stages.0.blocks.0.mlp.fc1.weight
 (192, 48)" fillcolor=lightblue]
	140492433381312 -> 140492159285136
	140492159285136 [label=AccumulateGrad]
	140492159284416 -> 140492159284368
	140492433381392 [label="stages.0.blocks.0.mlp.fc1.bias
 (192)" fillcolor=lightblue]
	140492433381392 -> 140492159284416
	140492159284416 [label=AccumulateGrad]
	140492159284080 -> 140492159284032
	140492159284080 [label=TBackward0]
	140492159284176 -> 140492159284080
	140492433381472 [label="stages.0.blocks.0.mlp.fc2.weight
 (48, 192)" fillcolor=lightblue]
	140492433381472 -> 140492159284176
	140492159284176 [label=AccumulateGrad]
	140492159283840 -> 140492159283552
	140492433381552 [label="stages.0.blocks.0.mlp.fc2.bias
 (48)" fillcolor=lightblue]
	140492433381552 -> 140492159283840
	140492159283840 [label=AccumulateGrad]
	140492159278576 -> 140492159278480
	140492159278576 [label=PermuteBackward0]
	140492159278672 -> 140492159278576
	140492159278672 [label=MulBackward0]
	140492159284320 -> 140492159278672
	140492433381712 [label="stages.0.blocks.1.gamma
 (48)" fillcolor=lightblue]
	140492433381712 -> 140492159284320
	140492159284320 [label=AccumulateGrad]
	140492159283888 -> 140492159278672
	140492159283888 [label=AddBackward0]
	140492159283936 -> 140492159283888
	140492159283936 [label=UnsafeViewBackward0]
	140492159284560 -> 140492159283936
	140492159284560 [label=MmBackward0]
	140492159285328 -> 140492159284560
	140492159285328 [label=ReshapeAliasBackward0]
	140492159285616 -> 140492159285328
	140492159285616 [label=GeluBackward0]
	140492159285712 -> 140492159285616
	140492159285712 [label=AddBackward0]
	140492159285808 -> 140492159285712
	140492159285808 [label=UnsafeViewBackward0]
	140492159285952 -> 140492159285808
	140492159285952 [label=MmBackward0]
	140492159286048 -> 140492159285952
	140492159286048 [label=ReshapeAliasBackward0]
	140492159286192 -> 140492159286048
	140492159286192 [label=NativeLayerNormBackward0]
	140492159286288 -> 140492159286192
	140492159286288 [label=PermuteBackward0]
	140492159286480 -> 140492159286288
	140492159286480 [label=ConvolutionBackward0]
	140492159278624 -> 140492159286480
	140492159286576 -> 140492159286480
	140492433381792 [label="stages.0.blocks.1.conv_dw.weight
 (48, 1, 3, 3)" fillcolor=lightblue]
	140492433381792 -> 140492159286576
	140492159286576 [label=AccumulateGrad]
	140492159286528 -> 140492159286480
	140492433381872 [label="stages.0.blocks.1.conv_dw.bias
 (48)" fillcolor=lightblue]
	140492433381872 -> 140492159286528
	140492159286528 [label=AccumulateGrad]
	140492159286240 -> 140492159286192
	140492433381952 [label="stages.0.blocks.1.norm.weight
 (48)" fillcolor=lightblue]
	140492433381952 -> 140492159286240
	140492159286240 [label=AccumulateGrad]
	140492159286096 -> 140492159286192
	140492433382032 [label="stages.0.blocks.1.norm.bias
 (48)" fillcolor=lightblue]
	140492433382032 -> 140492159286096
	140492159286096 [label=AccumulateGrad]
	140492159286000 -> 140492159285952
	140492159286000 [label=TBackward0]
	140492159286432 -> 140492159286000
	140492433382112 [label="stages.0.blocks.1.mlp.fc1.weight
 (192, 48)" fillcolor=lightblue]
	140492433382112 -> 140492159286432
	140492159286432 [label=AccumulateGrad]
	140492159285760 -> 140492159285712
	140492433382192 [label="stages.0.blocks.1.mlp.fc1.bias
 (192)" fillcolor=lightblue]
	140492433382192 -> 140492159285760
	140492159285760 [label=AccumulateGrad]
	140492159285520 -> 140492159284560
	140492159285520 [label=TBackward0]
	140492159284800 -> 140492159285520
	140492433382272 [label="stages.0.blocks.1.mlp.fc2.weight
 (48, 192)" fillcolor=lightblue]
	140492433382272 -> 140492159284800
	140492159284800 [label=AccumulateGrad]
	140492159283984 -> 140492159283888
	140492433382352 [label="stages.0.blocks.1.mlp.fc2.bias
 (48)" fillcolor=lightblue]
	140492433382352 -> 140492159283984
	140492159283984 [label=AccumulateGrad]
	140492159278432 -> 140492159278384
	140492159278432 [label=PermuteBackward0]
	140492159278528 -> 140492159278432
	140492159278528 [label=MulBackward0]
	140492159285664 -> 140492159278528
	140492433382512 [label="stages.0.blocks.2.gamma
 (48)" fillcolor=lightblue]
	140492433382512 -> 140492159285664
	140492159285664 [label=AccumulateGrad]
	140492159284512 -> 140492159278528
	140492159284512 [label=AddBackward0]
	140492159285040 -> 140492159284512
	140492159285040 [label=UnsafeViewBackward0]
	140492159285904 -> 140492159285040
	140492159285904 [label=MmBackward0]
	140492159286384 -> 140492159285904
	140492159286384 [label=ReshapeAliasBackward0]
	140492159286720 -> 140492159286384
	140492159286720 [label=GeluBackward0]
	140492159286816 -> 140492159286720
	140492159286816 [label=AddBackward0]
	140492159286912 -> 140492159286816
	140492159286912 [label=UnsafeViewBackward0]
	140492159287056 -> 140492159286912
	140492159287056 [label=MmBackward0]
	140492159287152 -> 140492159287056
	140492159287152 [label=ReshapeAliasBackward0]
	140492159287248 -> 140492159287152
	140492159287248 [label=NativeLayerNormBackward0]
	140492159303840 -> 140492159287248
	140492159303840 [label=PermuteBackward0]
	140492159304032 -> 140492159303840
	140492159304032 [label=ConvolutionBackward0]
	140492159278480 -> 140492159304032
	140492159304128 -> 140492159304032
	140492433382592 [label="stages.0.blocks.2.conv_dw.weight
 (48, 1, 3, 3)" fillcolor=lightblue]
	140492433382592 -> 140492159304128
	140492159304128 [label=AccumulateGrad]
	140492159304080 -> 140492159304032
	140492433382672 [label="stages.0.blocks.2.conv_dw.bias
 (48)" fillcolor=lightblue]
	140492433382672 -> 140492159304080
	140492159304080 [label=AccumulateGrad]
	140492159303792 -> 140492159287248
	140492433382752 [label="stages.0.blocks.2.norm.weight
 (48)" fillcolor=lightblue]
	140492433382752 -> 140492159303792
	140492159303792 [label=AccumulateGrad]
	140492159303744 -> 140492159287248
	140492433382832 [label="stages.0.blocks.2.norm.bias
 (48)" fillcolor=lightblue]
	140492433382832 -> 140492159303744
	140492159303744 [label=AccumulateGrad]
	140492159287104 -> 140492159287056
	140492159287104 [label=TBackward0]
	140492159287200 -> 140492159287104
	140492433382912 [label="stages.0.blocks.2.mlp.fc1.weight
 (192, 48)" fillcolor=lightblue]
	140492433382912 -> 140492159287200
	140492159287200 [label=AccumulateGrad]
	140492159286864 -> 140492159286816
	140492433382992 [label="stages.0.blocks.2.mlp.fc1.bias
 (192)" fillcolor=lightblue]
	140492433382992 -> 140492159286864
	140492159286864 [label=AccumulateGrad]
	140492159286624 -> 140492159285904
	140492159286624 [label=TBackward0]
	140492159286144 -> 140492159286624
	140492433383072 [label="stages.0.blocks.2.mlp.fc2.weight
 (48, 192)" fillcolor=lightblue]
	140492433383072 -> 140492159286144
	140492159286144 [label=AccumulateGrad]
	140492159284224 -> 140492159284512
	140492433383152 [label="stages.0.blocks.2.mlp.fc2.bias
 (48)" fillcolor=lightblue]
	140492433383152 -> 140492159284224
	140492159284224 [label=AccumulateGrad]
	140492159278144 -> 140492159278096
	140492433383312 [label="stages.1.downsample.0.weight
 (48)" fillcolor=lightblue]
	140492433383312 -> 140492159278144
	140492159278144 [label=AccumulateGrad]
	140492159278000 -> 140492159278096
	140492433502272 [label="stages.1.downsample.0.bias
 (48)" fillcolor=lightblue]
	140492433502272 -> 140492159278000
	140492159278000 [label=AccumulateGrad]
	140492159277856 -> 140492159277760
	140492433502432 [label="stages.1.downsample.1.weight
 (96, 48, 2, 2)" fillcolor=lightblue]
	140492433502432 -> 140492159277856
	140492159277856 [label=AccumulateGrad]
	140492159277808 -> 140492159277760
	140492433502512 [label="stages.1.downsample.1.bias
 (96)" fillcolor=lightblue]
	140492433502512 -> 140492159277808
	140492159277808 [label=AccumulateGrad]
	140492159277712 -> 140492159277616
	140492159277712 [label=PermuteBackward0]
	140492159278240 -> 140492159277712
	140492159278240 [label=MulBackward0]
	140492159278288 -> 140492159278240
	140492433502592 [label="stages.1.blocks.0.gamma
 (96)" fillcolor=lightblue]
	140492433502592 -> 140492159278288
	140492159278288 [label=AccumulateGrad]
	140492159278336 -> 140492159278240
	140492159278336 [label=AddBackward0]
	140492159283360 -> 140492159278336
	140492159283360 [label=UnsafeViewBackward0]
	140492159285568 -> 140492159283360
	140492159285568 [label=MmBackward0]
	140492159286960 -> 140492159285568
	140492159286960 [label=ReshapeAliasBackward0]
	140492159286672 -> 140492159286960
	140492159286672 [label=GeluBackward0]
	140492159303888 -> 140492159286672
	140492159303888 [label=AddBackward0]
	140492159304272 -> 140492159303888
	140492159304272 [label=UnsafeViewBackward0]
	140492159304416 -> 140492159304272
	140492159304416 [label=MmBackward0]
	140492159304512 -> 140492159304416
	140492159304512 [label=ReshapeAliasBackward0]
	140492159304656 -> 140492159304512
	140492159304656 [label=NativeLayerNormBackward0]
	140492159304752 -> 140492159304656
	140492159304752 [label=PermuteBackward0]
	140492159304944 -> 140492159304752
	140492159304944 [label=ConvolutionBackward0]
	140492159277760 -> 140492159304944
	140492159305040 -> 140492159304944
	140492433502672 [label="stages.1.blocks.0.conv_dw.weight
 (96, 1, 5, 5)" fillcolor=lightblue]
	140492433502672 -> 140492159305040
	140492159305040 [label=AccumulateGrad]
	140492159304992 -> 140492159304944
	140492433502752 [label="stages.1.blocks.0.conv_dw.bias
 (96)" fillcolor=lightblue]
	140492433502752 -> 140492159304992
	140492159304992 [label=AccumulateGrad]
	140492159304704 -> 140492159304656
	140492433502832 [label="stages.1.blocks.0.norm.weight
 (96)" fillcolor=lightblue]
	140492433502832 -> 140492159304704
	140492159304704 [label=AccumulateGrad]
	140492159304560 -> 140492159304656
	140492433502912 [label="stages.1.blocks.0.norm.bias
 (96)" fillcolor=lightblue]
	140492433502912 -> 140492159304560
	140492159304560 [label=AccumulateGrad]
	140492159304464 -> 140492159304416
	140492159304464 [label=TBackward0]
	140492159304896 -> 140492159304464
	140492433502992 [label="stages.1.blocks.0.mlp.fc1.weight
 (384, 96)" fillcolor=lightblue]
	140492433502992 -> 140492159304896
	140492159304896 [label=AccumulateGrad]
	140492159304224 -> 140492159303888
	140492433503072 [label="stages.1.blocks.0.mlp.fc1.bias
 (384)" fillcolor=lightblue]
	140492433503072 -> 140492159304224
	140492159304224 [label=AccumulateGrad]
	140492159287008 -> 140492159285568
	140492159287008 [label=TBackward0]
	140492159303984 -> 140492159287008
	140492433503152 [label="stages.1.blocks.0.mlp.fc2.weight
 (96, 384)" fillcolor=lightblue]
	140492433503152 -> 140492159303984
	140492159303984 [label=AccumulateGrad]
	140492159283264 -> 140492159278336
	140492433503232 [label="stages.1.blocks.0.mlp.fc2.bias
 (96)" fillcolor=lightblue]
	140492433503232 -> 140492159283264
	140492159283264 [label=AccumulateGrad]
	140492159277568 -> 140492159277472
	140492159277568 [label=PermuteBackward0]
	140492159277952 -> 140492159277568
	140492159277952 [label=MulBackward0]
	140492159277664 -> 140492159277952
	140492433503392 [label="stages.1.blocks.1.gamma
 (96)" fillcolor=lightblue]
	140492433503392 -> 140492159277664
	140492159277664 [label=AccumulateGrad]
	140492159286768 -> 140492159277952
	140492159286768 [label=AddBackward0]
	140492159286336 -> 140492159286768
	140492159286336 [label=UnsafeViewBackward0]
	140492159304368 -> 140492159286336
	140492159304368 [label=MmBackward0]
	140492159304848 -> 140492159304368
	140492159304848 [label=ReshapeAliasBackward0]
	140492159305184 -> 140492159304848
	140492159305184 [label=GeluBackward0]
	140492159305280 -> 140492159305184
	140492159305280 [label=AddBackward0]
	140492159305376 -> 140492159305280
	140492159305376 [label=UnsafeViewBackward0]
	140492159305520 -> 140492159305376
	140492159305520 [label=MmBackward0]
	140492159305616 -> 140492159305520
	140492159305616 [label=ReshapeAliasBackward0]
	140492159305760 -> 140492159305616
	140492159305760 [label=NativeLayerNormBackward0]
	140492159305856 -> 140492159305760
	140492159305856 [label=PermuteBackward0]
	140492159306048 -> 140492159305856
	140492159306048 [label=ConvolutionBackward0]
	140492159277616 -> 140492159306048
	140492159306144 -> 140492159306048
	140492433503472 [label="stages.1.blocks.1.conv_dw.weight
 (96, 1, 5, 5)" fillcolor=lightblue]
	140492433503472 -> 140492159306144
	140492159306144 [label=AccumulateGrad]
	140492159306096 -> 140492159306048
	140492433503552 [label="stages.1.blocks.1.conv_dw.bias
 (96)" fillcolor=lightblue]
	140492433503552 -> 140492159306096
	140492159306096 [label=AccumulateGrad]
	140492159305808 -> 140492159305760
	140492433503632 [label="stages.1.blocks.1.norm.weight
 (96)" fillcolor=lightblue]
	140492433503632 -> 140492159305808
	140492159305808 [label=AccumulateGrad]
	140492159305664 -> 140492159305760
	140492433503712 [label="stages.1.blocks.1.norm.bias
 (96)" fillcolor=lightblue]
	140492433503712 -> 140492159305664
	140492159305664 [label=AccumulateGrad]
	140492159305568 -> 140492159305520
	140492159305568 [label=TBackward0]
	140492159306000 -> 140492159305568
	140492433503792 [label="stages.1.blocks.1.mlp.fc1.weight
 (384, 96)" fillcolor=lightblue]
	140492433503792 -> 140492159306000
	140492159306000 [label=AccumulateGrad]
	140492159305328 -> 140492159305280
	140492433503872 [label="stages.1.blocks.1.mlp.fc1.bias
 (384)" fillcolor=lightblue]
	140492433503872 -> 140492159305328
	140492159305328 [label=AccumulateGrad]
	140492159305088 -> 140492159304368
	140492159305088 [label=TBackward0]
	140492159304608 -> 140492159305088
	140492433503952 [label="stages.1.blocks.1.mlp.fc2.weight
 (96, 384)" fillcolor=lightblue]
	140492433503952 -> 140492159304608
	140492159304608 [label=AccumulateGrad]
	140492159303936 -> 140492159286768
	140492433504032 [label="stages.1.blocks.1.mlp.fc2.bias
 (96)" fillcolor=lightblue]
	140492433504032 -> 140492159303936
	140492159303936 [label=AccumulateGrad]
	140492159277424 -> 140492159277376
	140492159277424 [label=PermuteBackward0]
	140492159285856 -> 140492159277424
	140492159285856 [label=MulBackward0]
	140492159277520 -> 140492159285856
	140492433504832 [label="stages.1.blocks.2.gamma
 (96)" fillcolor=lightblue]
	140492433504832 -> 140492159277520
	140492159277520 [label=AccumulateGrad]
	140492159305232 -> 140492159285856
	140492159305232 [label=AddBackward0]
	140492159304800 -> 140492159305232
	140492159304800 [label=UnsafeViewBackward0]
	140492159306192 -> 140492159304800
	140492159306192 [label=MmBackward0]
	140492159305712 -> 140492159306192
	140492159305712 [label=ReshapeAliasBackward0]
	140492159306336 -> 140492159305712
	140492159306336 [label=GeluBackward0]
	140492159306432 -> 140492159306336
	140492159306432 [label=AddBackward0]
	140492159306528 -> 140492159306432
	140492159306528 [label=UnsafeViewBackward0]
	140492159306672 -> 140492159306528
	140492159306672 [label=MmBackward0]
	140492159306768 -> 140492159306672
	140492159306768 [label=ReshapeAliasBackward0]
	140492159306912 -> 140492159306768
	140492159306912 [label=NativeLayerNormBackward0]
	140492159307008 -> 140492159306912
	140492159307008 [label=ReshapeAliasBackward0]
	140492159307200 -> 140492159307008
	140492159307200 [label=AddBackward0]
	140492159307296 -> 140492159307200
	140492159307296 [label=AddBackward0]
	140492159307440 -> 140492159307296
	140492159307440 [label=PermuteBackward0]
	140492159307584 -> 140492159307440
	140492159307584 [label=ReshapeAliasBackward0]
	140492159307680 -> 140492159307584
	140492159307680 [label=CatBackward0]
	140492159307728 -> 140492159307680
	140492159307728 [label=ConvolutionBackward0]
	140492159307488 -> 140492159307728
	140492159307488 [label=SplitBackward0]
	140492159277472 -> 140492159307488
	140492159328464 -> 140492159307728
	140492433504272 [label="stages.1.blocks.2.convs.0.weight
 (48, 1, 3, 3)" fillcolor=lightblue]
	140492433504272 -> 140492159328464
	140492159328464 [label=AccumulateGrad]
	140492159328416 -> 140492159307728
	140492433504352 [label="stages.1.blocks.2.convs.0.bias
 (48)" fillcolor=lightblue]
	140492433504352 -> 140492159328416
	140492159328416 [label=AccumulateGrad]
	140492159307488 -> 140492159307680
	140492159307392 -> 140492159307296
	140492159307392 [label=PermuteBackward0]
	140492159307632 -> 140492159307392
	140492159307632 [label=ReshapeAliasBackward0]
	140492159328608 -> 140492159307632
	140492159328608 [label=ConvolutionBackward0]
	140492159328512 -> 140492159328608
	140492433504512 [label="stages.1.blocks.2.pos_embd.token_projection.weight
 (96, 64, 1, 1)" fillcolor=lightblue]
	140492433504512 -> 140492159328512
	140492159328512 [label=AccumulateGrad]
	140492159328560 -> 140492159328608
	140492433504592 [label="stages.1.blocks.2.pos_embd.token_projection.bias
 (96)" fillcolor=lightblue]
	140492433504592 -> 140492159328560
	140492159328560 [label=AccumulateGrad]
	140492159307248 -> 140492159307200
	140492159307248 [label=MulBackward0]
	140492159307536 -> 140492159307248
	140492433504432 [label="stages.1.blocks.2.gamma_xca
 (96)" fillcolor=lightblue]
	140492433504432 -> 140492159307536
	140492159307536 [label=AccumulateGrad]
	140492159307344 -> 140492159307248
	140492159307344 [label=AddBackward0]
	140492159328320 -> 140492159307344
	140492159328320 [label=UnsafeViewBackward0]
	140492159328848 -> 140492159328320
	140492159328848 [label=MmBackward0]
	140492159328944 -> 140492159328848
	140492159328944 [label=ReshapeAliasBackward0]
	140492159329088 -> 140492159328944
	140492159329088 [label=ReshapeAliasBackward0]
	140492159329184 -> 140492159329088
	140492159329184 [label=PermuteBackward0]
	140492159329280 -> 140492159329184
	140492159329280 [label=UnsafeViewBackward0]
	140492159329376 -> 140492159329280
	140492159329376 [label=BmmBackward0]
	140492159329472 -> 140492159329376
	140492159329472 [label=ReshapeAliasBackward0]
	140492159329616 -> 140492159329472
	140492159329616 [label=ExpandBackward0]
	140492159329664 -> 140492159329616
	140492159329664 [label=SoftmaxBackward0]
	140492159329808 -> 140492159329664
	140492159329808 [label=MulBackward0]
	140492159329952 -> 140492159329808
	140492159329952 [label=UnsafeViewBackward0]
	140492159330192 -> 140492159329952
	140492159330192 [label=BmmBackward0]
	140492159330240 -> 140492159330192
	140492159330240 [label=ReshapeAliasBackward0]
	140492159330480 -> 140492159330240
	140492159330480 [label=ExpandBackward0]
	140492159330528 -> 140492159330480
	140492159330528 [label=DivBackward0]
	140492159330672 -> 140492159330528
	140492159330672 [label=UnbindBackward0]
	140492159330912 -> 140492159330672
	140492159330912 [label=PermuteBackward0]
	140492159330960 -> 140492159330912
	140492159330960 [label=ReshapeAliasBackward0]
	140492159331104 -> 140492159330960
	140492159331104 [label=ViewBackward0]
	140492159331248 -> 140492159331104
	140492159331248 [label=AddmmBackward0]
	140492159331392 -> 140492159331248
	140492433505072 [label="stages.1.blocks.2.xca.qkv.bias
 (288)" fillcolor=lightblue]
	140492433505072 -> 140492159331392
	140492159331392 [label=AccumulateGrad]
	140492159331344 -> 140492159331248
	140492159331344 [label=ViewBackward0]
	140492159331728 -> 140492159331344
	140492159331728 [label=NativeLayerNormBackward0]
	140492159307296 -> 140492159331728
	140492159331920 -> 140492159331728
	140492433504672 [label="stages.1.blocks.2.norm_xca.weight
 (96)" fillcolor=lightblue]
	140492433504672 -> 140492159331920
	140492159331920 [label=AccumulateGrad]
	140492159331872 -> 140492159331728
	140492433504752 [label="stages.1.blocks.2.norm_xca.bias
 (96)" fillcolor=lightblue]
	140492433504752 -> 140492159331872
	140492159331872 [label=AccumulateGrad]
	140492159331488 -> 140492159331248
	140492159331488 [label=TBackward0]
	140492159332016 -> 140492159331488
	140492433504992 [label="stages.1.blocks.2.xca.qkv.weight
 (288, 96)" fillcolor=lightblue]
	140492433504992 -> 140492159332016
	140492159332016 [label=AccumulateGrad]
	140492159330624 -> 140492159330528
	140492159330624 [label=ExpandBackward0]
	140492159331056 -> 140492159330624
	140492159331056 [label=ClampMinBackward0]
	140492159331536 -> 140492159331056
	140492159331536 [label=NormBackward1]
	140492159330672 -> 140492159331536
	140492159330096 -> 140492159330192
	140492159330096 [label=ReshapeAliasBackward0]
	140492159330768 -> 140492159330096
	140492159330768 [label=ExpandBackward0]
	140492159331200 -> 140492159330768
	140492159331200 [label=TransposeBackward0]
	140492159331824 -> 140492159331200
	140492159331824 [label=DivBackward0]
	140492159330672 -> 140492159331824
	140492159332064 -> 140492159331824
	140492159332064 [label=ExpandBackward0]
	140492159332160 -> 140492159332064
	140492159332160 [label=ClampMinBackward0]
	140492159332256 -> 140492159332160
	140492159332256 [label=NormBackward1]
	140492159330672 -> 140492159332256
	140492159329904 -> 140492159329808
	140492433504912 [label="stages.1.blocks.2.xca.temperature
 (8, 1, 1)" fillcolor=lightblue]
	140492433504912 -> 140492159329904
	140492159329904 [label=AccumulateGrad]
	140492159329424 -> 140492159329376
	140492159329424 [label=ReshapeAliasBackward0]
	140492159329760 -> 140492159329424
	140492159329760 [label=ExpandBackward0]
	140492159330672 -> 140492159329760
	140492159328896 -> 140492159328848
	140492159328896 [label=TBackward0]
	140492159329232 -> 140492159328896
	140492433505152 [label="stages.1.blocks.2.xca.proj.weight
 (96, 96)" fillcolor=lightblue]
	140492433505152 -> 140492159329232
	140492159329232 [label=AccumulateGrad]
	140492159328656 -> 140492159307344
	140492433505232 [label="stages.1.blocks.2.xca.proj.bias
 (96)" fillcolor=lightblue]
	140492433505232 -> 140492159328656
	140492159328656 [label=AccumulateGrad]
	140492159306960 -> 140492159306912
	140492433505312 [label="stages.1.blocks.2.norm.weight
 (96)" fillcolor=lightblue]
	140492433505312 -> 140492159306960
	140492159306960 [label=AccumulateGrad]
	140492159306816 -> 140492159306912
	140492433505392 [label="stages.1.blocks.2.norm.bias
 (96)" fillcolor=lightblue]
	140492433505392 -> 140492159306816
	140492159306816 [label=AccumulateGrad]
	140492159306720 -> 140492159306672
	140492159306720 [label=TBackward0]
	140492159307152 -> 140492159306720
	140492433505472 [label="stages.1.blocks.2.mlp.fc1.weight
 (384, 96)" fillcolor=lightblue]
	140492433505472 -> 140492159307152
	140492159307152 [label=AccumulateGrad]
	140492159306480 -> 140492159306432
	140492433505552 [label="stages.1.blocks.2.mlp.fc1.bias
 (384)" fillcolor=lightblue]
	140492433505552 -> 140492159306480
	140492159306480 [label=AccumulateGrad]
	140492159305952 -> 140492159306192
	140492159305952 [label=TBackward0]
	140492159306240 -> 140492159305952
	140492433505632 [label="stages.1.blocks.2.mlp.fc2.weight
 (96, 384)" fillcolor=lightblue]
	140492433505632 -> 140492159306240
	140492159306240 [label=AccumulateGrad]
	140492159304176 -> 140492159305232
	140492433505712 [label="stages.1.blocks.2.mlp.fc2.bias
 (96)" fillcolor=lightblue]
	140492433505712 -> 140492159304176
	140492159304176 [label=AccumulateGrad]
	140492159277136 -> 140492159277088
	140492433505872 [label="stages.2.downsample.0.weight
 (96)" fillcolor=lightblue]
	140492433505872 -> 140492159277136
	140492159277136 [label=AccumulateGrad]
	140492159276992 -> 140492159277088
	140492433505952 [label="stages.2.downsample.0.bias
 (96)" fillcolor=lightblue]
	140492433505952 -> 140492159276992
	140492159276992 [label=AccumulateGrad]
	140492159276848 -> 140492159276752
	140492433506112 [label="stages.2.downsample.1.weight
 (160, 96, 2, 2)" fillcolor=lightblue]
	140492433506112 -> 140492159276848
	140492159276848 [label=AccumulateGrad]
	140492159276800 -> 140492159276752
	140492433506192 [label="stages.2.downsample.1.bias
 (160)" fillcolor=lightblue]
	140492433506192 -> 140492159276800
	140492159276800 [label=AccumulateGrad]
	140492159276704 -> 140492159276608
	140492159276704 [label=PermuteBackward0]
	140492159277232 -> 140492159276704
	140492159277232 [label=MulBackward0]
	140492159277280 -> 140492159277232
	140493489602624 [label="stages.2.blocks.0.gamma
 (160)" fillcolor=lightblue]
	140493489602624 -> 140492159277280
	140492159277280 [label=AccumulateGrad]
	140492159277328 -> 140492159277232
	140492159277328 [label=AddBackward0]
	140492159278048 -> 140492159277328
	140492159278048 [label=UnsafeViewBackward0]
	140492159305904 -> 140492159278048
	140492159305904 [label=MmBackward0]
	140492159307056 -> 140492159305904
	140492159307056 [label=ReshapeAliasBackward0]
	140492159307104 -> 140492159307056
	140492159307104 [label=GeluBackward0]
	140492159306864 -> 140492159307104
	140492159306864 [label=AddBackward0]
	140492159329136 -> 140492159306864
	140492159329136 [label=UnsafeViewBackward0]
	140492159328992 -> 140492159329136
	140492159328992 [label=MmBackward0]
	140492159329040 -> 140492159328992
	140492159329040 [label=ReshapeAliasBackward0]
	140492159330048 -> 140492159329040
	140492159330048 [label=NativeLayerNormBackward0]
	140492159330336 -> 140492159330048
	140492159330336 [label=PermuteBackward0]
	140492159330864 -> 140492159330336
	140492159330864 [label=ConvolutionBackward0]
	140492159276752 -> 140492159330864
	140492159332208 -> 140492159330864
	140493489602704 [label="stages.2.blocks.0.conv_dw.weight
 (160, 1, 7, 7)" fillcolor=lightblue]
	140493489602704 -> 140492159332208
	140492159332208 [label=AccumulateGrad]
	140492159331776 -> 140492159330864
	140493489602784 [label="stages.2.blocks.0.conv_dw.bias
 (160)" fillcolor=lightblue]
	140493489602784 -> 140492159331776
	140492159331776 [label=AccumulateGrad]
	140492159329568 -> 140492159330048
	140493489602864 [label="stages.2.blocks.0.norm.weight
 (160)" fillcolor=lightblue]
	140493489602864 -> 140492159329568
	140492159329568 [label=AccumulateGrad]
	140492159329520 -> 140492159330048
	140493489602944 [label="stages.2.blocks.0.norm.bias
 (160)" fillcolor=lightblue]
	140493489602944 -> 140492159329520
	140492159329520 [label=AccumulateGrad]
	140492159329328 -> 140492159328992
	140492159329328 [label=TBackward0]
	140492159332112 -> 140492159329328
	140493489603024 [label="stages.2.blocks.0.mlp.fc1.weight
 (640, 160)" fillcolor=lightblue]
	140493489603024 -> 140492159332112
	140492159332112 [label=AccumulateGrad]
	140492159328704 -> 140492159306864
	140493489603104 [label="stages.2.blocks.0.mlp.fc1.bias
 (640)" fillcolor=lightblue]
	140493489603104 -> 140492159328704
	140492159328704 [label=AccumulateGrad]
	140492159306576 -> 140492159305904
	140492159306576 [label=TBackward0]
	140492159306288 -> 140492159306576
	140493489603184 [label="stages.2.blocks.0.mlp.fc2.weight
 (160, 640)" fillcolor=lightblue]
	140493489603184 -> 140492159306288
	140492159306288 [label=AccumulateGrad]
	140492159304320 -> 140492159277328
	140493489603264 [label="stages.2.blocks.0.mlp.fc2.bias
 (160)" fillcolor=lightblue]
	140493489603264 -> 140492159304320
	140492159304320 [label=AccumulateGrad]
	140492159276560 -> 140492159276464
	140492159276560 [label=PermuteBackward0]
	140492159276944 -> 140492159276560
	140492159276944 [label=MulBackward0]
	140492159276656 -> 140492159276944
	140493489603424 [label="stages.2.blocks.1.gamma
 (160)" fillcolor=lightblue]
	140493489603424 -> 140492159276656
	140492159276656 [label=AccumulateGrad]
	140492159306624 -> 140492159276944
	140492159306624 [label=AddBackward0]
	140492159306384 -> 140492159306624
	140492159306384 [label=UnsafeViewBackward0]
	140492159328800 -> 140492159306384
	140492159328800 [label=MmBackward0]
	140492159330816 -> 140492159328800
	140492159330816 [label=ReshapeAliasBackward0]
	140492159330432 -> 140492159330816
	140492159330432 [label=GeluBackward0]
	140492159361184 -> 140492159330432
	140492159361184 [label=AddBackward0]
	140492159361280 -> 140492159361184
	140492159361280 [label=UnsafeViewBackward0]
	140492159361424 -> 140492159361280
	140492159361424 [label=MmBackward0]
	140492159361520 -> 140492159361424
	140492159361520 [label=ReshapeAliasBackward0]
	140492159361664 -> 140492159361520
	140492159361664 [label=NativeLayerNormBackward0]
	140492159361760 -> 140492159361664
	140492159361760 [label=PermuteBackward0]
	140492159361952 -> 140492159361760
	140492159361952 [label=ConvolutionBackward0]
	140492159276608 -> 140492159361952
	140492159362048 -> 140492159361952
	140493489603504 [label="stages.2.blocks.1.conv_dw.weight
 (160, 1, 7, 7)" fillcolor=lightblue]
	140493489603504 -> 140492159362048
	140492159362048 [label=AccumulateGrad]
	140492159362000 -> 140492159361952
	140493489603584 [label="stages.2.blocks.1.conv_dw.bias
 (160)" fillcolor=lightblue]
	140493489603584 -> 140492159362000
	140492159362000 [label=AccumulateGrad]
	140492159361712 -> 140492159361664
	140493489603664 [label="stages.2.blocks.1.norm.weight
 (160)" fillcolor=lightblue]
	140493489603664 -> 140492159361712
	140492159361712 [label=AccumulateGrad]
	140492159361568 -> 140492159361664
	140493489603744 [label="stages.2.blocks.1.norm.bias
 (160)" fillcolor=lightblue]
	140493489603744 -> 140492159361568
	140492159361568 [label=AccumulateGrad]
	140492159361472 -> 140492159361424
	140492159361472 [label=TBackward0]
	140492159361904 -> 140492159361472
	140493489603824 [label="stages.2.blocks.1.mlp.fc1.weight
 (640, 160)" fillcolor=lightblue]
	140493489603824 -> 140492159361904
	140492159361904 [label=AccumulateGrad]
	140492159361232 -> 140492159361184
	140493489603904 [label="stages.2.blocks.1.mlp.fc1.bias
 (640)" fillcolor=lightblue]
	140493489603904 -> 140492159361232
	140492159361232 [label=AccumulateGrad]
	140492159332304 -> 140492159328800
	140492159332304 [label=TBackward0]
	140492159330384 -> 140492159332304
	140493489603984 [label="stages.2.blocks.1.mlp.fc2.weight
 (160, 640)" fillcolor=lightblue]
	140493489603984 -> 140492159330384
	140492159330384 [label=AccumulateGrad]
	140492159305424 -> 140492159306624
	140493489604064 [label="stages.2.blocks.1.mlp.fc2.bias
 (160)" fillcolor=lightblue]
	140493489604064 -> 140492159305424
	140492159305424 [label=AccumulateGrad]
	140492159276416 -> 140492159276320
	140492159276416 [label=PermuteBackward0]
	140492159305472 -> 140492159276416
	140492159305472 [label=MulBackward0]
	140492159276512 -> 140492159305472
	140493489604224 [label="stages.2.blocks.2.gamma
 (160)" fillcolor=lightblue]
	140493489604224 -> 140492159276512
	140492159276512 [label=AccumulateGrad]
	140492159330144 -> 140492159305472
	140492159330144 [label=AddBackward0]
	140492159328368 -> 140492159330144
	140492159328368 [label=UnsafeViewBackward0]
	140492159361376 -> 140492159328368
	140492159361376 [label=MmBackward0]
	140492159361856 -> 140492159361376
	140492159361856 [label=ReshapeAliasBackward0]
	140492159362192 -> 140492159361856
	140492159362192 [label=GeluBackward0]
	140492159362288 -> 140492159362192
	140492159362288 [label=AddBackward0]
	140492159362384 -> 140492159362288
	140492159362384 [label=UnsafeViewBackward0]
	140492159362528 -> 140492159362384
	140492159362528 [label=MmBackward0]
	140492159362624 -> 140492159362528
	140492159362624 [label=ReshapeAliasBackward0]
	140492159362768 -> 140492159362624
	140492159362768 [label=NativeLayerNormBackward0]
	140492159362864 -> 140492159362768
	140492159362864 [label=PermuteBackward0]
	140492159363056 -> 140492159362864
	140492159363056 [label=ConvolutionBackward0]
	140492159276464 -> 140492159363056
	140492159363152 -> 140492159363056
	140493489604304 [label="stages.2.blocks.2.conv_dw.weight
 (160, 1, 7, 7)" fillcolor=lightblue]
	140493489604304 -> 140492159363152
	140492159363152 [label=AccumulateGrad]
	140492159363104 -> 140492159363056
	140493489604384 [label="stages.2.blocks.2.conv_dw.bias
 (160)" fillcolor=lightblue]
	140493489604384 -> 140492159363104
	140492159363104 [label=AccumulateGrad]
	140492159362816 -> 140492159362768
	140493489604464 [label="stages.2.blocks.2.norm.weight
 (160)" fillcolor=lightblue]
	140493489604464 -> 140492159362816
	140492159362816 [label=AccumulateGrad]
	140492159362672 -> 140492159362768
	140493489604544 [label="stages.2.blocks.2.norm.bias
 (160)" fillcolor=lightblue]
	140493489604544 -> 140492159362672
	140492159362672 [label=AccumulateGrad]
	140492159362576 -> 140492159362528
	140492159362576 [label=TBackward0]
	140492159363008 -> 140492159362576
	140493489604624 [label="stages.2.blocks.2.mlp.fc1.weight
 (640, 160)" fillcolor=lightblue]
	140493489604624 -> 140492159363008
	140492159363008 [label=AccumulateGrad]
	140492159362336 -> 140492159362288
	140493489604704 [label="stages.2.blocks.2.mlp.fc1.bias
 (640)" fillcolor=lightblue]
	140493489604704 -> 140492159362336
	140492159362336 [label=AccumulateGrad]
	140492159362096 -> 140492159361376
	140492159362096 [label=TBackward0]
	140492159361616 -> 140492159362096
	140493489604784 [label="stages.2.blocks.2.mlp.fc2.weight
 (160, 640)" fillcolor=lightblue]
	140493489604784 -> 140492159361616
	140492159361616 [label=AccumulateGrad]
	140492159361088 -> 140492159330144
	140493489604864 [label="stages.2.blocks.2.mlp.fc2.bias
 (160)" fillcolor=lightblue]
	140493489604864 -> 140492159361088
	140492159361088 [label=AccumulateGrad]
	140492159276272 -> 140492159276176
	140492159276272 [label=PermuteBackward0]
	140492159328752 -> 140492159276272
	140492159328752 [label=MulBackward0]
	140492159276368 -> 140492159328752
	140493489605024 [label="stages.2.blocks.3.gamma
 (160)" fillcolor=lightblue]
	140493489605024 -> 140492159276368
	140492159276368 [label=AccumulateGrad]
	140492159362240 -> 140492159328752
	140492159362240 [label=AddBackward0]
	140492159361808 -> 140492159362240
	140492159361808 [label=UnsafeViewBackward0]
	140492159362480 -> 140492159361808
	140492159362480 [label=MmBackward0]
	140492159362960 -> 140492159362480
	140492159362960 [label=ReshapeAliasBackward0]
	140492159363296 -> 140492159362960
	140492159363296 [label=GeluBackward0]
	140492159363392 -> 140492159363296
	140492159363392 [label=AddBackward0]
	140492159363488 -> 140492159363392
	140492159363488 [label=UnsafeViewBackward0]
	140492159363632 -> 140492159363488
	140492159363632 [label=MmBackward0]
	140492159363728 -> 140492159363632
	140492159363728 [label=ReshapeAliasBackward0]
	140492159363872 -> 140492159363728
	140492159363872 [label=NativeLayerNormBackward0]
	140492159363968 -> 140492159363872
	140492159363968 [label=PermuteBackward0]
	140492159364160 -> 140492159363968
	140492159364160 [label=ConvolutionBackward0]
	140492159276320 -> 140492159364160
	140492159364256 -> 140492159364160
	140493489605104 [label="stages.2.blocks.3.conv_dw.weight
 (160, 1, 7, 7)" fillcolor=lightblue]
	140493489605104 -> 140492159364256
	140492159364256 [label=AccumulateGrad]
	140492159364208 -> 140492159364160
	140493489605184 [label="stages.2.blocks.3.conv_dw.bias
 (160)" fillcolor=lightblue]
	140493489605184 -> 140492159364208
	140492159364208 [label=AccumulateGrad]
	140492159363920 -> 140492159363872
	140493489605264 [label="stages.2.blocks.3.norm.weight
 (160)" fillcolor=lightblue]
	140493489605264 -> 140492159363920
	140492159363920 [label=AccumulateGrad]
	140492159363776 -> 140492159363872
	140493489605344 [label="stages.2.blocks.3.norm.bias
 (160)" fillcolor=lightblue]
	140493489605344 -> 140492159363776
	140492159363776 [label=AccumulateGrad]
	140492159363680 -> 140492159363632
	140492159363680 [label=TBackward0]
	140492159364112 -> 140492159363680
	140493489605424 [label="stages.2.blocks.3.mlp.fc1.weight
 (640, 160)" fillcolor=lightblue]
	140493489605424 -> 140492159364112
	140492159364112 [label=AccumulateGrad]
	140492159363440 -> 140492159363392
	140493489605504 [label="stages.2.blocks.3.mlp.fc1.bias
 (640)" fillcolor=lightblue]
	140493489605504 -> 140492159363440
	140492159363440 [label=AccumulateGrad]
	140492159363200 -> 140492159362480
	140492159363200 [label=TBackward0]
	140492159362720 -> 140492159363200
	140493489605584 [label="stages.2.blocks.3.mlp.fc2.weight
 (160, 640)" fillcolor=lightblue]
	140493489605584 -> 140492159362720
	140492159362720 [label=AccumulateGrad]
	140492159361136 -> 140492159362240
	140493489605664 [label="stages.2.blocks.3.mlp.fc2.bias
 (160)" fillcolor=lightblue]
	140493489605664 -> 140492159361136
	140492159361136 [label=AccumulateGrad]
	140492159276128 -> 140492159276032
	140492159276128 [label=PermuteBackward0]
	140492159277040 -> 140492159276128
	140492159277040 [label=MulBackward0]
	140492159363344 -> 140492159277040
	140493489605824 [label="stages.2.blocks.4.gamma
 (160)" fillcolor=lightblue]
	140493489605824 -> 140492159363344
	140492159363344 [label=AccumulateGrad]
	140492159362432 -> 140492159277040
	140492159362432 [label=AddBackward0]
	140492159362912 -> 140492159362432
	140492159362912 [label=UnsafeViewBackward0]
	140492159363584 -> 140492159362912
	140492159363584 [label=MmBackward0]
	140492159364064 -> 140492159363584
	140492159364064 [label=ReshapeAliasBackward0]
	140492159364400 -> 140492159364064
	140492159364400 [label=GeluBackward0]
	140492159364496 -> 140492159364400
	140492159364496 [label=AddBackward0]
	140492159364592 -> 140492159364496
	140492159364592 [label=UnsafeViewBackward0]
	140492159364736 -> 140492159364592
	140492159364736 [label=MmBackward0]
	140492159364832 -> 140492159364736
	140492159364832 [label=ReshapeAliasBackward0]
	140492159364976 -> 140492159364832
	140492159364976 [label=NativeLayerNormBackward0]
	140492159365072 -> 140492159364976
	140492159365072 [label=PermuteBackward0]
	140492975476944 -> 140492159365072
	140492975476944 [label=ConvolutionBackward0]
	140492159276176 -> 140492975476944
	140492975477040 -> 140492975476944
	140493489605904 [label="stages.2.blocks.4.conv_dw.weight
 (160, 1, 7, 7)" fillcolor=lightblue]
	140493489605904 -> 140492975477040
	140492975477040 [label=AccumulateGrad]
	140492975476992 -> 140492975476944
	140493489605984 [label="stages.2.blocks.4.conv_dw.bias
 (160)" fillcolor=lightblue]
	140493489605984 -> 140492975476992
	140492975476992 [label=AccumulateGrad]
	140492159365024 -> 140492159364976
	140493489606064 [label="stages.2.blocks.4.norm.weight
 (160)" fillcolor=lightblue]
	140493489606064 -> 140492159365024
	140492159365024 [label=AccumulateGrad]
	140492159364880 -> 140492159364976
	140493489606144 [label="stages.2.blocks.4.norm.bias
 (160)" fillcolor=lightblue]
	140493489606144 -> 140492159364880
	140492159364880 [label=AccumulateGrad]
	140492159364784 -> 140492159364736
	140492159364784 [label=TBackward0]
	140492159364928 -> 140492159364784
	140493489606224 [label="stages.2.blocks.4.mlp.fc1.weight
 (640, 160)" fillcolor=lightblue]
	140493489606224 -> 140492159364928
	140492159364928 [label=AccumulateGrad]
	140492159364544 -> 140492159364496
	140493489606304 [label="stages.2.blocks.4.mlp.fc1.bias
 (640)" fillcolor=lightblue]
	140493489606304 -> 140492159364544
	140492159364544 [label=AccumulateGrad]
	140492159364304 -> 140492159363584
	140492159364304 [label=TBackward0]
	140492159363824 -> 140492159364304
	140493489606384 [label="stages.2.blocks.4.mlp.fc2.weight
 (160, 640)" fillcolor=lightblue]
	140493489606384 -> 140492159363824
	140492159363824 [label=AccumulateGrad]
	140492159362144 -> 140492159362432
	140493489606464 [label="stages.2.blocks.4.mlp.fc2.bias
 (160)" fillcolor=lightblue]
	140493489606464 -> 140492159362144
	140492159362144 [label=AccumulateGrad]
	140492159275984 -> 140492159275888
	140492159275984 [label=PermuteBackward0]
	140492159276224 -> 140492159275984
	140492159276224 [label=MulBackward0]
	140492159364448 -> 140492159276224
	140493489795136 [label="stages.2.blocks.5.gamma
 (160)" fillcolor=lightblue]
	140493489795136 -> 140492159364448
	140492159364448 [label=AccumulateGrad]
	140492159363536 -> 140492159276224
	140492159363536 [label=AddBackward0]
	140492159364016 -> 140492159363536
	140492159364016 [label=UnsafeViewBackward0]
	140492159364352 -> 140492159364016
	140492159364352 [label=MmBackward0]
	140492975477088 -> 140492159364352
	140492975477088 [label=ReshapeAliasBackward0]
	140492975477232 -> 140492975477088
	140492975477232 [label=GeluBackward0]
	140492975477328 -> 140492975477232
	140492975477328 [label=AddBackward0]
	140492975477424 -> 140492975477328
	140492975477424 [label=UnsafeViewBackward0]
	140492975477568 -> 140492975477424
	140492975477568 [label=MmBackward0]
	140492975477664 -> 140492975477568
	140492975477664 [label=ReshapeAliasBackward0]
	140492975477808 -> 140492975477664
	140492975477808 [label=NativeLayerNormBackward0]
	140492975477904 -> 140492975477808
	140492975477904 [label=PermuteBackward0]
	140492975478096 -> 140492975477904
	140492975478096 [label=ConvolutionBackward0]
	140492159276032 -> 140492975478096
	140492975478192 -> 140492975478096
	140493489795216 [label="stages.2.blocks.5.conv_dw.weight
 (160, 1, 7, 7)" fillcolor=lightblue]
	140493489795216 -> 140492975478192
	140492975478192 [label=AccumulateGrad]
	140492975478144 -> 140492975478096
	140493489795296 [label="stages.2.blocks.5.conv_dw.bias
 (160)" fillcolor=lightblue]
	140493489795296 -> 140492975478144
	140492975478144 [label=AccumulateGrad]
	140492975477856 -> 140492975477808
	140493489795376 [label="stages.2.blocks.5.norm.weight
 (160)" fillcolor=lightblue]
	140493489795376 -> 140492975477856
	140492975477856 [label=AccumulateGrad]
	140492975477712 -> 140492975477808
	140493489795456 [label="stages.2.blocks.5.norm.bias
 (160)" fillcolor=lightblue]
	140493489795456 -> 140492975477712
	140492975477712 [label=AccumulateGrad]
	140492975477616 -> 140492975477568
	140492975477616 [label=TBackward0]
	140492975478048 -> 140492975477616
	140493489795536 [label="stages.2.blocks.5.mlp.fc1.weight
 (640, 160)" fillcolor=lightblue]
	140493489795536 -> 140492975478048
	140492975478048 [label=AccumulateGrad]
	140492975477376 -> 140492975477328
	140493489795616 [label="stages.2.blocks.5.mlp.fc1.bias
 (640)" fillcolor=lightblue]
	140493489795616 -> 140492975477376
	140492975477376 [label=AccumulateGrad]
	140492975477136 -> 140492159364352
	140492975477136 [label=TBackward0]
	140492975476800 -> 140492975477136
	140493489795696 [label="stages.2.blocks.5.mlp.fc2.weight
 (160, 640)" fillcolor=lightblue]
	140493489795696 -> 140492975476800
	140492975476800 [label=AccumulateGrad]
	140492159363248 -> 140492159363536
	140493489795776 [label="stages.2.blocks.5.mlp.fc2.bias
 (160)" fillcolor=lightblue]
	140493489795776 -> 140492159363248
	140492159363248 [label=AccumulateGrad]
	140492159275840 -> 140492159275744
	140492159275840 [label=PermuteBackward0]
	140492159276080 -> 140492159275840
	140492159276080 [label=MulBackward0]
	140492159364640 -> 140492159276080
	140493489795936 [label="stages.2.blocks.6.gamma
 (160)" fillcolor=lightblue]
	140493489795936 -> 140492159364640
	140492159364640 [label=AccumulateGrad]
	140492159364688 -> 140492159276080
	140492159364688 [label=AddBackward0]
	140492975477280 -> 140492159364688
	140492975477280 [label=UnsafeViewBackward0]
	140492975477520 -> 140492975477280
	140492975477520 [label=MmBackward0]
	140492975478000 -> 140492975477520
	140492975478000 [label=ReshapeAliasBackward0]
	140492975478336 -> 140492975478000
	140492975478336 [label=GeluBackward0]
	140492975478432 -> 140492975478336
	140492975478432 [label=AddBackward0]
	140492975478528 -> 140492975478432
	140492975478528 [label=UnsafeViewBackward0]
	140492975478672 -> 140492975478528
	140492975478672 [label=MmBackward0]
	140492975478768 -> 140492975478672
	140492975478768 [label=ReshapeAliasBackward0]
	140492975478912 -> 140492975478768
	140492975478912 [label=NativeLayerNormBackward0]
	140492975479008 -> 140492975478912
	140492975479008 [label=PermuteBackward0]
	140492975479200 -> 140492975479008
	140492975479200 [label=ConvolutionBackward0]
	140492159275888 -> 140492975479200
	140492975479296 -> 140492975479200
	140493489796016 [label="stages.2.blocks.6.conv_dw.weight
 (160, 1, 7, 7)" fillcolor=lightblue]
	140493489796016 -> 140492975479296
	140492975479296 [label=AccumulateGrad]
	140492975479248 -> 140492975479200
	140493489796096 [label="stages.2.blocks.6.conv_dw.bias
 (160)" fillcolor=lightblue]
	140493489796096 -> 140492975479248
	140492975479248 [label=AccumulateGrad]
	140492975478960 -> 140492975478912
	140493489796176 [label="stages.2.blocks.6.norm.weight
 (160)" fillcolor=lightblue]
	140493489796176 -> 140492975478960
	140492975478960 [label=AccumulateGrad]
	140492975478816 -> 140492975478912
	140493489796256 [label="stages.2.blocks.6.norm.bias
 (160)" fillcolor=lightblue]
	140493489796256 -> 140492975478816
	140492975478816 [label=AccumulateGrad]
	140492975478720 -> 140492975478672
	140492975478720 [label=TBackward0]
	140492975479152 -> 140492975478720
	140493489796336 [label="stages.2.blocks.6.mlp.fc1.weight
 (640, 160)" fillcolor=lightblue]
	140493489796336 -> 140492975479152
	140492975479152 [label=AccumulateGrad]
	140492975478480 -> 140492975478432
	140493489796416 [label="stages.2.blocks.6.mlp.fc1.bias
 (640)" fillcolor=lightblue]
	140493489796416 -> 140492975478480
	140492975478480 [label=AccumulateGrad]
	140492975478240 -> 140492975477520
	140492975478240 [label=TBackward0]
	140492975477760 -> 140492975478240
	140493489796496 [label="stages.2.blocks.6.mlp.fc2.weight
 (160, 640)" fillcolor=lightblue]
	140493489796496 -> 140492975477760
	140492975477760 [label=AccumulateGrad]
	140492975476848 -> 140492159364688
	140493489796576 [label="stages.2.blocks.6.mlp.fc2.bias
 (160)" fillcolor=lightblue]
	140493489796576 -> 140492975476848
	140492975476848 [label=AccumulateGrad]
	140492159275696 -> 140492159275600
	140492159275696 [label=PermuteBackward0]
	140492159361328 -> 140492159275696
	140492159361328 [label=MulBackward0]
	140492159275792 -> 140492159361328
	140493489796736 [label="stages.2.blocks.7.gamma
 (160)" fillcolor=lightblue]
	140493489796736 -> 140492159275792
	140492159275792 [label=AccumulateGrad]
	140492975478384 -> 140492159361328
	140492975478384 [label=AddBackward0]
	140492975477952 -> 140492975478384
	140492975477952 [label=UnsafeViewBackward0]
	140492975478624 -> 140492975477952
	140492975478624 [label=MmBackward0]
	140492975479104 -> 140492975478624
	140492975479104 [label=ReshapeAliasBackward0]
	140492975479440 -> 140492975479104
	140492975479440 [label=GeluBackward0]
	140492975479536 -> 140492975479440
	140492975479536 [label=AddBackward0]
	140492975479632 -> 140492975479536
	140492975479632 [label=UnsafeViewBackward0]
	140492975479776 -> 140492975479632
	140492975479776 [label=MmBackward0]
	140492975479872 -> 140492975479776
	140492975479872 [label=ReshapeAliasBackward0]
	140492975480016 -> 140492975479872
	140492975480016 [label=NativeLayerNormBackward0]
	140492975480112 -> 140492975480016
	140492975480112 [label=PermuteBackward0]
	140492975480304 -> 140492975480112
	140492975480304 [label=ConvolutionBackward0]
	140492159275744 -> 140492975480304
	140492975480400 -> 140492975480304
	140493489796816 [label="stages.2.blocks.7.conv_dw.weight
 (160, 1, 7, 7)" fillcolor=lightblue]
	140493489796816 -> 140492975480400
	140492975480400 [label=AccumulateGrad]
	140492975480352 -> 140492975480304
	140493489796896 [label="stages.2.blocks.7.conv_dw.bias
 (160)" fillcolor=lightblue]
	140493489796896 -> 140492975480352
	140492975480352 [label=AccumulateGrad]
	140492975480064 -> 140492975480016
	140493489796976 [label="stages.2.blocks.7.norm.weight
 (160)" fillcolor=lightblue]
	140493489796976 -> 140492975480064
	140492975480064 [label=AccumulateGrad]
	140492975479920 -> 140492975480016
	140493489797056 [label="stages.2.blocks.7.norm.bias
 (160)" fillcolor=lightblue]
	140493489797056 -> 140492975479920
	140492975479920 [label=AccumulateGrad]
	140492975479824 -> 140492975479776
	140492975479824 [label=TBackward0]
	140492975480256 -> 140492975479824
	140493489797136 [label="stages.2.blocks.7.mlp.fc1.weight
 (640, 160)" fillcolor=lightblue]
	140493489797136 -> 140492975480256
	140492975480256 [label=AccumulateGrad]
	140492975479584 -> 140492975479536
	140493489797216 [label="stages.2.blocks.7.mlp.fc1.bias
 (640)" fillcolor=lightblue]
	140493489797216 -> 140492975479584
	140492975479584 [label=AccumulateGrad]
	140492975479344 -> 140492975478624
	140492975479344 [label=TBackward0]
	140492975478864 -> 140492975479344
	140493489797296 [label="stages.2.blocks.7.mlp.fc2.weight
 (160, 640)" fillcolor=lightblue]
	140493489797296 -> 140492975478864
	140492975478864 [label=AccumulateGrad]
	140492975477184 -> 140492975478384
	140493489797376 [label="stages.2.blocks.7.mlp.fc2.bias
 (160)" fillcolor=lightblue]
	140493489797376 -> 140492975477184
	140492975477184 [label=AccumulateGrad]
	140492159275552 -> 140492159275504
	140492159275552 [label=PermuteBackward0]
	140492159275936 -> 140492159275552
	140492159275936 [label=MulBackward0]
	140492975479488 -> 140492159275936
	140493489798176 [label="stages.2.blocks.8.gamma
 (160)" fillcolor=lightblue]
	140493489798176 -> 140492975479488
	140492975479488 [label=AccumulateGrad]
	140492975478576 -> 140492159275936
	140492975478576 [label=AddBackward0]
	140492975479056 -> 140492975478576
	140492975479056 [label=UnsafeViewBackward0]
	140492975479728 -> 140492975479056
	140492975479728 [label=MmBackward0]
	140492975480208 -> 140492975479728
	140492975480208 [label=ReshapeAliasBackward0]
	140492975480544 -> 140492975480208
	140492975480544 [label=GeluBackward0]
	140492975480640 -> 140492975480544
	140492975480640 [label=AddBackward0]
	140492975480736 -> 140492975480640
	140492975480736 [label=UnsafeViewBackward0]
	140492975480784 -> 140492975480736
	140492975480784 [label=MmBackward0]
	140492975505616 -> 140492975480784
	140492975505616 [label=ReshapeAliasBackward0]
	140492975505760 -> 140492975505616
	140492975505760 [label=NativeLayerNormBackward0]
	140492975505856 -> 140492975505760
	140492975505856 [label=ReshapeAliasBackward0]
	140492975506048 -> 140492975505856
	140492975506048 [label=AddBackward0]
	140492975506144 -> 140492975506048
	140492975506144 [label=PermuteBackward0]
	140492975506288 -> 140492975506144
	140492975506288 [label=ReshapeAliasBackward0]
	140492975506384 -> 140492975506288
	140492975506384 [label=CatBackward0]
	140492975506480 -> 140492975506384
	140492975506480 [label=ConvolutionBackward0]
	140492975506192 -> 140492975506480
	140492975506192 [label=SplitBackward0]
	140492159275600 -> 140492975506192
	140492975506672 -> 140492975506480
	140493489797616 [label="stages.2.blocks.8.convs.0.weight
 (54, 1, 3, 3)" fillcolor=lightblue]
	140493489797616 -> 140492975506672
	140492975506672 [label=AccumulateGrad]
	140492975506624 -> 140492975506480
	140493489797696 [label="stages.2.blocks.8.convs.0.bias
 (54)" fillcolor=lightblue]
	140493489797696 -> 140492975506624
	140492975506624 [label=AccumulateGrad]
	140492975506432 -> 140492975506384
	140492975506432 [label=ConvolutionBackward0]
	140492975506768 -> 140492975506432
	140492975506768 [label=AddBackward0]
	140492975506480 -> 140492975506768
	140492975506192 -> 140492975506768
	140492975506816 -> 140492975506432
	140493489797856 [label="stages.2.blocks.8.convs.1.weight
 (54, 1, 3, 3)" fillcolor=lightblue]
	140493489797856 -> 140492975506816
	140492975506816 [label=AccumulateGrad]
	140492975506576 -> 140492975506432
	140493489797936 [label="stages.2.blocks.8.convs.1.bias
 (54)" fillcolor=lightblue]
	140493489797936 -> 140492975506576
	140492975506576 [label=AccumulateGrad]
	140492975506192 -> 140492975506384
	140492975506096 -> 140492975506048
	140492975506096 [label=MulBackward0]
	140492975506528 -> 140492975506096
	140493489797776 [label="stages.2.blocks.8.gamma_xca
 (160)" fillcolor=lightblue]
	140493489797776 -> 140492975506528
	140492975506528 [label=AccumulateGrad]
	140492975506336 -> 140492975506096
	140492975506336 [label=AddBackward0]
	140492975506720 -> 140492975506336
	140492975506720 [label=UnsafeViewBackward0]
	140492975507056 -> 140492975506720
	140492975507056 [label=MmBackward0]
	140492975507152 -> 140492975507056
	140492975507152 [label=ReshapeAliasBackward0]
	140492975507296 -> 140492975507152
	140492975507296 [label=ReshapeAliasBackward0]
	140492975507392 -> 140492975507296
	140492975507392 [label=PermuteBackward0]
	140492975507488 -> 140492975507392
	140492975507488 [label=UnsafeViewBackward0]
	140492975507584 -> 140492975507488
	140492975507584 [label=BmmBackward0]
	140492975507680 -> 140492975507584
	140492975507680 [label=ReshapeAliasBackward0]
	140492975507824 -> 140492975507680
	140492975507824 [label=ExpandBackward0]
	140492975507920 -> 140492975507824
	140492975507920 [label=SoftmaxBackward0]
	140492975508016 -> 140492975507920
	140492975508016 [label=MulBackward0]
	140492975508112 -> 140492975508016
	140492975508112 [label=UnsafeViewBackward0]
	140492975508256 -> 140492975508112
	140492975508256 [label=BmmBackward0]
	140492975508352 -> 140492975508256
	140492975508352 [label=ReshapeAliasBackward0]
	140492975508496 -> 140492975508352
	140492975508496 [label=ExpandBackward0]
	140492975508592 -> 140492975508496
	140492975508592 [label=DivBackward0]
	140492975508688 -> 140492975508592
	140492975508688 [label=UnbindBackward0]
	140492975508832 -> 140492975508688
	140492975508832 [label=PermuteBackward0]
	140492975508928 -> 140492975508832
	140492975508928 [label=ReshapeAliasBackward0]
	140492975509024 -> 140492975508928
	140492975509024 [label=ViewBackward0]
	140492975509120 -> 140492975509024
	140492975509120 [label=AddmmBackward0]
	140492975509216 -> 140492975509120
	140493489798416 [label="stages.2.blocks.8.xca.qkv.bias
 (480)" fillcolor=lightblue]
	140493489798416 -> 140492975509216
	140492975509216 [label=AccumulateGrad]
	140492975509168 -> 140492975509120
	140492975509168 [label=ViewBackward0]
	140492975509312 -> 140492975509168
	140492975509312 [label=NativeLayerNormBackward0]
	140492975506144 -> 140492975509312
	140492975509456 -> 140492975509312
	140493489798016 [label="stages.2.blocks.8.norm_xca.weight
 (160)" fillcolor=lightblue]
	140493489798016 -> 140492975509456
	140492975509456 [label=AccumulateGrad]
	140492975509408 -> 140492975509312
	140493489798096 [label="stages.2.blocks.8.norm_xca.bias
 (160)" fillcolor=lightblue]
	140493489798096 -> 140492975509408
	140492975509408 [label=AccumulateGrad]
	140492975508736 -> 140492975509120
	140492975508736 [label=TBackward0]
	140492975509360 -> 140492975508736
	140493489798336 [label="stages.2.blocks.8.xca.qkv.weight
 (480, 160)" fillcolor=lightblue]
	140493489798336 -> 140492975509360
	140492975509360 [label=AccumulateGrad]
	140492975508640 -> 140492975508592
	140492975508640 [label=ExpandBackward0]
	140492975508976 -> 140492975508640
	140492975508976 [label=ClampMinBackward0]
	140492975509264 -> 140492975508976
	140492975509264 [label=NormBackward1]
	140492975508688 -> 140492975509264
	140492975508304 -> 140492975508256
	140492975508304 [label=ReshapeAliasBackward0]
	140492975508400 -> 140492975508304
	140492975508400 [label=ExpandBackward0]
	140492975509072 -> 140492975508400
	140492975509072 [label=TransposeBackward0]
	140492975508784 -> 140492975509072
	140492975508784 [label=DivBackward0]
	140492975508688 -> 140492975508784
	140492975521952 -> 140492975508784
	140492975521952 [label=ExpandBackward0]
	140492975522048 -> 140492975521952
	140492975522048 [label=ClampMinBackward0]
	140492975522144 -> 140492975522048
	140492975522144 [label=NormBackward1]
	140492975508688 -> 140492975522144
	140492975508064 -> 140492975508016
	140493489798256 [label="stages.2.blocks.8.xca.temperature
 (8, 1, 1)" fillcolor=lightblue]
	140493489798256 -> 140492975508064
	140492975508064 [label=AccumulateGrad]
	140492975507632 -> 140492975507584
	140492975507632 [label=ReshapeAliasBackward0]
	140492975507968 -> 140492975507632
	140492975507968 [label=ExpandBackward0]
	140492975508688 -> 140492975507968
	140492975507104 -> 140492975507056
	140492975507104 [label=TBackward0]
	140492975507440 -> 140492975507104
	140493489798496 [label="stages.2.blocks.8.xca.proj.weight
 (160, 160)" fillcolor=lightblue]
	140493489798496 -> 140492975507440
	140492975507440 [label=AccumulateGrad]
	140492975506960 -> 140492975506336
	140493489798576 [label="stages.2.blocks.8.xca.proj.bias
 (160)" fillcolor=lightblue]
	140493489798576 -> 140492975506960
	140492975506960 [label=AccumulateGrad]
	140492975505808 -> 140492975505760
	140493489798656 [label="stages.2.blocks.8.norm.weight
 (160)" fillcolor=lightblue]
	140493489798656 -> 140492975505808
	140492975505808 [label=AccumulateGrad]
	140492975505664 -> 140492975505760
	140493489798736 [label="stages.2.blocks.8.norm.bias
 (160)" fillcolor=lightblue]
	140493489798736 -> 140492975505664
	140492975505664 [label=AccumulateGrad]
	140492975505568 -> 140492975480784
	140492975505568 [label=TBackward0]
	140492975506000 -> 140492975505568
	140493489798816 [label="stages.2.blocks.8.mlp.fc1.weight
 (640, 160)" fillcolor=lightblue]
	140493489798816 -> 140492975506000
	140492975506000 [label=AccumulateGrad]
	140492975480688 -> 140492975480640
	140493489798896 [label="stages.2.blocks.8.mlp.fc1.bias
 (640)" fillcolor=lightblue]
	140493489798896 -> 140492975480688
	140492975480688 [label=AccumulateGrad]
	140492975480448 -> 140492975479728
	140492975480448 [label=TBackward0]
	140492975479968 -> 140492975480448
	140493489798976 [label="stages.2.blocks.8.mlp.fc2.weight
 (160, 640)" fillcolor=lightblue]
	140493489798976 -> 140492975479968
	140492975479968 [label=AccumulateGrad]
	140492975478288 -> 140492975478576
	140493489799056 [label="stages.2.blocks.8.mlp.fc2.bias
 (160)" fillcolor=lightblue]
	140493489799056 -> 140492975478288
	140492975478288 [label=AccumulateGrad]
	140492159275264 -> 140492159275216
	140493494673552 [label="stages.3.downsample.0.weight
 (160)" fillcolor=lightblue]
	140493494673552 -> 140492159275264
	140492159275264 [label=AccumulateGrad]
	140492159275120 -> 140492159275216
	140493494673632 [label="stages.3.downsample.0.bias
 (160)" fillcolor=lightblue]
	140493494673632 -> 140492159275120
	140492159275120 [label=AccumulateGrad]
	140492154511264 -> 140492154511168
	140493494673792 [label="stages.3.downsample.1.weight
 (304, 160, 2, 2)" fillcolor=lightblue]
	140493494673792 -> 140492154511264
	140492154511264 [label=AccumulateGrad]
	140492154511216 -> 140492154511168
	140493494673872 [label="stages.3.downsample.1.bias
 (304)" fillcolor=lightblue]
	140493494673872 -> 140492154511216
	140492154511216 [label=AccumulateGrad]
	140492154511120 -> 140492154511024
	140492154511120 [label=PermuteBackward0]
	140492159275360 -> 140492154511120
	140492159275360 [label=MulBackward0]
	140492159275408 -> 140492159275360
	140493494673952 [label="stages.3.blocks.0.gamma
 (304)" fillcolor=lightblue]
	140493494673952 -> 140492159275408
	140492159275408 [label=AccumulateGrad]
	140492159275456 -> 140492159275360
	140492159275456 [label=AddBackward0]
	140492159275648 -> 140492159275456
	140492159275648 [label=UnsafeViewBackward0]
	140492975479392 -> 140492159275648
	140492975479392 [label=MmBackward0]
	140492975480496 -> 140492975479392
	140492975480496 [label=ReshapeAliasBackward0]
	140492975506240 -> 140492975480496
	140492975506240 [label=GeluBackward0]
	140492975505712 -> 140492975506240
	140492975505712 [label=AddBackward0]
	140492975507344 -> 140492975505712
	140492975507344 [label=UnsafeViewBackward0]
	140492975507200 -> 140492975507344
	140492975507200 [label=MmBackward0]
	140492975507248 -> 140492975507200
	140492975507248 [label=ReshapeAliasBackward0]
	140492975507728 -> 140492975507248
	140492975507728 [label=NativeLayerNormBackward0]
	140492975508160 -> 140492975507728
	140492975508160 [label=PermuteBackward0]
	140492975508448 -> 140492975508160
	140492975508448 [label=ConvolutionBackward0]
	140492154511168 -> 140492975508448
	140492975522096 -> 140492975508448
	140493494674032 [label="stages.3.blocks.0.conv_dw.weight
 (304, 1, 9, 9)" fillcolor=lightblue]
	140493494674032 -> 140492975522096
	140492975522096 [label=AccumulateGrad]
	140492975521856 -> 140492975508448
	140493494674112 [label="stages.3.blocks.0.conv_dw.bias
 (304)" fillcolor=lightblue]
	140493494674112 -> 140492975521856
	140492975521856 [label=AccumulateGrad]
	140492975507776 -> 140492975507728
	140493494674192 [label="stages.3.blocks.0.norm.weight
 (304)" fillcolor=lightblue]
	140493494674192 -> 140492975507776
	140492975507776 [label=AccumulateGrad]
	140492975507872 -> 140492975507728
	140493494674272 [label="stages.3.blocks.0.norm.bias
 (304)" fillcolor=lightblue]
	140493494674272 -> 140492975507872
	140492975507872 [label=AccumulateGrad]
	140492975507536 -> 140492975507200
	140492975507536 [label=TBackward0]
	140492975508880 -> 140492975507536
	140493494674352 [label="stages.3.blocks.0.mlp.fc1.weight
 (1216, 304)" fillcolor=lightblue]
	140493494674352 -> 140492975508880
	140492975508880 [label=AccumulateGrad]
	140492975506912 -> 140492975505712
	140493494674432 [label="stages.3.blocks.0.mlp.fc1.bias
 (1216)" fillcolor=lightblue]
	140493494674432 -> 140492975506912
	140492975506912 [label=AccumulateGrad]
	140492975480592 -> 140492975479392
	140492975480592 [label=TBackward0]
	140492975505904 -> 140492975480592
	140493494674512 [label="stages.3.blocks.0.mlp.fc2.weight
 (304, 1216)" fillcolor=lightblue]
	140493494674512 -> 140492975505904
	140492975505904 [label=AccumulateGrad]
	140492975477472 -> 140492159275456
	140493494674592 [label="stages.3.blocks.0.mlp.fc2.bias
 (304)" fillcolor=lightblue]
	140493494674592 -> 140492975477472
	140492975477472 [label=AccumulateGrad]
	140492154510976 -> 140492154510880
	140492154510976 [label=PermuteBackward0]
	140492159275072 -> 140492154510976
	140492159275072 [label=MulBackward0]
	140492975480160 -> 140492159275072
	140493494674752 [label="stages.3.blocks.1.gamma
 (304)" fillcolor=lightblue]
	140493494674752 -> 140492975480160
	140492975480160 [label=AccumulateGrad]
	140492975479680 -> 140492159275072
	140492975479680 [label=AddBackward0]
	140492975505952 -> 140492975479680
	140492975505952 [label=UnsafeViewBackward0]
	140492975507008 -> 140492975505952
	140492975507008 [label=MmBackward0]
	140492975508544 -> 140492975507008
	140492975508544 [label=ReshapeAliasBackward0]
	140492975522000 -> 140492975508544
	140492975522000 [label=GeluBackward0]
	140492975522336 -> 140492975522000
	140492975522336 [label=AddBackward0]
	140492975522432 -> 140492975522336
	140492975522432 [label=UnsafeViewBackward0]
	140492975522576 -> 140492975522432
	140492975522576 [label=MmBackward0]
	140492975522672 -> 140492975522576
	140492975522672 [label=ReshapeAliasBackward0]
	140492975522816 -> 140492975522672
	140492975522816 [label=NativeLayerNormBackward0]
	140492975522912 -> 140492975522816
	140492975522912 [label=PermuteBackward0]
	140492975523104 -> 140492975522912
	140492975523104 [label=ConvolutionBackward0]
	140492154511024 -> 140492975523104
	140492975523200 -> 140492975523104
	140493494674832 [label="stages.3.blocks.1.conv_dw.weight
 (304, 1, 9, 9)" fillcolor=lightblue]
	140493494674832 -> 140492975523200
	140492975523200 [label=AccumulateGrad]
	140492975523152 -> 140492975523104
	140493494674912 [label="stages.3.blocks.1.conv_dw.bias
 (304)" fillcolor=lightblue]
	140493494674912 -> 140492975523152
	140492975523152 [label=AccumulateGrad]
	140492975522864 -> 140492975522816
	140493494674992 [label="stages.3.blocks.1.norm.weight
 (304)" fillcolor=lightblue]
	140493494674992 -> 140492975522864
	140492975522864 [label=AccumulateGrad]
	140492975522720 -> 140492975522816
	140493494675072 [label="stages.3.blocks.1.norm.bias
 (304)" fillcolor=lightblue]
	140493494675072 -> 140492975522720
	140492975522720 [label=AccumulateGrad]
	140492975522624 -> 140492975522576
	140492975522624 [label=TBackward0]
	140492975523056 -> 140492975522624
	140493494675152 [label="stages.3.blocks.1.mlp.fc1.weight
 (1216, 304)" fillcolor=lightblue]
	140493494675152 -> 140492975523056
	140492975523056 [label=AccumulateGrad]
	140492975522384 -> 140492975522336
	140493494675232 [label="stages.3.blocks.1.mlp.fc1.bias
 (1216)" fillcolor=lightblue]
	140493494675232 -> 140492975522384
	140492975522384 [label=AccumulateGrad]
	140492975508208 -> 140492975507008
	140492975508208 [label=TBackward0]
	140492975521904 -> 140492975508208
	140493494675312 [label="stages.3.blocks.1.mlp.fc2.weight
 (304, 1216)" fillcolor=lightblue]
	140493494675312 -> 140492975521904
	140492975521904 [label=AccumulateGrad]
	140492975505472 -> 140492975479680
	140493494675392 [label="stages.3.blocks.1.mlp.fc2.bias
 (304)" fillcolor=lightblue]
	140493494675392 -> 140492975505472
	140492975505472 [label=AccumulateGrad]
	140492154510832 -> 140492154510784
	140492154510832 [label=PermuteBackward0]
	140492154511072 -> 140492154510832
	140492154511072 [label=MulBackward0]
	140492975505520 -> 140492154511072
	140493494676432 [label="stages.3.blocks.2.gamma
 (304)" fillcolor=lightblue]
	140493494676432 -> 140492975505520
	140492975505520 [label=AccumulateGrad]
	140492975506864 -> 140492154511072
	140492975506864 [label=AddBackward0]
	140492975522288 -> 140492975506864
	140492975522288 [label=UnsafeViewBackward0]
	140492975522528 -> 140492975522288
	140492975522528 [label=MmBackward0]
	140492975523008 -> 140492975522528
	140492975523008 [label=ReshapeAliasBackward0]
	140492975523344 -> 140492975523008
	140492975523344 [label=GeluBackward0]
	140492975523440 -> 140492975523344
	140492975523440 [label=AddBackward0]
	140492975523536 -> 140492975523440
	140492975523536 [label=UnsafeViewBackward0]
	140492975523680 -> 140492975523536
	140492975523680 [label=MmBackward0]
	140492975523776 -> 140492975523680
	140492975523776 [label=ReshapeAliasBackward0]
	140492975523920 -> 140492975523776
	140492975523920 [label=NativeLayerNormBackward0]
	140492975524016 -> 140492975523920
	140492975524016 [label=ReshapeAliasBackward0]
	140492975524208 -> 140492975524016
	140492975524208 [label=AddBackward0]
	140492975524304 -> 140492975524208
	140492975524304 [label=PermuteBackward0]
	140492975524448 -> 140492975524304
	140492975524448 [label=ReshapeAliasBackward0]
	140492975524544 -> 140492975524448
	140492975524544 [label=CatBackward0]
	140492975524640 -> 140492975524544
	140492975524640 [label=ConvolutionBackward0]
	140492975524688 -> 140492975524640
	140492975524688 [label=SplitBackward0]
	140492154510880 -> 140492975524688
	140492975524880 -> 140492975524640
	140493494675632 [label="stages.3.blocks.2.convs.0.weight
 (76, 1, 3, 3)" fillcolor=lightblue]
	140493494675632 -> 140492975524880
	140492975524880 [label=AccumulateGrad]
	140492975524832 -> 140492975524640
	140493494675712 [label="stages.3.blocks.2.convs.0.bias
 (76)" fillcolor=lightblue]
	140493494675712 -> 140492975524832
	140492975524832 [label=AccumulateGrad]
	140492975524592 -> 140492975524544
	140492975524592 [label=ConvolutionBackward0]
	140492975524976 -> 140492975524592
	140492975524976 [label=AddBackward0]
	140492975524640 -> 140492975524976
	140492975524688 -> 140492975524976
	140492975525024 -> 140492975524592
	140493494675872 [label="stages.3.blocks.2.convs.1.weight
 (76, 1, 3, 3)" fillcolor=lightblue]
	140493494675872 -> 140492975525024
	140492975525024 [label=AccumulateGrad]
	140492975524784 -> 140492975524592
	140493494675952 [label="stages.3.blocks.2.convs.1.bias
 (76)" fillcolor=lightblue]
	140493494675952 -> 140492975524784
	140492975524784 [label=AccumulateGrad]
	140492975524352 -> 140492975524544
	140492975524352 [label=ConvolutionBackward0]
	140492975525120 -> 140492975524352
	140492975525120 [label=AddBackward0]
	140492975524592 -> 140492975525120
	140492975524688 -> 140492975525120
	140492975525168 -> 140492975524352
	140493494676112 [label="stages.3.blocks.2.convs.2.weight
 (76, 1, 3, 3)" fillcolor=lightblue]
	140493494676112 -> 140492975525168
	140492975525168 [label=AccumulateGrad]
	140492975524928 -> 140492975524352
	140493494676192 [label="stages.3.blocks.2.convs.2.bias
 (76)" fillcolor=lightblue]
	140493494676192 -> 140492975524928
	140492975524928 [label=AccumulateGrad]
	140492975524688 -> 140492975524544
	140492975524256 -> 140492975524208
	140492975524256 [label=MulBackward0]
	140492975524736 -> 140492975524256
	140493494676032 [label="stages.3.blocks.2.gamma_xca
 (304)" fillcolor=lightblue]
	140493494676032 -> 140492975524736
	140492975524736 [label=AccumulateGrad]
	140492975524496 -> 140492975524256
	140492975524496 [label=AddBackward0]
	140492975525072 -> 140492975524496
	140492975525072 [label=UnsafeViewBackward0]
	140492975525408 -> 140492975525072
	140492975525408 [label=MmBackward0]
	140492975525504 -> 140492975525408
	140492975525504 [label=ReshapeAliasBackward0]
	140492975525648 -> 140492975525504
	140492975525648 [label=ReshapeAliasBackward0]
	140492975525744 -> 140492975525648
	140492975525744 [label=PermuteBackward0]
	140492975525840 -> 140492975525744
	140492975525840 [label=UnsafeViewBackward0]
	140492975525552 -> 140492975525840
	140492975525552 [label=BmmBackward0]
	140492975554768 -> 140492975525552
	140492975554768 [label=ReshapeAliasBackward0]
	140492975554912 -> 140492975554768
	140492975554912 [label=ExpandBackward0]
	140492975555008 -> 140492975554912
	140492975555008 [label=SoftmaxBackward0]
	140492975555104 -> 140492975555008
	140492975555104 [label=MulBackward0]
	140492975555200 -> 140492975555104
	140492975555200 [label=UnsafeViewBackward0]
	140492975555344 -> 140492975555200
	140492975555344 [label=BmmBackward0]
	140492975555440 -> 140492975555344
	140492975555440 [label=ReshapeAliasBackward0]
	140492975555584 -> 140492975555440
	140492975555584 [label=ExpandBackward0]
	140492975555680 -> 140492975555584
	140492975555680 [label=DivBackward0]
	140492975555776 -> 140492975555680
	140492975555776 [label=UnbindBackward0]
	140492975555920 -> 140492975555776
	140492975555920 [label=PermuteBackward0]
	140492975556016 -> 140492975555920
	140492975556016 [label=ReshapeAliasBackward0]
	140492975556112 -> 140492975556016
	140492975556112 [label=ViewBackward0]
	140492975556208 -> 140492975556112
	140492975556208 [label=AddmmBackward0]
	140492975556304 -> 140492975556208
	140493494676672 [label="stages.3.blocks.2.xca.qkv.bias
 (912)" fillcolor=lightblue]
	140493494676672 -> 140492975556304
	140492975556304 [label=AccumulateGrad]
	140492975556256 -> 140492975556208
	140492975556256 [label=ViewBackward0]
	140492975556400 -> 140492975556256
	140492975556400 [label=NativeLayerNormBackward0]
	140492975524304 -> 140492975556400
	140492975556592 -> 140492975556400
	140493494676272 [label="stages.3.blocks.2.norm_xca.weight
 (304)" fillcolor=lightblue]
	140493494676272 -> 140492975556592
	140492975556592 [label=AccumulateGrad]
	140492975556544 -> 140492975556400
	140493494676352 [label="stages.3.blocks.2.norm_xca.bias
 (304)" fillcolor=lightblue]
	140493494676352 -> 140492975556544
	140492975556544 [label=AccumulateGrad]
	140492975555824 -> 140492975556208
	140492975555824 [label=TBackward0]
	140492975556640 -> 140492975555824
	140493494676592 [label="stages.3.blocks.2.xca.qkv.weight
 (912, 304)" fillcolor=lightblue]
	140493494676592 -> 140492975556640
	140492975556640 [label=AccumulateGrad]
	140492975555728 -> 140492975555680
	140492975555728 [label=ExpandBackward0]
	140492975556064 -> 140492975555728
	140492975556064 [label=ClampMinBackward0]
	140492975556352 -> 140492975556064
	140492975556352 [label=NormBackward1]
	140492975555776 -> 140492975556352
	140492975555392 -> 140492975555344
	140492975555392 [label=ReshapeAliasBackward0]
	140492975555488 -> 140492975555392
	140492975555488 [label=ExpandBackward0]
	140492975556160 -> 140492975555488
	140492975556160 [label=TransposeBackward0]
	140492975556496 -> 140492975556160
	140492975556496 [label=DivBackward0]
	140492975555776 -> 140492975556496
	140492975556688 -> 140492975556496
	140492975556688 [label=ExpandBackward0]
	140492975556784 -> 140492975556688
	140492975556784 [label=ClampMinBackward0]
	140492975556880 -> 140492975556784
	140492975556880 [label=NormBackward1]
	140492975555776 -> 140492975556880
	140492975555152 -> 140492975555104
	140493494676512 [label="stages.3.blocks.2.xca.temperature
 (8, 1, 1)" fillcolor=lightblue]
	140493494676512 -> 140492975555152
	140492975555152 [label=AccumulateGrad]
	140492975554720 -> 140492975525552
	140492975554720 [label=ReshapeAliasBackward0]
	140492975555056 -> 140492975554720
	140492975555056 [label=ExpandBackward0]
	140492975555776 -> 140492975555056
	140492975525456 -> 140492975525408
	140492975525456 [label=TBackward0]
	140492975525792 -> 140492975525456
	140493494676752 [label="stages.3.blocks.2.xca.proj.weight
 (304, 304)" fillcolor=lightblue]
	140493494676752 -> 140492975525792
	140492975525792 [label=AccumulateGrad]
	140492975525312 -> 140492975524496
	140493494676832 [label="stages.3.blocks.2.xca.proj.bias
 (304)" fillcolor=lightblue]
	140493494676832 -> 140492975525312
	140492975525312 [label=AccumulateGrad]
	140492975523968 -> 140492975523920
	140493494676912 [label="stages.3.blocks.2.norm.weight
 (304)" fillcolor=lightblue]
	140493494676912 -> 140492975523968
	140492975523968 [label=AccumulateGrad]
	140492975523824 -> 140492975523920
	140493494676992 [label="stages.3.blocks.2.norm.bias
 (304)" fillcolor=lightblue]
	140493494676992 -> 140492975523824
	140492975523824 [label=AccumulateGrad]
	140492975523728 -> 140492975523680
	140492975523728 [label=TBackward0]
	140492975524160 -> 140492975523728
	140493494677072 [label="stages.3.blocks.2.mlp.fc1.weight
 (1216, 304)" fillcolor=lightblue]
	140493494677072 -> 140492975524160
	140492975524160 [label=AccumulateGrad]
	140492975523488 -> 140492975523440
	140493494677152 [label="stages.3.blocks.2.mlp.fc1.bias
 (1216)" fillcolor=lightblue]
	140493494677152 -> 140492975523488
	140492975523488 [label=AccumulateGrad]
	140492975523248 -> 140492975522528
	140492975523248 [label=TBackward0]
	140492975522768 -> 140492975523248
	140493494677232 [label="stages.3.blocks.2.mlp.fc2.weight
 (304, 1216)" fillcolor=lightblue]
	140493494677232 -> 140492975522768
	140492975522768 [label=AccumulateGrad]
	140492975522240 -> 140492975506864
	140493494677312 [label="stages.3.blocks.2.mlp.fc2.bias
 (304)" fillcolor=lightblue]
	140493494677312 -> 140492975522240
	140492975522240 [label=AccumulateGrad]
	140492154510352 -> 140492154510208
	140492154449984 [label="head.norm.weight
 (304)" fillcolor=lightblue]
	140492154449984 -> 140492154510352
	140492154510352 [label=AccumulateGrad]
	140492154510256 -> 140492154510208
	140492154450064 [label="head.norm.bias
 (304)" fillcolor=lightblue]
	140492154450064 -> 140492154510256
	140492154510256 [label=AccumulateGrad]
	140492154508096 -> 140492154507760
	140492154508096 [label=TBackward0]
	140492154510448 -> 140492154508096
	140492154450144 [label="head.fc.weight
 (1000, 304)" fillcolor=lightblue]
	140492154450144 -> 140492154510448
	140492154510448 [label=AccumulateGrad]
	140492154507760 -> 140492159210096
}
