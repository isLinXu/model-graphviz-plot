digraph {
	graph [size="588.15,588.15"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	140551507433248 [label="
 (1, 1000)" fillcolor=darkolivegreen1]
	140550708217888 [label=AddmmBackward0]
	140550707778320 -> 140550708217888
	140550429641520 [label="head.fc.bias
 (1000)" fillcolor=lightblue]
	140550429641520 -> 140550707778320
	140550707778320 [label=AccumulateGrad]
	140550707778368 -> 140550708217888
	140550707778368 [label=ReshapeAliasBackward0]
	140550707778416 -> 140550707778368
	140550707778416 [label=AsStridedBackward1]
	140550707781104 -> 140550707778416
	140550707781104 [label=MeanBackward1]
	140550707781248 -> 140550707781104
	140550707781248 [label=PermuteBackward0]
	140550707779184 -> 140550707781248
	140550707779184 [label=NativeLayerNormBackward0]
	140550707780240 -> 140550707779184
	140550707780240 [label=PermuteBackward0]
	140550708243136 -> 140550707780240
	140550708243136 [label=AddBackward0]
	140550708243328 -> 140550708243136
	140550708243328 [label=AddBackward0]
	140550708243712 -> 140550708243328
	140550708243712 [label=AddBackward0]
	140550708243616 -> 140550708243712
	140550708243616 [label=AddBackward0]
	140550708242944 -> 140550708243616
	140550708242944 [label=PermuteBackward0]
	140550708243520 -> 140550708242944
	140550708243520 [label=NativeLayerNormBackward0]
	140550708243856 -> 140550708243520
	140550708243856 [label=PermuteBackward0]
	140550708244144 -> 140550708243856
	140550708244144 [label=ConvolutionBackward0]
	140550708244288 -> 140550708244144
	140550708244288 [label=AddBackward0]
	140550708244432 -> 140550708244288
	140550708244432 [label=AddBackward0]
	140551507550416 -> 140550708244432
	140551507550416 [label=AddBackward0]
	140551507550560 -> 140551507550416
	140551507550560 [label=AddBackward0]
	140551507550704 -> 140551507550560
	140551507550704 [label=AddBackward0]
	140551507550848 -> 140551507550704
	140551507550848 [label=AddBackward0]
	140551507550992 -> 140551507550848
	140551507550992 [label=AddBackward0]
	140551507551136 -> 140551507550992
	140551507551136 [label=AddBackward0]
	140551507551280 -> 140551507551136
	140551507551280 [label=AddBackward0]
	140551507551424 -> 140551507551280
	140551507551424 [label=AddBackward0]
	140551507551568 -> 140551507551424
	140551507551568 [label=AddBackward0]
	140551507551712 -> 140551507551568
	140551507551712 [label=AddBackward0]
	140551507551856 -> 140551507551712
	140551507551856 [label=AddBackward0]
	140551507552000 -> 140551507551856
	140551507552000 [label=AddBackward0]
	140551507552144 -> 140551507552000
	140551507552144 [label=AddBackward0]
	140551507552288 -> 140551507552144
	140551507552288 [label=AddBackward0]
	140551507552432 -> 140551507552288
	140551507552432 [label=AddBackward0]
	140551507552576 -> 140551507552432
	140551507552576 [label=AddBackward0]
	140551507552720 -> 140551507552576
	140551507552720 [label=AddBackward0]
	140551507552864 -> 140551507552720
	140551507552864 [label=AddBackward0]
	140551507553008 -> 140551507552864
	140551507553008 [label=AddBackward0]
	140551507553152 -> 140551507553008
	140551507553152 [label=AddBackward0]
	140551507553296 -> 140551507553152
	140551507553296 [label=AddBackward0]
	140551507553440 -> 140551507553296
	140551507553440 [label=AddBackward0]
	140551507553584 -> 140551507553440
	140551507553584 [label=AddBackward0]
	140551507553728 -> 140551507553584
	140551507553728 [label=AddBackward0]
	140551507553872 -> 140551507553728
	140551507553872 [label=AddBackward0]
	140551507554016 -> 140551507553872
	140551507554016 [label=AddBackward0]
	140551507554160 -> 140551507554016
	140551507554160 [label=AddBackward0]
	140551507554256 -> 140551507554160
	140551507554256 [label=AddBackward0]
	140551507562704 -> 140551507554256
	140551507562704 [label=AddBackward0]
	140551507562848 -> 140551507562704
	140551507562848 [label=AddBackward0]
	140551507562992 -> 140551507562848
	140551507562992 [label=AddBackward0]
	140551507563136 -> 140551507562992
	140551507563136 [label=AddBackward0]
	140551507563280 -> 140551507563136
	140551507563280 [label=AddBackward0]
	140551507563424 -> 140551507563280
	140551507563424 [label=AddBackward0]
	140551507563568 -> 140551507563424
	140551507563568 [label=PermuteBackward0]
	140551507563712 -> 140551507563568
	140551507563712 [label=NativeLayerNormBackward0]
	140551507563808 -> 140551507563712
	140551507563808 [label=PermuteBackward0]
	140551507564000 -> 140551507563808
	140551507564000 [label=ConvolutionBackward0]
	140551507564096 -> 140551507564000
	140551507564096 [label=AddBackward0]
	140551507564288 -> 140551507564096
	140551507564288 [label=AddBackward0]
	140551507564432 -> 140551507564288
	140551507564432 [label=AddBackward0]
	140551507564576 -> 140551507564432
	140551507564576 [label=AddBackward0]
	140551507564720 -> 140551507564576
	140551507564720 [label=PermuteBackward0]
	140551507564864 -> 140551507564720
	140551507564864 [label=NativeLayerNormBackward0]
	140551507564960 -> 140551507564864
	140551507564960 [label=PermuteBackward0]
	140551507565152 -> 140551507564960
	140551507565152 [label=ConvolutionBackward0]
	140551507565248 -> 140551507565152
	140551507565248 [label=AddBackward0]
	140551507565440 -> 140551507565248
	140551507565440 [label=AddBackward0]
	140551507565584 -> 140551507565440
	140551507565584 [label=AddBackward0]
	140551507565728 -> 140551507565584
	140551507565728 [label=AddBackward0]
	140551507565872 -> 140551507565728
	140551507565872 [label=PermuteBackward0]
	140551507566016 -> 140551507565872
	140551507566016 [label=NativeLayerNormBackward0]
	140551507566064 -> 140551507566016
	140551507566064 [label=PermuteBackward0]
	140551507566352 -> 140551507566064
	140551507566352 [label=ConvolutionBackward0]
	140551507566400 -> 140551507566352
	140551225422368 [label="stem.proj.weight
 (128, 3, 4, 4)" fillcolor=lightblue]
	140551225422368 -> 140551507566400
	140551507566400 [label=AccumulateGrad]
	140551507566256 -> 140551507566352
	140551225422448 [label="stem.proj.bias
 (128)" fillcolor=lightblue]
	140551225422448 -> 140551507566256
	140551507566256 [label=AccumulateGrad]
	140551507565920 -> 140551507566016
	140550926589840 [label="stem.norm.weight
 (128)" fillcolor=lightblue]
	140550926589840 -> 140551507565920
	140551507565920 [label=AccumulateGrad]
	140551507566160 -> 140551507566016
	140550793048928 [label="stem.norm.bias
 (128)" fillcolor=lightblue]
	140550793048928 -> 140551507566160
	140551507566160 [label=AccumulateGrad]
	140551507565824 -> 140551507565728
	140551507565824 [label=ConvolutionBackward0]
	140551507566304 -> 140551507565824
	140551507566304 [label=MulBackward0]
	140551507570944 -> 140551507566304
	140551507570944 [label=SplitWithSizesBackward0]
	140551507571088 -> 140551507570944
	140551507571088 [label=ConvolutionBackward0]
	140551507571184 -> 140551507571088
	140551507571184 [label=PermuteBackward0]
	140551507571376 -> 140551507571184
	140551507571376 [label=NativeLayerNormBackward0]
	140551507571472 -> 140551507571376
	140551507571472 [label=PermuteBackward0]
	140551507565872 -> 140551507571472
	140551507571424 -> 140551507571376
	140551225424288 [label="layers.0.blocks.0.norm1.weight
 (128)" fillcolor=lightblue]
	140551225424288 -> 140551507571424
	140551507571424 [label=AccumulateGrad]
	140551507571280 -> 140551507571376
	140551225422288 [label="layers.0.blocks.0.norm1.bias
 (128)" fillcolor=lightblue]
	140551225422288 -> 140551507571280
	140551507571280 [label=AccumulateGrad]
	140551507571136 -> 140551507571088
	140551225422608 [label="layers.0.blocks.0.modulation.f.weight
 (260, 128, 1, 1)" fillcolor=lightblue]
	140551225422608 -> 140551507571136
	140551507571136 [label=AccumulateGrad]
	140551507570992 -> 140551507571088
	140551225422688 [label="layers.0.blocks.0.modulation.f.bias
 (260)" fillcolor=lightblue]
	140551225422688 -> 140551507570992
	140551507570992 [label=AccumulateGrad]
	140551507570896 -> 140551507566304
	140551507570896 [label=ConvolutionBackward0]
	140551507571328 -> 140551507570896
	140551507571328 [label=AddBackward0]
	140551507571568 -> 140551507571328
	140551507571568 [label=AddBackward0]
	140551507571808 -> 140551507571568
	140551507571808 [label=AddBackward0]
	140551507571952 -> 140551507571808
	140551507571952 [label=AddBackward0]
	140551507572096 -> 140551507571952
	140551507572096 [label=MulBackward0]
	140551507572192 -> 140551507572096
	140551507572192 [label=GeluBackward0]
	140551507572336 -> 140551507572192
	140551507572336 [label=ConvolutionBackward0]
	140551507570944 -> 140551507572336
	140551507572432 -> 140551507572336
	140551225423328 [label="layers.0.blocks.0.modulation.focal_layers.0.0.weight
 (128, 1, 3, 3)" fillcolor=lightblue]
	140551225423328 -> 140551507572432
	140551507572432 [label=AccumulateGrad]
	140551507572144 -> 140551507572096
	140551507572144 [label=SliceBackward0]
	140551507572528 -> 140551507572144
	140551507572528 [label=SliceBackward0]
	140551507570944 -> 140551507572528
	140551507571904 -> 140551507571808
	140551507571904 [label=MulBackward0]
	140551507572384 -> 140551507571904
	140551507572384 [label=GeluBackward0]
	140551507572288 -> 140551507572384
	140551507572288 [label=ConvolutionBackward0]
	140551507572192 -> 140551507572288
	140551507572624 -> 140551507572288
	140551225423488 [label="layers.0.blocks.0.modulation.focal_layers.1.0.weight
 (128, 1, 5, 5)" fillcolor=lightblue]
	140551225423488 -> 140551507572624
	140551507572624 [label=AccumulateGrad]
	140551507572000 -> 140551507571904
	140551507572000 [label=SliceBackward0]
	140551507572720 -> 140551507572000
	140551507572720 [label=SliceBackward0]
	140551507570944 -> 140551507572720
	140551507571760 -> 140551507571568
	140551507571760 [label=MulBackward0]
	140551507572576 -> 140551507571760
	140551507572576 [label=GeluBackward0]
	140551507572480 -> 140551507572576
	140551507572480 [label=ConvolutionBackward0]
	140551507572384 -> 140551507572480
	140551507572816 -> 140551507572480
	140551225423648 [label="layers.0.blocks.0.modulation.focal_layers.2.0.weight
 (128, 1, 7, 7)" fillcolor=lightblue]
	140551225423648 -> 140551507572816
	140551507572816 [label=AccumulateGrad]
	140551507572048 -> 140551507571760
	140551507572048 [label=SliceBackward0]
	140551507572912 -> 140551507572048
	140551507572912 [label=SliceBackward0]
	140551507570944 -> 140551507572912
	140551507571616 -> 140551507571328
	140551507571616 [label=MulBackward0]
	140551507572768 -> 140551507571616
	140551507572768 [label=GeluBackward0]
	140551507572672 -> 140551507572768
	140551507572672 [label=MeanBackward1]
	140551507572576 -> 140551507572672
	140551507571856 -> 140551507571616
	140551507571856 [label=SliceBackward0]
	140551507572960 -> 140551507571856
	140551507572960 [label=SliceBackward0]
	140551507570944 -> 140551507572960
	140551507571232 -> 140551507570896
	140551225422848 [label="layers.0.blocks.0.modulation.h.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	140551225422848 -> 140551507571232
	140551507571232 [label=AccumulateGrad]
	140551507571040 -> 140551507570896
	140551225422928 [label="layers.0.blocks.0.modulation.h.bias
 (128)" fillcolor=lightblue]
	140551225422928 -> 140551507571040
	140551507571040 [label=AccumulateGrad]
	140551507566208 -> 140551507565824
	140551225423088 [label="layers.0.blocks.0.modulation.proj.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	140551225423088 -> 140551507566208
	140551507566208 [label=AccumulateGrad]
	140551507565968 -> 140551507565824
	140551225423168 [label="layers.0.blocks.0.modulation.proj.bias
 (128)" fillcolor=lightblue]
	140551225423168 -> 140551507565968
	140551507565968 [label=AccumulateGrad]
	140551507565680 -> 140551507565584
	140551507565680 [label=ConvolutionBackward0]
	140551507566496 -> 140551507565680
	140551507566496 [label=GeluBackward0]
	140551507573008 -> 140551507566496
	140551507573008 [label=ConvolutionBackward0]
	140551507572240 -> 140551507573008
	140551507572240 [label=PermuteBackward0]
	140551507573200 -> 140551507572240
	140551507573200 [label=NativeLayerNormBackward0]
	140551507573296 -> 140551507573200
	140551507573296 [label=PermuteBackward0]
	140551507565728 -> 140551507573296
	140551507573248 -> 140551507573200
	140551225423728 [label="layers.0.blocks.0.norm2.weight
 (128)" fillcolor=lightblue]
	140551225423728 -> 140551507573248
	140551507573248 [label=AccumulateGrad]
	140551507573104 -> 140551507573200
	140551225423808 [label="layers.0.blocks.0.norm2.bias
 (128)" fillcolor=lightblue]
	140551225423808 -> 140551507573104
	140551507573104 [label=AccumulateGrad]
	140551507573056 -> 140551507573008
	140550939352016 [label="layers.0.blocks.0.mlp.fc1.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	140550939352016 -> 140551507573056
	140551507573056 [label=AccumulateGrad]
	140551507571664 -> 140551507573008
	140550939352096 [label="layers.0.blocks.0.mlp.fc1.bias
 (512)" fillcolor=lightblue]
	140550939352096 -> 140551507571664
	140551507571664 [label=AccumulateGrad]
	140551507565776 -> 140551507565680
	140550939352256 [label="layers.0.blocks.0.mlp.fc2.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	140550939352256 -> 140551507565776
	140551507565776 [label=AccumulateGrad]
	140551507570848 -> 140551507565680
	140550939352336 [label="layers.0.blocks.0.mlp.fc2.bias
 (128)" fillcolor=lightblue]
	140550939352336 -> 140551507570848
	140551507570848 [label=AccumulateGrad]
	140551507565536 -> 140551507565440
	140551507565536 [label=ConvolutionBackward0]
	140551507565632 -> 140551507565536
	140551507565632 [label=MulBackward0]
	140551507573488 -> 140551507565632
	140551507573488 [label=SplitWithSizesBackward0]
	140551507573536 -> 140551507573488
	140551507573536 [label=ConvolutionBackward0]
	140551507573632 -> 140551507573536
	140551507573632 [label=PermuteBackward0]
	140551507573824 -> 140551507573632
	140551507573824 [label=NativeLayerNormBackward0]
	140551507573920 -> 140551507573824
	140551507573920 [label=PermuteBackward0]
	140551507565584 -> 140551507573920
	140551507573872 -> 140551507573824
	140550939352176 [label="layers.0.blocks.1.norm1.weight
 (128)" fillcolor=lightblue]
	140550939352176 -> 140551507573872
	140551507573872 [label=AccumulateGrad]
	140551507573728 -> 140551507573824
	140550939352416 [label="layers.0.blocks.1.norm1.bias
 (128)" fillcolor=lightblue]
	140550939352416 -> 140551507573728
	140551507573728 [label=AccumulateGrad]
	140551507573584 -> 140551507573536
	140550939352576 [label="layers.0.blocks.1.modulation.f.weight
 (260, 128, 1, 1)" fillcolor=lightblue]
	140550939352576 -> 140551507573584
	140551507573584 [label=AccumulateGrad]
	140551507573440 -> 140551507573536
	140550939352656 [label="layers.0.blocks.1.modulation.f.bias
 (260)" fillcolor=lightblue]
	140550939352656 -> 140551507573440
	140551507573440 [label=AccumulateGrad]
	140551507573344 -> 140551507565632
	140551507573344 [label=ConvolutionBackward0]
	140551507573776 -> 140551507573344
	140551507573776 [label=AddBackward0]
	140551507574016 -> 140551507573776
	140551507574016 [label=AddBackward0]
	140551507574256 -> 140551507574016
	140551507574256 [label=AddBackward0]
	140551507574400 -> 140551507574256
	140551507574400 [label=AddBackward0]
	140551507574544 -> 140551507574400
	140551507574544 [label=MulBackward0]
	140551507574640 -> 140551507574544
	140551507574640 [label=GeluBackward0]
	140551507574736 -> 140551507574640
	140551507574736 [label=ConvolutionBackward0]
	140551507573488 -> 140551507574736
	140551507419296 -> 140551507574736
	140550939414832 [label="layers.0.blocks.1.modulation.focal_layers.0.0.weight
 (128, 1, 3, 3)" fillcolor=lightblue]
	140550939414832 -> 140551507419296
	140551507419296 [label=AccumulateGrad]
	140551507574592 -> 140551507574544
	140551507574592 [label=SliceBackward0]
	140551507574688 -> 140551507574592
	140551507574688 [label=SliceBackward0]
	140551507573488 -> 140551507574688
	140551507574352 -> 140551507574256
	140551507574352 [label=MulBackward0]
	140551507574448 -> 140551507574352
	140551507574448 [label=GeluBackward0]
	140551507419248 -> 140551507574448
	140551507419248 [label=ConvolutionBackward0]
	140551507574640 -> 140551507419248
	140551507419488 -> 140551507419248
	140550939414992 [label="layers.0.blocks.1.modulation.focal_layers.1.0.weight
 (128, 1, 5, 5)" fillcolor=lightblue]
	140550939414992 -> 140551507419488
	140551507419488 [label=AccumulateGrad]
	140551507574496 -> 140551507574352
	140551507574496 [label=SliceBackward0]
	140551507419584 -> 140551507574496
	140551507419584 [label=SliceBackward0]
	140551507573488 -> 140551507419584
	140551507574208 -> 140551507574016
	140551507574208 [label=MulBackward0]
	140551507574304 -> 140551507574208
	140551507574304 [label=GeluBackward0]
	140551507419344 -> 140551507574304
	140551507419344 [label=ConvolutionBackward0]
	140551507574448 -> 140551507419344
	140551507419680 -> 140551507419344
	140550939415152 [label="layers.0.blocks.1.modulation.focal_layers.2.0.weight
 (128, 1, 7, 7)" fillcolor=lightblue]
	140550939415152 -> 140551507419680
	140551507419680 [label=AccumulateGrad]
	140551507419440 -> 140551507574208
	140551507419440 [label=SliceBackward0]
	140551507419776 -> 140551507419440
	140551507419776 [label=SliceBackward0]
	140551507573488 -> 140551507419776
	140551507574064 -> 140551507573776
	140551507574064 [label=MulBackward0]
	140551507574160 -> 140551507574064
	140551507574160 [label=GeluBackward0]
	140551507419536 -> 140551507574160
	140551507419536 [label=MeanBackward1]
	140551507574304 -> 140551507419536
	140551507419632 -> 140551507574064
	140551507419632 [label=SliceBackward0]
	140551507419824 -> 140551507419632
	140551507419824 [label=SliceBackward0]
	140551507573488 -> 140551507419824
	140551507573680 -> 140551507573344
	140550939352816 [label="layers.0.blocks.1.modulation.h.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	140550939352816 -> 140551507573680
	140551507573680 [label=AccumulateGrad]
	140551507573392 -> 140551507573344
	140550939352896 [label="layers.0.blocks.1.modulation.h.bias
 (128)" fillcolor=lightblue]
	140550939352896 -> 140551507573392
	140551507573392 [label=AccumulateGrad]
	140551507571712 -> 140551507565536
	140550939414592 [label="layers.0.blocks.1.modulation.proj.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	140550939414592 -> 140551507571712
	140551507571712 [label=AccumulateGrad]
	140551507571520 -> 140551507565536
	140550939414672 [label="layers.0.blocks.1.modulation.proj.bias
 (128)" fillcolor=lightblue]
	140550939414672 -> 140551507571520
	140551507571520 [label=AccumulateGrad]
	140551507565392 -> 140551507565248
	140551507565392 [label=ConvolutionBackward0]
	140551507565488 -> 140551507565392
	140551507565488 [label=GeluBackward0]
	140551507574112 -> 140551507565488
	140551507574112 [label=ConvolutionBackward0]
	140551507419200 -> 140551507574112
	140551507419200 [label=PermuteBackward0]
	140551507420064 -> 140551507419200
	140551507420064 [label=NativeLayerNormBackward0]
	140551507420160 -> 140551507420064
	140551507420160 [label=PermuteBackward0]
	140551507565440 -> 140551507420160
	140551507420112 -> 140551507420064
	140550939415232 [label="layers.0.blocks.1.norm2.weight
 (128)" fillcolor=lightblue]
	140550939415232 -> 140551507420112
	140551507420112 [label=AccumulateGrad]
	140551507419968 -> 140551507420064
	140550939415312 [label="layers.0.blocks.1.norm2.bias
 (128)" fillcolor=lightblue]
	140550939415312 -> 140551507419968
	140551507419968 [label=AccumulateGrad]
	140551507419920 -> 140551507574112
	140550939415552 [label="layers.0.blocks.1.mlp.fc1.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	140550939415552 -> 140551507419920
	140551507419920 [label=AccumulateGrad]
	140551507419392 -> 140551507574112
	140550939415632 [label="layers.0.blocks.1.mlp.fc1.bias
 (512)" fillcolor=lightblue]
	140550939415632 -> 140551507419392
	140551507419392 [label=AccumulateGrad]
	140551507573152 -> 140551507565392
	140550939415792 [label="layers.0.blocks.1.mlp.fc2.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	140550939415792 -> 140551507573152
	140551507573152 [label=AccumulateGrad]
	140551507572864 -> 140551507565392
	140550939415872 [label="layers.0.blocks.1.mlp.fc2.bias
 (128)" fillcolor=lightblue]
	140550939415872 -> 140551507572864
	140551507572864 [label=AccumulateGrad]
	140551507565200 -> 140551507565152
	140550939415952 [label="layers.1.downsample.proj.weight
 (256, 128, 2, 2)" fillcolor=lightblue]
	140550939415952 -> 140551507565200
	140551507565200 [label=AccumulateGrad]
	140551507565056 -> 140551507565152
	140550939416032 [label="layers.1.downsample.proj.bias
 (256)" fillcolor=lightblue]
	140550939416032 -> 140551507565056
	140551507565056 [label=AccumulateGrad]
	140551507564912 -> 140551507564864
	140550939416112 [label="layers.1.downsample.norm.weight
 (256)" fillcolor=lightblue]
	140550939416112 -> 140551507564912
	140551507564912 [label=AccumulateGrad]
	140551507564768 -> 140551507564864
	140550939416192 [label="layers.1.downsample.norm.bias
 (256)" fillcolor=lightblue]
	140550939416192 -> 140551507564768
	140551507564768 [label=AccumulateGrad]
	140551507564672 -> 140551507564576
	140551507564672 [label=ConvolutionBackward0]
	140551507565104 -> 140551507564672
	140551507565104 [label=MulBackward0]
	140551507573968 -> 140551507565104
	140551507573968 [label=SplitWithSizesBackward0]
	140551507420352 -> 140551507573968
	140551507420352 [label=ConvolutionBackward0]
	140551507420256 -> 140551507420352
	140551507420256 [label=PermuteBackward0]
	140551507420544 -> 140551507420256
	140551507420544 [label=NativeLayerNormBackward0]
	140551507420640 -> 140551507420544
	140551507420640 [label=PermuteBackward0]
	140551507564720 -> 140551507420640
	140551507420592 -> 140551507420544
	140550939416272 [label="layers.1.blocks.0.norm1.weight
 (256)" fillcolor=lightblue]
	140550939416272 -> 140551507420592
	140551507420592 [label=AccumulateGrad]
	140551507420448 -> 140551507420544
	140550939416352 [label="layers.1.blocks.0.norm1.bias
 (256)" fillcolor=lightblue]
	140550939416352 -> 140551507420448
	140551507420448 [label=AccumulateGrad]
	140551507420304 -> 140551507420352
	140550939416512 [label="layers.1.blocks.0.modulation.f.weight
 (516, 256, 1, 1)" fillcolor=lightblue]
	140550939416512 -> 140551507420304
	140551507420304 [label=AccumulateGrad]
	140551507420016 -> 140551507420352
	140550939416592 [label="layers.1.blocks.0.modulation.f.bias
 (516)" fillcolor=lightblue]
	140550939416592 -> 140551507420016
	140551507420016 [label=AccumulateGrad]
	140551507419728 -> 140551507565104
	140551507419728 [label=ConvolutionBackward0]
	140551507420496 -> 140551507419728
	140551507420496 [label=AddBackward0]
	140551507420736 -> 140551507420496
	140551507420736 [label=AddBackward0]
	140551507420976 -> 140551507420736
	140551507420976 [label=AddBackward0]
	140551507421120 -> 140551507420976
	140551507421120 [label=AddBackward0]
	140551507421264 -> 140551507421120
	140551507421264 [label=MulBackward0]
	140551507421360 -> 140551507421264
	140551507421360 [label=GeluBackward0]
	140551507421504 -> 140551507421360
	140551507421504 [label=ConvolutionBackward0]
	140551507573968 -> 140551507421504
	140551507421600 -> 140551507421504
	140550939417232 [label="layers.1.blocks.0.modulation.focal_layers.0.0.weight
 (256, 1, 3, 3)" fillcolor=lightblue]
	140550939417232 -> 140551507421600
	140551507421600 [label=AccumulateGrad]
	140551507421312 -> 140551507421264
	140551507421312 [label=SliceBackward0]
	140551507421696 -> 140551507421312
	140551507421696 [label=SliceBackward0]
	140551507573968 -> 140551507421696
	140551507421072 -> 140551507420976
	140551507421072 [label=MulBackward0]
	140551507421552 -> 140551507421072
	140551507421552 [label=GeluBackward0]
	140551507421456 -> 140551507421552
	140551507421456 [label=ConvolutionBackward0]
	140551507421360 -> 140551507421456
	140551507421792 -> 140551507421456
	140550939417392 [label="layers.1.blocks.0.modulation.focal_layers.1.0.weight
 (256, 1, 5, 5)" fillcolor=lightblue]
	140550939417392 -> 140551507421792
	140551507421792 [label=AccumulateGrad]
	140551507421168 -> 140551507421072
	140551507421168 [label=SliceBackward0]
	140551507421888 -> 140551507421168
	140551507421888 [label=SliceBackward0]
	140551507573968 -> 140551507421888
	140551507420928 -> 140551507420736
	140551507420928 [label=MulBackward0]
	140551507421744 -> 140551507420928
	140551507421744 [label=GeluBackward0]
	140551507421648 -> 140551507421744
	140551507421648 [label=ConvolutionBackward0]
	140551507421552 -> 140551507421648
	140551507421984 -> 140551507421648
	140550939417552 [label="layers.1.blocks.0.modulation.focal_layers.2.0.weight
 (256, 1, 7, 7)" fillcolor=lightblue]
	140550939417552 -> 140551507421984
	140551507421984 [label=AccumulateGrad]
	140551507421216 -> 140551507420928
	140551507421216 [label=SliceBackward0]
	140551507422080 -> 140551507421216
	140551507422080 [label=SliceBackward0]
	140551507573968 -> 140551507422080
	140551507420784 -> 140551507420496
	140551507420784 [label=MulBackward0]
	140551507421936 -> 140551507420784
	140551507421936 [label=GeluBackward0]
	140551507421840 -> 140551507421936
	140551507421840 [label=MeanBackward1]
	140551507421744 -> 140551507421840
	140551507421024 -> 140551507420784
	140551507421024 [label=SliceBackward0]
	140551507422128 -> 140551507421024
	140551507422128 [label=SliceBackward0]
	140551507573968 -> 140551507422128
	140551507420400 -> 140551507419728
	140550939416752 [label="layers.1.blocks.0.modulation.h.weight
 (256, 256, 1, 1)" fillcolor=lightblue]
	140550939416752 -> 140551507420400
	140551507420400 [label=AccumulateGrad]
	140551507420208 -> 140551507419728
	140550939416832 [label="layers.1.blocks.0.modulation.h.bias
 (256)" fillcolor=lightblue]
	140550939416832 -> 140551507420208
	140551507420208 [label=AccumulateGrad]
	140551507565008 -> 140551507564672
	140550939416992 [label="layers.1.blocks.0.modulation.proj.weight
 (256, 256, 1, 1)" fillcolor=lightblue]
	140550939416992 -> 140551507565008
	140551507565008 [label=AccumulateGrad]
	140551507564816 -> 140551507564672
	140550939417072 [label="layers.1.blocks.0.modulation.proj.bias
 (256)" fillcolor=lightblue]
	140550939417072 -> 140551507564816
	140551507564816 [label=AccumulateGrad]
	140551507564528 -> 140551507564432
	140551507564528 [label=ConvolutionBackward0]
	140551507565296 -> 140551507564528
	140551507565296 [label=GeluBackward0]
	140551507422176 -> 140551507565296
	140551507422176 [label=ConvolutionBackward0]
	140551507421408 -> 140551507422176
	140551507421408 [label=PermuteBackward0]
	140551507422368 -> 140551507421408
	140551507422368 [label=NativeLayerNormBackward0]
	140551507422464 -> 140551507422368
	140551507422464 [label=PermuteBackward0]
	140551507564576 -> 140551507422464
	140551507422416 -> 140551507422368
	140550939417632 [label="layers.1.blocks.0.norm2.weight
 (256)" fillcolor=lightblue]
	140550939417632 -> 140551507422416
	140551507422416 [label=AccumulateGrad]
	140551507422272 -> 140551507422368
	140550939417712 [label="layers.1.blocks.0.norm2.bias
 (256)" fillcolor=lightblue]
	140550939417712 -> 140551507422272
	140551507422272 [label=AccumulateGrad]
	140551507422224 -> 140551507422176
	140550939417952 [label="layers.1.blocks.0.mlp.fc1.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	140550939417952 -> 140551507422224
	140551507422224 [label=AccumulateGrad]
	140551507420832 -> 140551507422176
	140550939418032 [label="layers.1.blocks.0.mlp.fc1.bias
 (1024)" fillcolor=lightblue]
	140550939418032 -> 140551507420832
	140551507420832 [label=AccumulateGrad]
	140551507564624 -> 140551507564528
	140550939418192 [label="layers.1.blocks.0.mlp.fc2.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	140550939418192 -> 140551507564624
	140551507564624 [label=AccumulateGrad]
	140551507419872 -> 140551507564528
	140550939418272 [label="layers.1.blocks.0.mlp.fc2.bias
 (256)" fillcolor=lightblue]
	140550939418272 -> 140551507419872
	140551507419872 [label=AccumulateGrad]
	140551507564384 -> 140551507564288
	140551507564384 [label=ConvolutionBackward0]
	140551507564480 -> 140551507564384
	140551507564480 [label=MulBackward0]
	140551507422656 -> 140551507564480
	140551507422656 [label=SplitWithSizesBackward0]
	140551507422704 -> 140551507422656
	140551507422704 [label=ConvolutionBackward0]
	140551507422800 -> 140551507422704
	140551507422800 [label=PermuteBackward0]
	140551507422992 -> 140551507422800
	140551507422992 [label=NativeLayerNormBackward0]
	140551507423088 -> 140551507422992
	140551507423088 [label=PermuteBackward0]
	140551507564432 -> 140551507423088
	140551507423040 -> 140551507422992
	140550939418112 [label="layers.1.blocks.1.norm1.weight
 (256)" fillcolor=lightblue]
	140550939418112 -> 140551507423040
	140551507423040 [label=AccumulateGrad]
	140551507422896 -> 140551507422992
	140550939418352 [label="layers.1.blocks.1.norm1.bias
 (256)" fillcolor=lightblue]
	140550939418352 -> 140551507422896
	140551507422896 [label=AccumulateGrad]
	140551507422752 -> 140551507422704
	140550939418512 [label="layers.1.blocks.1.modulation.f.weight
 (516, 256, 1, 1)" fillcolor=lightblue]
	140550939418512 -> 140551507422752
	140551507422752 [label=AccumulateGrad]
	140551507422608 -> 140551507422704
	140550799540288 [label="layers.1.blocks.1.modulation.f.bias
 (516)" fillcolor=lightblue]
	140550799540288 -> 140551507422608
	140551507422608 [label=AccumulateGrad]
	140551507422512 -> 140551507564480
	140551507422512 [label=ConvolutionBackward0]
	140551507422944 -> 140551507422512
	140551507422944 [label=AddBackward0]
	140551507423184 -> 140551507422944
	140551507423184 [label=AddBackward0]
	140551507607808 -> 140551507423184
	140551507607808 [label=AddBackward0]
	140551507607952 -> 140551507607808
	140551507607952 [label=AddBackward0]
	140551507608096 -> 140551507607952
	140551507608096 [label=MulBackward0]
	140551507608192 -> 140551507608096
	140551507608192 [label=GeluBackward0]
	140551507608336 -> 140551507608192
	140551507608336 [label=ConvolutionBackward0]
	140551507422656 -> 140551507608336
	140551507608432 -> 140551507608336
	140550799540928 [label="layers.1.blocks.1.modulation.focal_layers.0.0.weight
 (256, 1, 3, 3)" fillcolor=lightblue]
	140550799540928 -> 140551507608432
	140551507608432 [label=AccumulateGrad]
	140551507608144 -> 140551507608096
	140551507608144 [label=SliceBackward0]
	140551507608528 -> 140551507608144
	140551507608528 [label=SliceBackward0]
	140551507422656 -> 140551507608528
	140551507607904 -> 140551507607808
	140551507607904 [label=MulBackward0]
	140551507608384 -> 140551507607904
	140551507608384 [label=GeluBackward0]
	140551507608288 -> 140551507608384
	140551507608288 [label=ConvolutionBackward0]
	140551507608192 -> 140551507608288
	140551507608624 -> 140551507608288
	140550799541088 [label="layers.1.blocks.1.modulation.focal_layers.1.0.weight
 (256, 1, 5, 5)" fillcolor=lightblue]
	140550799541088 -> 140551507608624
	140551507608624 [label=AccumulateGrad]
	140551507608000 -> 140551507607904
	140551507608000 [label=SliceBackward0]
	140551507608720 -> 140551507608000
	140551507608720 [label=SliceBackward0]
	140551507422656 -> 140551507608720
	140551507607760 -> 140551507423184
	140551507607760 [label=MulBackward0]
	140551507608576 -> 140551507607760
	140551507608576 [label=GeluBackward0]
	140551507608480 -> 140551507608576
	140551507608480 [label=ConvolutionBackward0]
	140551507608384 -> 140551507608480
	140551507608816 -> 140551507608480
	140550799541248 [label="layers.1.blocks.1.modulation.focal_layers.2.0.weight
 (256, 1, 7, 7)" fillcolor=lightblue]
	140550799541248 -> 140551507608816
	140551507608816 [label=AccumulateGrad]
	140551507608048 -> 140551507607760
	140551507608048 [label=SliceBackward0]
	140551507608912 -> 140551507608048
	140551507608912 [label=SliceBackward0]
	140551507422656 -> 140551507608912
	140551507607616 -> 140551507422944
	140551507607616 [label=MulBackward0]
	140551507608768 -> 140551507607616
	140551507608768 [label=GeluBackward0]
	140551507608672 -> 140551507608768
	140551507608672 [label=MeanBackward1]
	140551507608576 -> 140551507608672
	140551507607856 -> 140551507607616
	140551507607856 [label=SliceBackward0]
	140551507608960 -> 140551507607856
	140551507608960 [label=SliceBackward0]
	140551507422656 -> 140551507608960
	140551507422848 -> 140551507422512
	140550799540448 [label="layers.1.blocks.1.modulation.h.weight
 (256, 256, 1, 1)" fillcolor=lightblue]
	140550799540448 -> 140551507422848
	140551507422848 [label=AccumulateGrad]
	140551507422560 -> 140551507422512
	140550799540528 [label="layers.1.blocks.1.modulation.h.bias
 (256)" fillcolor=lightblue]
	140550799540528 -> 140551507422560
	140551507422560 [label=AccumulateGrad]
	140551507420880 -> 140551507564384
	140550799540688 [label="layers.1.blocks.1.modulation.proj.weight
 (256, 256, 1, 1)" fillcolor=lightblue]
	140550799540688 -> 140551507420880
	140551507420880 [label=AccumulateGrad]
	140551507420688 -> 140551507564384
	140550799540768 [label="layers.1.blocks.1.modulation.proj.bias
 (256)" fillcolor=lightblue]
	140550799540768 -> 140551507420688
	140551507420688 [label=AccumulateGrad]
	140551507564240 -> 140551507564096
	140551507564240 [label=ConvolutionBackward0]
	140551507564336 -> 140551507564240
	140551507564336 [label=GeluBackward0]
	140551507609008 -> 140551507564336
	140551507609008 [label=ConvolutionBackward0]
	140551507608240 -> 140551507609008
	140551507608240 [label=PermuteBackward0]
	140551507609200 -> 140551507608240
	140551507609200 [label=NativeLayerNormBackward0]
	140551507609296 -> 140551507609200
	140551507609296 [label=PermuteBackward0]
	140551507564288 -> 140551507609296
	140551507609248 -> 140551507609200
	140550799541328 [label="layers.1.blocks.1.norm2.weight
 (256)" fillcolor=lightblue]
	140550799541328 -> 140551507609248
	140551507609248 [label=AccumulateGrad]
	140551507609104 -> 140551507609200
	140550799541408 [label="layers.1.blocks.1.norm2.bias
 (256)" fillcolor=lightblue]
	140550799541408 -> 140551507609104
	140551507609104 [label=AccumulateGrad]
	140551507609056 -> 140551507609008
	140550799541648 [label="layers.1.blocks.1.mlp.fc1.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	140550799541648 -> 140551507609056
	140551507609056 [label=AccumulateGrad]
	140551507607664 -> 140551507609008
	140550799541728 [label="layers.1.blocks.1.mlp.fc1.bias
 (1024)" fillcolor=lightblue]
	140550799541728 -> 140551507607664
	140551507607664 [label=AccumulateGrad]
	140551507422320 -> 140551507564240
	140550799541888 [label="layers.1.blocks.1.mlp.fc2.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	140550799541888 -> 140551507422320
	140551507422320 [label=AccumulateGrad]
	140551507422032 -> 140551507564240
	140550799541968 [label="layers.1.blocks.1.mlp.fc2.bias
 (256)" fillcolor=lightblue]
	140550799541968 -> 140551507422032
	140551507422032 [label=AccumulateGrad]
	140551507564048 -> 140551507564000
	140550799542048 [label="layers.2.downsample.proj.weight
 (512, 256, 2, 2)" fillcolor=lightblue]
	140550799542048 -> 140551507564048
	140551507564048 [label=AccumulateGrad]
	140551507563904 -> 140551507564000
	140550799542128 [label="layers.2.downsample.proj.bias
 (512)" fillcolor=lightblue]
	140550799542128 -> 140551507563904
	140551507563904 [label=AccumulateGrad]
	140551507563760 -> 140551507563712
	140550799542208 [label="layers.2.downsample.norm.weight
 (512)" fillcolor=lightblue]
	140550799542208 -> 140551507563760
	140551507563760 [label=AccumulateGrad]
	140551507563616 -> 140551507563712
	140550799542288 [label="layers.2.downsample.norm.bias
 (512)" fillcolor=lightblue]
	140550799542288 -> 140551507563616
	140551507563616 [label=AccumulateGrad]
	140551507563520 -> 140551507563424
	140551507563520 [label=ConvolutionBackward0]
	140551507563952 -> 140551507563520
	140551507563952 [label=MulBackward0]
	140551507423136 -> 140551507563952
	140551507423136 [label=SplitWithSizesBackward0]
	140551507609344 -> 140551507423136
	140551507609344 [label=ConvolutionBackward0]
	140551507609440 -> 140551507609344
	140551507609440 [label=PermuteBackward0]
	140551507609632 -> 140551507609440
	140551507609632 [label=NativeLayerNormBackward0]
	140551507609728 -> 140551507609632
	140551507609728 [label=PermuteBackward0]
	140551507563568 -> 140551507609728
	140551507609680 -> 140551507609632
	140550799542368 [label="layers.2.blocks.0.norm1.weight
 (512)" fillcolor=lightblue]
	140550799542368 -> 140551507609680
	140551507609680 [label=AccumulateGrad]
	140551507609536 -> 140551507609632
	140550799542448 [label="layers.2.blocks.0.norm1.bias
 (512)" fillcolor=lightblue]
	140550799542448 -> 140551507609536
	140551507609536 [label=AccumulateGrad]
	140551507609488 -> 140551507609344
	140550799542608 [label="layers.2.blocks.0.modulation.f.weight
 (1028, 512, 1, 1)" fillcolor=lightblue]
	140550799542608 -> 140551507609488
	140551507609488 [label=AccumulateGrad]
	140551507608864 -> 140551507609344
	140550799542688 [label="layers.2.blocks.0.modulation.f.bias
 (1028)" fillcolor=lightblue]
	140550799542688 -> 140551507608864
	140551507608864 [label=AccumulateGrad]
	140551507564192 -> 140551507563952
	140551507564192 [label=ConvolutionBackward0]
	140551507609584 -> 140551507564192
	140551507609584 [label=AddBackward0]
	140551507609824 -> 140551507609584
	140551507609824 [label=AddBackward0]
	140551507610064 -> 140551507609824
	140551507610064 [label=AddBackward0]
	140551507610208 -> 140551507610064
	140551507610208 [label=AddBackward0]
	140551507610352 -> 140551507610208
	140551507610352 [label=MulBackward0]
	140551507610448 -> 140551507610352
	140551507610448 [label=GeluBackward0]
	140551507610592 -> 140551507610448
	140551507610592 [label=ConvolutionBackward0]
	140551507423136 -> 140551507610592
	140551507610688 -> 140551507610592
	140550799543328 [label="layers.2.blocks.0.modulation.focal_layers.0.0.weight
 (512, 1, 3, 3)" fillcolor=lightblue]
	140550799543328 -> 140551507610688
	140551507610688 [label=AccumulateGrad]
	140551507610400 -> 140551507610352
	140551507610400 [label=SliceBackward0]
	140551507610784 -> 140551507610400
	140551507610784 [label=SliceBackward0]
	140551507423136 -> 140551507610784
	140551507610160 -> 140551507610064
	140551507610160 [label=MulBackward0]
	140551507610640 -> 140551507610160
	140551507610640 [label=GeluBackward0]
	140551507610544 -> 140551507610640
	140551507610544 [label=ConvolutionBackward0]
	140551507610448 -> 140551507610544
	140551507610880 -> 140551507610544
	140550799543488 [label="layers.2.blocks.0.modulation.focal_layers.1.0.weight
 (512, 1, 5, 5)" fillcolor=lightblue]
	140550799543488 -> 140551507610880
	140551507610880 [label=AccumulateGrad]
	140551507610256 -> 140551507610160
	140551507610256 [label=SliceBackward0]
	140551507610976 -> 140551507610256
	140551507610976 [label=SliceBackward0]
	140551507423136 -> 140551507610976
	140551507610016 -> 140551507609824
	140551507610016 [label=MulBackward0]
	140551507610832 -> 140551507610016
	140551507610832 [label=GeluBackward0]
	140551507610736 -> 140551507610832
	140551507610736 [label=ConvolutionBackward0]
	140551507610640 -> 140551507610736
	140551507611072 -> 140551507610736
	140550799543648 [label="layers.2.blocks.0.modulation.focal_layers.2.0.weight
 (512, 1, 7, 7)" fillcolor=lightblue]
	140550799543648 -> 140551507611072
	140551507611072 [label=AccumulateGrad]
	140551507610304 -> 140551507610016
	140551507610304 [label=SliceBackward0]
	140551507611168 -> 140551507610304
	140551507611168 [label=SliceBackward0]
	140551507423136 -> 140551507611168
	140551507609872 -> 140551507609584
	140551507609872 [label=MulBackward0]
	140551507611024 -> 140551507609872
	140551507611024 [label=GeluBackward0]
	140551507610928 -> 140551507611024
	140551507610928 [label=MeanBackward1]
	140551507610832 -> 140551507610928
	140551507610112 -> 140551507609872
	140551507610112 [label=SliceBackward0]
	140551507611216 -> 140551507610112
	140551507611216 [label=SliceBackward0]
	140551507423136 -> 140551507611216
	140551507609392 -> 140551507564192
	140550799542848 [label="layers.2.blocks.0.modulation.h.weight
 (512, 512, 1, 1)" fillcolor=lightblue]
	140550799542848 -> 140551507609392
	140551507609392 [label=AccumulateGrad]
	140551507609152 -> 140551507564192
	140550799542928 [label="layers.2.blocks.0.modulation.h.bias
 (512)" fillcolor=lightblue]
	140550799542928 -> 140551507609152
	140551507609152 [label=AccumulateGrad]
	140551507563856 -> 140551507563520
	140550799543088 [label="layers.2.blocks.0.modulation.proj.weight
 (512, 512, 1, 1)" fillcolor=lightblue]
	140550799543088 -> 140551507563856
	140551507563856 [label=AccumulateGrad]
	140551507563664 -> 140551507563520
	140550799543168 [label="layers.2.blocks.0.modulation.proj.bias
 (512)" fillcolor=lightblue]
	140550799543168 -> 140551507563664
	140551507563664 [label=AccumulateGrad]
	140551507563376 -> 140551507563280
	140551507563376 [label=ConvolutionBackward0]
	140551507564144 -> 140551507563376
	140551507564144 [label=GeluBackward0]
	140551507611264 -> 140551507564144
	140551507611264 [label=ConvolutionBackward0]
	140551507610496 -> 140551507611264
	140551507610496 [label=PermuteBackward0]
	140551507611456 -> 140551507610496
	140551507611456 [label=NativeLayerNormBackward0]
	140551507611552 -> 140551507611456
	140551507611552 [label=PermuteBackward0]
	140551507563424 -> 140551507611552
	140551507611504 -> 140551507611456
	140550799543728 [label="layers.2.blocks.0.norm2.weight
 (512)" fillcolor=lightblue]
	140550799543728 -> 140551507611504
	140551507611504 [label=AccumulateGrad]
	140551507611360 -> 140551507611456
	140550799543808 [label="layers.2.blocks.0.norm2.bias
 (512)" fillcolor=lightblue]
	140550799543808 -> 140551507611360
	140551507611360 [label=AccumulateGrad]
	140551507611312 -> 140551507611264
	140550799544048 [label="layers.2.blocks.0.mlp.fc1.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	140550799544048 -> 140551507611312
	140551507611312 [label=AccumulateGrad]
	140551507609920 -> 140551507611264
	140550799544128 [label="layers.2.blocks.0.mlp.fc1.bias
 (2048)" fillcolor=lightblue]
	140550799544128 -> 140551507609920
	140551507609920 [label=AccumulateGrad]
	140551507563472 -> 140551507563376
	140550126743616 [label="layers.2.blocks.0.mlp.fc2.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	140550126743616 -> 140551507563472
	140551507563472 [label=AccumulateGrad]
	140551507607712 -> 140551507563376
	140550126743696 [label="layers.2.blocks.0.mlp.fc2.bias
 (512)" fillcolor=lightblue]
	140550126743696 -> 140551507607712
	140551507607712 [label=AccumulateGrad]
	140551507563232 -> 140551507563136
	140551507563232 [label=ConvolutionBackward0]
	140551507563328 -> 140551507563232
	140551507563328 [label=MulBackward0]
	140551507611600 -> 140551507563328
	140551507611600 [label=SplitWithSizesBackward0]
	140551793672400 -> 140551507611600
	140551793672400 [label=ConvolutionBackward0]
	140551793672496 -> 140551793672400
	140551793672496 [label=PermuteBackward0]
	140551793672688 -> 140551793672496
	140551793672688 [label=NativeLayerNormBackward0]
	140551793672832 -> 140551793672688
	140551793672832 [label=PermuteBackward0]
	140551507563280 -> 140551793672832
	140551793672784 -> 140551793672688
	140550799544208 [label="layers.2.blocks.1.norm1.weight
 (512)" fillcolor=lightblue]
	140550799544208 -> 140551793672784
	140551793672784 [label=AccumulateGrad]
	140551793672736 -> 140551793672688
	140550126743776 [label="layers.2.blocks.1.norm1.bias
 (512)" fillcolor=lightblue]
	140550126743776 -> 140551793672736
	140551793672736 [label=AccumulateGrad]
	140551793672448 -> 140551793672400
	140550126743936 [label="layers.2.blocks.1.modulation.f.weight
 (1028, 512, 1, 1)" fillcolor=lightblue]
	140550126743936 -> 140551793672448
	140551793672448 [label=AccumulateGrad]
	140551793672304 -> 140551793672400
	140550126744016 [label="layers.2.blocks.1.modulation.f.bias
 (1028)" fillcolor=lightblue]
	140550126744016 -> 140551793672304
	140551793672304 [label=AccumulateGrad]
	140551507611408 -> 140551507563328
	140551507611408 [label=ConvolutionBackward0]
	140551793672640 -> 140551507611408
	140551793672640 [label=AddBackward0]
	140551793672928 -> 140551793672640
	140551793672928 [label=AddBackward0]
	140551793673168 -> 140551793672928
	140551793673168 [label=AddBackward0]
	140551793673312 -> 140551793673168
	140551793673312 [label=AddBackward0]
	140551793673456 -> 140551793673312
	140551793673456 [label=MulBackward0]
	140551793673552 -> 140551793673456
	140551793673552 [label=GeluBackward0]
	140551793673696 -> 140551793673552
	140551793673696 [label=ConvolutionBackward0]
	140551507611600 -> 140551793673696
	140551793673792 -> 140551793673696
	140550126744656 [label="layers.2.blocks.1.modulation.focal_layers.0.0.weight
 (512, 1, 3, 3)" fillcolor=lightblue]
	140550126744656 -> 140551793673792
	140551793673792 [label=AccumulateGrad]
	140551793673504 -> 140551793673456
	140551793673504 [label=SliceBackward0]
	140551793673888 -> 140551793673504
	140551793673888 [label=SliceBackward0]
	140551507611600 -> 140551793673888
	140551793673264 -> 140551793673168
	140551793673264 [label=MulBackward0]
	140551793673744 -> 140551793673264
	140551793673744 [label=GeluBackward0]
	140551793673648 -> 140551793673744
	140551793673648 [label=ConvolutionBackward0]
	140551793673552 -> 140551793673648
	140551793673984 -> 140551793673648
	140550126744816 [label="layers.2.blocks.1.modulation.focal_layers.1.0.weight
 (512, 1, 5, 5)" fillcolor=lightblue]
	140550126744816 -> 140551793673984
	140551793673984 [label=AccumulateGrad]
	140551793673360 -> 140551793673264
	140551793673360 [label=SliceBackward0]
	140551793674080 -> 140551793673360
	140551793674080 [label=SliceBackward0]
	140551507611600 -> 140551793674080
	140551793673120 -> 140551793672928
	140551793673120 [label=MulBackward0]
	140551793673936 -> 140551793673120
	140551793673936 [label=GeluBackward0]
	140551793673840 -> 140551793673936
	140551793673840 [label=ConvolutionBackward0]
	140551793673744 -> 140551793673840
	140551793674176 -> 140551793673840
	140550126744976 [label="layers.2.blocks.1.modulation.focal_layers.2.0.weight
 (512, 1, 7, 7)" fillcolor=lightblue]
	140550126744976 -> 140551793674176
	140551793674176 [label=AccumulateGrad]
	140551793673408 -> 140551793673120
	140551793673408 [label=SliceBackward0]
	140551793674272 -> 140551793673408
	140551793674272 [label=SliceBackward0]
	140551507611600 -> 140551793674272
	140551793672976 -> 140551793672640
	140551793672976 [label=MulBackward0]
	140551793674128 -> 140551793672976
	140551793674128 [label=GeluBackward0]
	140551793674032 -> 140551793674128
	140551793674032 [label=MeanBackward1]
	140551793673936 -> 140551793674032
	140551793673216 -> 140551793672976
	140551793673216 [label=SliceBackward0]
	140551793674320 -> 140551793673216
	140551793674320 [label=SliceBackward0]
	140551507611600 -> 140551793674320
	140551793672544 -> 140551507611408
	140550126744176 [label="layers.2.blocks.1.modulation.h.weight
 (512, 512, 1, 1)" fillcolor=lightblue]
	140550126744176 -> 140551793672544
	140551793672544 [label=AccumulateGrad]
	140551793672352 -> 140551507611408
	140550126744256 [label="layers.2.blocks.1.modulation.h.bias
 (512)" fillcolor=lightblue]
	140550126744256 -> 140551793672352
	140551793672352 [label=AccumulateGrad]
	140551507609968 -> 140551507563232
	140550126744416 [label="layers.2.blocks.1.modulation.proj.weight
 (512, 512, 1, 1)" fillcolor=lightblue]
	140550126744416 -> 140551507609968
	140551507609968 [label=AccumulateGrad]
	140551507609776 -> 140551507563232
	140550126744496 [label="layers.2.blocks.1.modulation.proj.bias
 (512)" fillcolor=lightblue]
	140550126744496 -> 140551507609776
	140551507609776 [label=AccumulateGrad]
	140551507563088 -> 140551507562992
	140551507563088 [label=ConvolutionBackward0]
	140551507611120 -> 140551507563088
	140551507611120 [label=GeluBackward0]
	140551793674368 -> 140551507611120
	140551793674368 [label=ConvolutionBackward0]
	140551793673600 -> 140551793674368
	140551793673600 [label=PermuteBackward0]
	140551793674560 -> 140551793673600
	140551793674560 [label=NativeLayerNormBackward0]
	140551793674656 -> 140551793674560
	140551793674656 [label=PermuteBackward0]
	140551507563136 -> 140551793674656
	140551793674608 -> 140551793674560
	140550126745056 [label="layers.2.blocks.1.norm2.weight
 (512)" fillcolor=lightblue]
	140550126745056 -> 140551793674608
	140551793674608 [label=AccumulateGrad]
	140551793674464 -> 140551793674560
	140550126745136 [label="layers.2.blocks.1.norm2.bias
 (512)" fillcolor=lightblue]
	140550126745136 -> 140551793674464
	140551793674464 [label=AccumulateGrad]
	140551793674416 -> 140551793674368
	140550126745376 [label="layers.2.blocks.1.mlp.fc1.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	140550126745376 -> 140551793674416
	140551793674416 [label=AccumulateGrad]
	140551793673024 -> 140551793674368
	140550126745456 [label="layers.2.blocks.1.mlp.fc1.bias
 (2048)" fillcolor=lightblue]
	140550126745456 -> 140551793673024
	140551793673024 [label=AccumulateGrad]
	140551507563184 -> 140551507563088
	140550126745616 [label="layers.2.blocks.1.mlp.fc2.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	140550126745616 -> 140551507563184
	140551507563184 [label=AccumulateGrad]
	140551793672256 -> 140551507563088
	140550126745696 [label="layers.2.blocks.1.mlp.fc2.bias
 (512)" fillcolor=lightblue]
	140550126745696 -> 140551793672256
	140551793672256 [label=AccumulateGrad]
	140551507562944 -> 140551507562848
	140551507562944 [label=ConvolutionBackward0]
	140551507563040 -> 140551507562944
	140551507563040 [label=MulBackward0]
	140551793674848 -> 140551507563040
	140551793674848 [label=SplitWithSizesBackward0]
	140551793674896 -> 140551793674848
	140551793674896 [label=ConvolutionBackward0]
	140551793674992 -> 140551793674896
	140551793674992 [label=PermuteBackward0]
	140551793675184 -> 140551793674992
	140551793675184 [label=NativeLayerNormBackward0]
	140551793675280 -> 140551793675184
	140551793675280 [label=PermuteBackward0]
	140551507562992 -> 140551793675280
	140551793675232 -> 140551793675184
	140550126745536 [label="layers.2.blocks.2.norm1.weight
 (512)" fillcolor=lightblue]
	140550126745536 -> 140551793675232
	140551793675232 [label=AccumulateGrad]
	140551793675088 -> 140551793675184
	140550126745776 [label="layers.2.blocks.2.norm1.bias
 (512)" fillcolor=lightblue]
	140550126745776 -> 140551793675088
	140551793675088 [label=AccumulateGrad]
	140551793674944 -> 140551793674896
	140550126745936 [label="layers.2.blocks.2.modulation.f.weight
 (1028, 512, 1, 1)" fillcolor=lightblue]
	140550126745936 -> 140551793674944
	140551793674944 [label=AccumulateGrad]
	140551793674800 -> 140551793674896
	140550126746016 [label="layers.2.blocks.2.modulation.f.bias
 (1028)" fillcolor=lightblue]
	140550126746016 -> 140551793674800
	140551793674800 [label=AccumulateGrad]
	140551793674704 -> 140551507563040
	140551793674704 [label=ConvolutionBackward0]
	140551793675136 -> 140551793674704
	140551793675136 [label=AddBackward0]
	140551793675376 -> 140551793675136
	140551793675376 [label=AddBackward0]
	140551793675616 -> 140551793675376
	140551793675616 [label=AddBackward0]
	140551793675760 -> 140551793675616
	140551793675760 [label=AddBackward0]
	140551793675904 -> 140551793675760
	140551793675904 [label=MulBackward0]
	140551793676000 -> 140551793675904
	140551793676000 [label=GeluBackward0]
	140551793676144 -> 140551793676000
	140551793676144 [label=ConvolutionBackward0]
	140551793674848 -> 140551793676144
	140551793676240 -> 140551793676144
	140550126746656 [label="layers.2.blocks.2.modulation.focal_layers.0.0.weight
 (512, 1, 3, 3)" fillcolor=lightblue]
	140550126746656 -> 140551793676240
	140551793676240 [label=AccumulateGrad]
	140551793675952 -> 140551793675904
	140551793675952 [label=SliceBackward0]
	140551793676048 -> 140551793675952
	140551793676048 [label=SliceBackward0]
	140551793674848 -> 140551793676048
	140551793675712 -> 140551793675616
	140551793675712 [label=MulBackward0]
	140551793676192 -> 140551793675712
	140551793676192 [label=GeluBackward0]
	140551793676096 -> 140551793676192
	140551793676096 [label=ConvolutionBackward0]
	140551793676000 -> 140551793676096
	140551793705168 -> 140551793676096
	140550126746816 [label="layers.2.blocks.2.modulation.focal_layers.1.0.weight
 (512, 1, 5, 5)" fillcolor=lightblue]
	140550126746816 -> 140551793705168
	140551793705168 [label=AccumulateGrad]
	140551793675808 -> 140551793675712
	140551793675808 [label=SliceBackward0]
	140551793705264 -> 140551793675808
	140551793705264 [label=SliceBackward0]
	140551793674848 -> 140551793705264
	140551793675568 -> 140551793675376
	140551793675568 [label=MulBackward0]
	140551793675856 -> 140551793675568
	140551793675856 [label=GeluBackward0]
	140551793705072 -> 140551793675856
	140551793705072 [label=ConvolutionBackward0]
	140551793676192 -> 140551793705072
	140551793705360 -> 140551793705072
	140550126746976 [label="layers.2.blocks.2.modulation.focal_layers.2.0.weight
 (512, 1, 7, 7)" fillcolor=lightblue]
	140550126746976 -> 140551793705360
	140551793705360 [label=AccumulateGrad]
	140551793675664 -> 140551793675568
	140551793675664 [label=SliceBackward0]
	140551793705456 -> 140551793675664
	140551793705456 [label=SliceBackward0]
	140551793674848 -> 140551793705456
	140551793675424 -> 140551793675136
	140551793675424 [label=MulBackward0]
	140551793675520 -> 140551793675424
	140551793675520 [label=GeluBackward0]
	140551793705216 -> 140551793675520
	140551793705216 [label=MeanBackward1]
	140551793675856 -> 140551793705216
	140551793705312 -> 140551793675424
	140551793705312 [label=SliceBackward0]
	140551793705504 -> 140551793705312
	140551793705504 [label=SliceBackward0]
	140551793674848 -> 140551793705504
	140551793675040 -> 140551793674704
	140550126746176 [label="layers.2.blocks.2.modulation.h.weight
 (512, 512, 1, 1)" fillcolor=lightblue]
	140550126746176 -> 140551793675040
	140551793675040 [label=AccumulateGrad]
	140551793674752 -> 140551793674704
	140550126746256 [label="layers.2.blocks.2.modulation.h.bias
 (512)" fillcolor=lightblue]
	140550126746256 -> 140551793674752
	140551793674752 [label=AccumulateGrad]
	140551793673072 -> 140551507562944
	140550126746416 [label="layers.2.blocks.2.modulation.proj.weight
 (512, 512, 1, 1)" fillcolor=lightblue]
	140550126746416 -> 140551793673072
	140551793673072 [label=AccumulateGrad]
	140551793672880 -> 140551507562944
	140550126746496 [label="layers.2.blocks.2.modulation.proj.bias
 (512)" fillcolor=lightblue]
	140550126746496 -> 140551793672880
	140551793672880 [label=AccumulateGrad]
	140551507562800 -> 140551507562704
	140551507562800 [label=ConvolutionBackward0]
	140551507562896 -> 140551507562800
	140551507562896 [label=GeluBackward0]
	140551793675472 -> 140551507562896
	140551793675472 [label=ConvolutionBackward0]
	140551793705024 -> 140551793675472
	140551793705024 [label=PermuteBackward0]
	140551793705744 -> 140551793705024
	140551793705744 [label=NativeLayerNormBackward0]
	140551793705840 -> 140551793705744
	140551793705840 [label=PermuteBackward0]
	140551507562848 -> 140551793705840
	140551793705792 -> 140551793705744
	140550126747056 [label="layers.2.blocks.2.norm2.weight
 (512)" fillcolor=lightblue]
	140550126747056 -> 140551793705792
	140551793705792 [label=AccumulateGrad]
	140551793705648 -> 140551793705744
	140550126747136 [label="layers.2.blocks.2.norm2.bias
 (512)" fillcolor=lightblue]
	140550126747136 -> 140551793705648
	140551793705648 [label=AccumulateGrad]
	140551793705600 -> 140551793675472
	140550126747376 [label="layers.2.blocks.2.mlp.fc1.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	140550126747376 -> 140551793705600
	140551793705600 [label=AccumulateGrad]
	140551793705120 -> 140551793675472
	140550126747456 [label="layers.2.blocks.2.mlp.fc1.bias
 (2048)" fillcolor=lightblue]
	140550126747456 -> 140551793705120
	140551793705120 [label=AccumulateGrad]
	140551793674512 -> 140551507562800
	140550810656832 [label="layers.2.blocks.2.mlp.fc2.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	140550810656832 -> 140551793674512
	140551793674512 [label=AccumulateGrad]
	140551793674224 -> 140551507562800
	140550810656912 [label="layers.2.blocks.2.mlp.fc2.bias
 (512)" fillcolor=lightblue]
	140550810656912 -> 140551793674224
	140551793674224 [label=AccumulateGrad]
	140551507562656 -> 140551507554256
	140551507562656 [label=ConvolutionBackward0]
	140551793675328 -> 140551507562656
	140551793675328 [label=MulBackward0]
	140551793706032 -> 140551793675328
	140551793706032 [label=SplitWithSizesBackward0]
	140551793706080 -> 140551793706032
	140551793706080 [label=ConvolutionBackward0]
	140551793706176 -> 140551793706080
	140551793706176 [label=PermuteBackward0]
	140551793706368 -> 140551793706176
	140551793706368 [label=NativeLayerNormBackward0]
	140551793706464 -> 140551793706368
	140551793706464 [label=PermuteBackward0]
	140551507562704 -> 140551793706464
	140551793706416 -> 140551793706368
	140550126747536 [label="layers.2.blocks.3.norm1.weight
 (512)" fillcolor=lightblue]
	140550126747536 -> 140551793706416
	140551793706416 [label=AccumulateGrad]
	140551793706272 -> 140551793706368
	140550810656992 [label="layers.2.blocks.3.norm1.bias
 (512)" fillcolor=lightblue]
	140550810656992 -> 140551793706272
	140551793706272 [label=AccumulateGrad]
	140551793706128 -> 140551793706080
	140550810657152 [label="layers.2.blocks.3.modulation.f.weight
 (1028, 512, 1, 1)" fillcolor=lightblue]
	140550810657152 -> 140551793706128
	140551793706128 [label=AccumulateGrad]
	140551793705984 -> 140551793706080
	140550810657232 [label="layers.2.blocks.3.modulation.f.bias
 (1028)" fillcolor=lightblue]
	140550810657232 -> 140551793705984
	140551793705984 [label=AccumulateGrad]
	140551793705888 -> 140551793675328
	140551793705888 [label=ConvolutionBackward0]
	140551793706320 -> 140551793705888
	140551793706320 [label=AddBackward0]
	140551793706560 -> 140551793706320
	140551793706560 [label=AddBackward0]
	140551793706800 -> 140551793706560
	140551793706800 [label=AddBackward0]
	140551793706944 -> 140551793706800
	140551793706944 [label=AddBackward0]
	140551793707088 -> 140551793706944
	140551793707088 [label=MulBackward0]
	140551793707184 -> 140551793707088
	140551793707184 [label=GeluBackward0]
	140551793707328 -> 140551793707184
	140551793707328 [label=ConvolutionBackward0]
	140551793706032 -> 140551793707328
	140551793707424 -> 140551793707328
	140550810657872 [label="layers.2.blocks.3.modulation.focal_layers.0.0.weight
 (512, 1, 3, 3)" fillcolor=lightblue]
	140550810657872 -> 140551793707424
	140551793707424 [label=AccumulateGrad]
	140551793707136 -> 140551793707088
	140551793707136 [label=SliceBackward0]
	140551793707520 -> 140551793707136
	140551793707520 [label=SliceBackward0]
	140551793706032 -> 140551793707520
	140551793706896 -> 140551793706800
	140551793706896 [label=MulBackward0]
	140551793707376 -> 140551793706896
	140551793707376 [label=GeluBackward0]
	140551793707280 -> 140551793707376
	140551793707280 [label=ConvolutionBackward0]
	140551793707184 -> 140551793707280
	140551793707616 -> 140551793707280
	140550810658032 [label="layers.2.blocks.3.modulation.focal_layers.1.0.weight
 (512, 1, 5, 5)" fillcolor=lightblue]
	140550810658032 -> 140551793707616
	140551793707616 [label=AccumulateGrad]
	140551793706992 -> 140551793706896
	140551793706992 [label=SliceBackward0]
	140551793707712 -> 140551793706992
	140551793707712 [label=SliceBackward0]
	140551793706032 -> 140551793707712
	140551793706752 -> 140551793706560
	140551793706752 [label=MulBackward0]
	140551793707568 -> 140551793706752
	140551793707568 [label=GeluBackward0]
	140551793707472 -> 140551793707568
	140551793707472 [label=ConvolutionBackward0]
	140551793707376 -> 140551793707472
	140551793707808 -> 140551793707472
	140550810658192 [label="layers.2.blocks.3.modulation.focal_layers.2.0.weight
 (512, 1, 7, 7)" fillcolor=lightblue]
	140550810658192 -> 140551793707808
	140551793707808 [label=AccumulateGrad]
	140551793707040 -> 140551793706752
	140551793707040 [label=SliceBackward0]
	140551793707904 -> 140551793707040
	140551793707904 [label=SliceBackward0]
	140551793706032 -> 140551793707904
	140551793706608 -> 140551793706320
	140551793706608 [label=MulBackward0]
	140551793707760 -> 140551793706608
	140551793707760 [label=GeluBackward0]
	140551793707664 -> 140551793707760
	140551793707664 [label=MeanBackward1]
	140551793707568 -> 140551793707664
	140551793706848 -> 140551793706608
	140551793706848 [label=SliceBackward0]
	140551793707952 -> 140551793706848
	140551793707952 [label=SliceBackward0]
	140551793706032 -> 140551793707952
	140551793706224 -> 140551793705888
	140550810657392 [label="layers.2.blocks.3.modulation.h.weight
 (512, 512, 1, 1)" fillcolor=lightblue]
	140550810657392 -> 140551793706224
	140551793706224 [label=AccumulateGrad]
	140551793705936 -> 140551793705888
	140550810657472 [label="layers.2.blocks.3.modulation.h.bias
 (512)" fillcolor=lightblue]
	140550810657472 -> 140551793705936
	140551793705936 [label=AccumulateGrad]
	140551507562752 -> 140551507562656
	140550810657632 [label="layers.2.blocks.3.modulation.proj.weight
 (512, 512, 1, 1)" fillcolor=lightblue]
	140550810657632 -> 140551507562752
	140551507562752 [label=AccumulateGrad]
	140551793705552 -> 140551507562656
	140550810657712 [label="layers.2.blocks.3.modulation.proj.bias
 (512)" fillcolor=lightblue]
	140550810657712 -> 140551793705552
	140551793705552 [label=AccumulateGrad]
	140551507554208 -> 140551507554160
	140551507554208 [label=ConvolutionBackward0]
	140551507562608 -> 140551507554208
	140551507562608 [label=GeluBackward0]
	140551793708000 -> 140551507562608
	140551793708000 [label=ConvolutionBackward0]
	140551793707232 -> 140551793708000
	140551793707232 [label=PermuteBackward0]
	140551793708192 -> 140551793707232
	140551793708192 [label=NativeLayerNormBackward0]
	140551793708288 -> 140551793708192
	140551793708288 [label=PermuteBackward0]
	140551507554256 -> 140551793708288
	140551793708240 -> 140551793708192
	140550810658272 [label="layers.2.blocks.3.norm2.weight
 (512)" fillcolor=lightblue]
	140550810658272 -> 140551793708240
	140551793708240 [label=AccumulateGrad]
	140551793708096 -> 140551793708192
	140550810658352 [label="layers.2.blocks.3.norm2.bias
 (512)" fillcolor=lightblue]
	140550810658352 -> 140551793708096
	140551793708096 [label=AccumulateGrad]
	140551793708048 -> 140551793708000
	140550810658592 [label="layers.2.blocks.3.mlp.fc1.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	140550810658592 -> 140551793708048
	140551793708048 [label=AccumulateGrad]
	140551793706656 -> 140551793708000
	140550810658672 [label="layers.2.blocks.3.mlp.fc1.bias
 (2048)" fillcolor=lightblue]
	140550810658672 -> 140551793706656
	140551793706656 [label=AccumulateGrad]
	140551793705696 -> 140551507554208
	140550810658832 [label="layers.2.blocks.3.mlp.fc2.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	140550810658832 -> 140551793705696
	140551793705696 [label=AccumulateGrad]
	140551793705408 -> 140551507554208
	140550810658912 [label="layers.2.blocks.3.mlp.fc2.bias
 (512)" fillcolor=lightblue]
	140550810658912 -> 140551793705408
	140551793705408 [label=AccumulateGrad]
	140551507554112 -> 140551507554016
	140551507554112 [label=ConvolutionBackward0]
	140551507562560 -> 140551507554112
	140551507562560 [label=MulBackward0]
	140551793708480 -> 140551507562560
	140551793708480 [label=SplitWithSizesBackward0]
	140551793708528 -> 140551793708480
	140551793708528 [label=ConvolutionBackward0]
	140551793708624 -> 140551793708528
	140551793708624 [label=PermuteBackward0]
	140551793708816 -> 140551793708624
	140551793708816 [label=NativeLayerNormBackward0]
	140551793708912 -> 140551793708816
	140551793708912 [label=PermuteBackward0]
	140551507554160 -> 140551793708912
	140551793708864 -> 140551793708816
	140550810658752 [label="layers.2.blocks.4.norm1.weight
 (512)" fillcolor=lightblue]
	140550810658752 -> 140551793708864
	140551793708864 [label=AccumulateGrad]
	140551793708720 -> 140551793708816
	140550810658992 [label="layers.2.blocks.4.norm1.bias
 (512)" fillcolor=lightblue]
	140550810658992 -> 140551793708720
	140551793708720 [label=AccumulateGrad]
	140551793708576 -> 140551793708528
	140550810659152 [label="layers.2.blocks.4.modulation.f.weight
 (1028, 512, 1, 1)" fillcolor=lightblue]
	140550810659152 -> 140551793708576
	140551793708576 [label=AccumulateGrad]
	140551793708432 -> 140551793708528
	140550810659232 [label="layers.2.blocks.4.modulation.f.bias
 (1028)" fillcolor=lightblue]
	140550810659232 -> 140551793708432
	140551793708432 [label=AccumulateGrad]
	140551793708336 -> 140551507562560
	140551793708336 [label=ConvolutionBackward0]
	140551793708768 -> 140551793708336
	140551793708768 [label=AddBackward0]
	140551793709008 -> 140551793708768
	140551793709008 [label=AddBackward0]
	140551793729792 -> 140551793709008
	140551793729792 [label=AddBackward0]
	140551793729936 -> 140551793729792
	140551793729936 [label=AddBackward0]
	140551793730080 -> 140551793729936
	140551793730080 [label=MulBackward0]
	140551793730176 -> 140551793730080
	140551793730176 [label=GeluBackward0]
	140551793730320 -> 140551793730176
	140551793730320 [label=ConvolutionBackward0]
	140551793708480 -> 140551793730320
	140551793730416 -> 140551793730320
	140550810659872 [label="layers.2.blocks.4.modulation.focal_layers.0.0.weight
 (512, 1, 3, 3)" fillcolor=lightblue]
	140550810659872 -> 140551793730416
	140551793730416 [label=AccumulateGrad]
	140551793730128 -> 140551793730080
	140551793730128 [label=SliceBackward0]
	140551793730512 -> 140551793730128
	140551793730512 [label=SliceBackward0]
	140551793708480 -> 140551793730512
	140551793729888 -> 140551793729792
	140551793729888 [label=MulBackward0]
	140551793730368 -> 140551793729888
	140551793730368 [label=GeluBackward0]
	140551793730272 -> 140551793730368
	140551793730272 [label=ConvolutionBackward0]
	140551793730176 -> 140551793730272
	140551793730608 -> 140551793730272
	140550810660032 [label="layers.2.blocks.4.modulation.focal_layers.1.0.weight
 (512, 1, 5, 5)" fillcolor=lightblue]
	140550810660032 -> 140551793730608
	140551793730608 [label=AccumulateGrad]
	140551793729984 -> 140551793729888
	140551793729984 [label=SliceBackward0]
	140551793730704 -> 140551793729984
	140551793730704 [label=SliceBackward0]
	140551793708480 -> 140551793730704
	140551793729744 -> 140551793709008
	140551793729744 [label=MulBackward0]
	140551793730560 -> 140551793729744
	140551793730560 [label=GeluBackward0]
	140551793730464 -> 140551793730560
	140551793730464 [label=ConvolutionBackward0]
	140551793730368 -> 140551793730464
	140551793730800 -> 140551793730464
	140550810660192 [label="layers.2.blocks.4.modulation.focal_layers.2.0.weight
 (512, 1, 7, 7)" fillcolor=lightblue]
	140550810660192 -> 140551793730800
	140551793730800 [label=AccumulateGrad]
	140551793730032 -> 140551793729744
	140551793730032 [label=SliceBackward0]
	140551793730896 -> 140551793730032
	140551793730896 [label=SliceBackward0]
	140551793708480 -> 140551793730896
	140551793729600 -> 140551793708768
	140551793729600 [label=MulBackward0]
	140551793730752 -> 140551793729600
	140551793730752 [label=GeluBackward0]
	140551793730656 -> 140551793730752
	140551793730656 [label=MeanBackward1]
	140551793730560 -> 140551793730656
	140551793729840 -> 140551793729600
	140551793729840 [label=SliceBackward0]
	140551793730944 -> 140551793729840
	140551793730944 [label=SliceBackward0]
	140551793708480 -> 140551793730944
	140551793708672 -> 140551793708336
	140550810659392 [label="layers.2.blocks.4.modulation.h.weight
 (512, 512, 1, 1)" fillcolor=lightblue]
	140550810659392 -> 140551793708672
	140551793708672 [label=AccumulateGrad]
	140551793708384 -> 140551793708336
	140550810659472 [label="layers.2.blocks.4.modulation.h.bias
 (512)" fillcolor=lightblue]
	140550810659472 -> 140551793708384
	140551793708384 [label=AccumulateGrad]
	140551793706704 -> 140551507554112
	140550810659632 [label="layers.2.blocks.4.modulation.proj.weight
 (512, 512, 1, 1)" fillcolor=lightblue]
	140550810659632 -> 140551793706704
	140551793706704 [label=AccumulateGrad]
	140551793706512 -> 140551507554112
	140550810659712 [label="layers.2.blocks.4.modulation.proj.bias
 (512)" fillcolor=lightblue]
	140550810659712 -> 140551793706512
	140551793706512 [label=AccumulateGrad]
	140551507553968 -> 140551507553872
	140551507553968 [label=ConvolutionBackward0]
	140551507554064 -> 140551507553968
	140551507554064 [label=GeluBackward0]
	140551793730992 -> 140551507554064
	140551793730992 [label=ConvolutionBackward0]
	140551793730224 -> 140551793730992
	140551793730224 [label=PermuteBackward0]
	140551793731184 -> 140551793730224
	140551793731184 [label=NativeLayerNormBackward0]
	140551793731280 -> 140551793731184
	140551793731280 [label=PermuteBackward0]
	140551507554016 -> 140551793731280
	140551793731232 -> 140551793731184
	140550810660272 [label="layers.2.blocks.4.norm2.weight
 (512)" fillcolor=lightblue]
	140550810660272 -> 140551793731232
	140551793731232 [label=AccumulateGrad]
	140551793731088 -> 140551793731184
	140550810660352 [label="layers.2.blocks.4.norm2.bias
 (512)" fillcolor=lightblue]
	140550810660352 -> 140551793731088
	140551793731088 [label=AccumulateGrad]
	140551793731040 -> 140551793730992
	140550810660592 [label="layers.2.blocks.4.mlp.fc1.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	140550810660592 -> 140551793731040
	140551793731040 [label=AccumulateGrad]
	140551793729648 -> 140551793730992
	140550810660672 [label="layers.2.blocks.4.mlp.fc1.bias
 (2048)" fillcolor=lightblue]
	140550810660672 -> 140551793729648
	140551793729648 [label=AccumulateGrad]
	140551793708144 -> 140551507553968
	140550147084352 [label="layers.2.blocks.4.mlp.fc2.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	140550147084352 -> 140551793708144
	140551793708144 [label=AccumulateGrad]
	140551793707856 -> 140551507553968
	140550147084432 [label="layers.2.blocks.4.mlp.fc2.bias
 (512)" fillcolor=lightblue]
	140550147084432 -> 140551793707856
	140551793707856 [label=AccumulateGrad]
	140551507553824 -> 140551507553728
	140551507553824 [label=ConvolutionBackward0]
	140551793708960 -> 140551507553824
	140551793708960 [label=MulBackward0]
	140551793731472 -> 140551793708960
	140551793731472 [label=SplitWithSizesBackward0]
	140551793731520 -> 140551793731472
	140551793731520 [label=ConvolutionBackward0]
	140551793731616 -> 140551793731520
	140551793731616 [label=PermuteBackward0]
	140551793731808 -> 140551793731616
	140551793731808 [label=NativeLayerNormBackward0]
	140551793731904 -> 140551793731808
	140551793731904 [label=PermuteBackward0]
	140551507553872 -> 140551793731904
	140551793731856 -> 140551793731808
	140550810660752 [label="layers.2.blocks.5.norm1.weight
 (512)" fillcolor=lightblue]
	140550810660752 -> 140551793731856
	140551793731856 [label=AccumulateGrad]
	140551793731712 -> 140551793731808
	140550147084512 [label="layers.2.blocks.5.norm1.bias
 (512)" fillcolor=lightblue]
	140550147084512 -> 140551793731712
	140551793731712 [label=AccumulateGrad]
	140551793731568 -> 140551793731520
	140550147084672 [label="layers.2.blocks.5.modulation.f.weight
 (1028, 512, 1, 1)" fillcolor=lightblue]
	140550147084672 -> 140551793731568
	140551793731568 [label=AccumulateGrad]
	140551793731424 -> 140551793731520
	140550147084752 [label="layers.2.blocks.5.modulation.f.bias
 (1028)" fillcolor=lightblue]
	140550147084752 -> 140551793731424
	140551793731424 [label=AccumulateGrad]
	140551793731328 -> 140551793708960
	140551793731328 [label=ConvolutionBackward0]
	140551793731760 -> 140551793731328
	140551793731760 [label=AddBackward0]
	140551793732000 -> 140551793731760
	140551793732000 [label=AddBackward0]
	140551793732240 -> 140551793732000
	140551793732240 [label=AddBackward0]
	140551793732384 -> 140551793732240
	140551793732384 [label=AddBackward0]
	140551793732528 -> 140551793732384
	140551793732528 [label=MulBackward0]
	140551793732624 -> 140551793732528
	140551793732624 [label=GeluBackward0]
	140551793732768 -> 140551793732624
	140551793732768 [label=ConvolutionBackward0]
	140551793731472 -> 140551793732768
	140551793732864 -> 140551793732768
	140550147085392 [label="layers.2.blocks.5.modulation.focal_layers.0.0.weight
 (512, 1, 3, 3)" fillcolor=lightblue]
	140550147085392 -> 140551793732864
	140551793732864 [label=AccumulateGrad]
	140551793732576 -> 140551793732528
	140551793732576 [label=SliceBackward0]
	140551793732960 -> 140551793732576
	140551793732960 [label=SliceBackward0]
	140551793731472 -> 140551793732960
	140551793732336 -> 140551793732240
	140551793732336 [label=MulBackward0]
	140551793732816 -> 140551793732336
	140551793732816 [label=GeluBackward0]
	140551793732720 -> 140551793732816
	140551793732720 [label=ConvolutionBackward0]
	140551793732624 -> 140551793732720
	140551793733056 -> 140551793732720
	140550147085552 [label="layers.2.blocks.5.modulation.focal_layers.1.0.weight
 (512, 1, 5, 5)" fillcolor=lightblue]
	140550147085552 -> 140551793733056
	140551793733056 [label=AccumulateGrad]
	140551793732432 -> 140551793732336
	140551793732432 [label=SliceBackward0]
	140551793733152 -> 140551793732432
	140551793733152 [label=SliceBackward0]
	140551793731472 -> 140551793733152
	140551793732192 -> 140551793732000
	140551793732192 [label=MulBackward0]
	140551793733008 -> 140551793732192
	140551793733008 [label=GeluBackward0]
	140551793732912 -> 140551793733008
	140551793732912 [label=ConvolutionBackward0]
	140551793732816 -> 140551793732912
	140551793733248 -> 140551793732912
	140550147085712 [label="layers.2.blocks.5.modulation.focal_layers.2.0.weight
 (512, 1, 7, 7)" fillcolor=lightblue]
	140550147085712 -> 140551793733248
	140551793733248 [label=AccumulateGrad]
	140551793732480 -> 140551793732192
	140551793732480 [label=SliceBackward0]
	140551793733344 -> 140551793732480
	140551793733344 [label=SliceBackward0]
	140551793731472 -> 140551793733344
	140551793732048 -> 140551793731760
	140551793732048 [label=MulBackward0]
	140551793733200 -> 140551793732048
	140551793733200 [label=GeluBackward0]
	140551793733104 -> 140551793733200
	140551793733104 [label=MeanBackward1]
	140551793733008 -> 140551793733104
	140551793732288 -> 140551793732048
	140551793732288 [label=SliceBackward0]
	140551793733392 -> 140551793732288
	140551793733392 [label=SliceBackward0]
	140551793731472 -> 140551793733392
	140551793731664 -> 140551793731328
	140550147084912 [label="layers.2.blocks.5.modulation.h.weight
 (512, 512, 1, 1)" fillcolor=lightblue]
	140550147084912 -> 140551793731664
	140551793731664 [label=AccumulateGrad]
	140551793731376 -> 140551793731328
	140550147084992 [label="layers.2.blocks.5.modulation.h.bias
 (512)" fillcolor=lightblue]
	140550147084992 -> 140551793731376
	140551793731376 [label=AccumulateGrad]
	140551507553920 -> 140551507553824
	140550147085152 [label="layers.2.blocks.5.modulation.proj.weight
 (512, 512, 1, 1)" fillcolor=lightblue]
	140550147085152 -> 140551507553920
	140551507553920 [label=AccumulateGrad]
	140551793729696 -> 140551507553824
	140550147085232 [label="layers.2.blocks.5.modulation.proj.bias
 (512)" fillcolor=lightblue]
	140550147085232 -> 140551793729696
	140551793729696 [label=AccumulateGrad]
	140551507553680 -> 140551507553584
	140551507553680 [label=ConvolutionBackward0]
	140551507553776 -> 140551507553680
	140551507553776 [label=GeluBackward0]
	140551793733440 -> 140551507553776
	140551793733440 [label=ConvolutionBackward0]
	140551793732672 -> 140551793733440
	140551793732672 [label=PermuteBackward0]
	140551793733584 -> 140551793732672
	140551793733584 [label=NativeLayerNormBackward0]
	140551793762464 -> 140551793733584
	140551793762464 [label=PermuteBackward0]
	140551507553728 -> 140551793762464
	140551793762416 -> 140551793733584
	140550147085792 [label="layers.2.blocks.5.norm2.weight
 (512)" fillcolor=lightblue]
	140550147085792 -> 140551793762416
	140551793762416 [label=AccumulateGrad]
	140551793762368 -> 140551793733584
	140550147085872 [label="layers.2.blocks.5.norm2.bias
 (512)" fillcolor=lightblue]
	140550147085872 -> 140551793762368
	140551793762368 [label=AccumulateGrad]
	140551793733488 -> 140551793733440
	140550147086112 [label="layers.2.blocks.5.mlp.fc1.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	140550147086112 -> 140551793733488
	140551793733488 [label=AccumulateGrad]
	140551793732096 -> 140551793733440
	140550147086192 [label="layers.2.blocks.5.mlp.fc1.bias
 (2048)" fillcolor=lightblue]
	140550147086192 -> 140551793732096
	140551793732096 [label=AccumulateGrad]
	140551793731136 -> 140551507553680
	140550147086352 [label="layers.2.blocks.5.mlp.fc2.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	140550147086352 -> 140551793731136
	140551793731136 [label=AccumulateGrad]
	140551793730848 -> 140551507553680
	140550147086432 [label="layers.2.blocks.5.mlp.fc2.bias
 (512)" fillcolor=lightblue]
	140550147086432 -> 140551793730848
	140551793730848 [label=AccumulateGrad]
	140551507553536 -> 140551507553440
	140551507553536 [label=ConvolutionBackward0]
	140551507553632 -> 140551507553536
	140551507553632 [label=MulBackward0]
	140551793733536 -> 140551507553632
	140551793733536 [label=SplitWithSizesBackward0]
	140551793762704 -> 140551793733536
	140551793762704 [label=ConvolutionBackward0]
	140551793762800 -> 140551793762704
	140551793762800 [label=PermuteBackward0]
	140551793762992 -> 140551793762800
	140551793762992 [label=NativeLayerNormBackward0]
	140551793763088 -> 140551793762992
	140551793763088 [label=PermuteBackward0]
	140551507553584 -> 140551793763088
	140551793763040 -> 140551793762992
	140550147086272 [label="layers.2.blocks.6.norm1.weight
 (512)" fillcolor=lightblue]
	140550147086272 -> 140551793763040
	140551793763040 [label=AccumulateGrad]
	140551793762896 -> 140551793762992
	140550147086512 [label="layers.2.blocks.6.norm1.bias
 (512)" fillcolor=lightblue]
	140550147086512 -> 140551793762896
	140551793762896 [label=AccumulateGrad]
	140551793762752 -> 140551793762704
	140550147086672 [label="layers.2.blocks.6.modulation.f.weight
 (1028, 512, 1, 1)" fillcolor=lightblue]
	140550147086672 -> 140551793762752
	140551793762752 [label=AccumulateGrad]
	140551793762608 -> 140551793762704
	140550147086752 [label="layers.2.blocks.6.modulation.f.bias
 (1028)" fillcolor=lightblue]
	140550147086752 -> 140551793762608
	140551793762608 [label=AccumulateGrad]
	140551793762656 -> 140551507553632
	140551793762656 [label=ConvolutionBackward0]
	140551793762944 -> 140551793762656
	140551793762944 [label=AddBackward0]
	140551793763184 -> 140551793762944
	140551793763184 [label=AddBackward0]
	140551793763424 -> 140551793763184
	140551793763424 [label=AddBackward0]
	140551793763568 -> 140551793763424
	140551793763568 [label=AddBackward0]
	140551793763712 -> 140551793763568
	140551793763712 [label=MulBackward0]
	140551793763808 -> 140551793763712
	140551793763808 [label=GeluBackward0]
	140551793763952 -> 140551793763808
	140551793763952 [label=ConvolutionBackward0]
	140551793733536 -> 140551793763952
	140551793764048 -> 140551793763952
	140550147087392 [label="layers.2.blocks.6.modulation.focal_layers.0.0.weight
 (512, 1, 3, 3)" fillcolor=lightblue]
	140550147087392 -> 140551793764048
	140551793764048 [label=AccumulateGrad]
	140551793763760 -> 140551793763712
	140551793763760 [label=SliceBackward0]
	140551793764144 -> 140551793763760
	140551793764144 [label=SliceBackward0]
	140551793733536 -> 140551793764144
	140551793763520 -> 140551793763424
	140551793763520 [label=MulBackward0]
	140551793764000 -> 140551793763520
	140551793764000 [label=GeluBackward0]
	140551793763904 -> 140551793764000
	140551793763904 [label=ConvolutionBackward0]
	140551793763808 -> 140551793763904
	140551793764240 -> 140551793763904
	140550147087552 [label="layers.2.blocks.6.modulation.focal_layers.1.0.weight
 (512, 1, 5, 5)" fillcolor=lightblue]
	140550147087552 -> 140551793764240
	140551793764240 [label=AccumulateGrad]
	140551793763616 -> 140551793763520
	140551793763616 [label=SliceBackward0]
	140551793764336 -> 140551793763616
	140551793764336 [label=SliceBackward0]
	140551793733536 -> 140551793764336
	140551793763376 -> 140551793763184
	140551793763376 [label=MulBackward0]
	140551793764192 -> 140551793763376
	140551793764192 [label=GeluBackward0]
	140551793764096 -> 140551793764192
	140551793764096 [label=ConvolutionBackward0]
	140551793764000 -> 140551793764096
	140551793764432 -> 140551793764096
	140550147087712 [label="layers.2.blocks.6.modulation.focal_layers.2.0.weight
 (512, 1, 7, 7)" fillcolor=lightblue]
	140550147087712 -> 140551793764432
	140551793764432 [label=AccumulateGrad]
	140551793763664 -> 140551793763376
	140551793763664 [label=SliceBackward0]
	140551793764528 -> 140551793763664
	140551793764528 [label=SliceBackward0]
	140551793733536 -> 140551793764528
	140551793763232 -> 140551793762944
	140551793763232 [label=MulBackward0]
	140551793764384 -> 140551793763232
	140551793764384 [label=GeluBackward0]
	140551793764288 -> 140551793764384
	140551793764288 [label=MeanBackward1]
	140551793764192 -> 140551793764288
	140551793763472 -> 140551793763232
	140551793763472 [label=SliceBackward0]
	140551793764576 -> 140551793763472
	140551793764576 [label=SliceBackward0]
	140551793733536 -> 140551793764576
	140551793762848 -> 140551793762656
	140550147086912 [label="layers.2.blocks.6.modulation.h.weight
 (512, 512, 1, 1)" fillcolor=lightblue]
	140550147086912 -> 140551793762848
	140551793762848 [label=AccumulateGrad]
	140551793762560 -> 140551793762656
	140550147086992 [label="layers.2.blocks.6.modulation.h.bias
 (512)" fillcolor=lightblue]
	140550147086992 -> 140551793762560
	140551793762560 [label=AccumulateGrad]
	140551793732144 -> 140551507553536
	140550147087152 [label="layers.2.blocks.6.modulation.proj.weight
 (512, 512, 1, 1)" fillcolor=lightblue]
	140550147087152 -> 140551793732144
	140551793732144 [label=AccumulateGrad]
	140551793731952 -> 140551507553536
	140550147087232 [label="layers.2.blocks.6.modulation.proj.bias
 (512)" fillcolor=lightblue]
	140550147087232 -> 140551793731952
	140551793731952 [label=AccumulateGrad]
	140551507553392 -> 140551507553296
	140551507553392 [label=ConvolutionBackward0]
	140551793733296 -> 140551507553392
	140551793733296 [label=GeluBackward0]
	140551793764624 -> 140551793733296
	140551793764624 [label=ConvolutionBackward0]
	140551793763856 -> 140551793764624
	140551793763856 [label=PermuteBackward0]
	140551793764816 -> 140551793763856
	140551793764816 [label=NativeLayerNormBackward0]
	140551793764912 -> 140551793764816
	140551793764912 [label=PermuteBackward0]
	140551507553440 -> 140551793764912
	140551793764864 -> 140551793764816
	140550147087792 [label="layers.2.blocks.6.norm2.weight
 (512)" fillcolor=lightblue]
	140550147087792 -> 140551793764864
	140551793764864 [label=AccumulateGrad]
	140551793764720 -> 140551793764816
	140550147087872 [label="layers.2.blocks.6.norm2.bias
 (512)" fillcolor=lightblue]
	140550147087872 -> 140551793764720
	140551793764720 [label=AccumulateGrad]
	140551793764672 -> 140551793764624
	140550147088112 [label="layers.2.blocks.6.mlp.fc1.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	140550147088112 -> 140551793764672
	140551793764672 [label=AccumulateGrad]
	140551793763280 -> 140551793764624
	140550147088192 [label="layers.2.blocks.6.mlp.fc1.bias
 (2048)" fillcolor=lightblue]
	140550147088192 -> 140551793763280
	140551793763280 [label=AccumulateGrad]
	140551507553488 -> 140551507553392
	140551477600320 [label="layers.2.blocks.6.mlp.fc2.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	140551477600320 -> 140551507553488
	140551507553488 [label=AccumulateGrad]
	140551793762512 -> 140551507553392
	140551477600400 [label="layers.2.blocks.6.mlp.fc2.bias
 (512)" fillcolor=lightblue]
	140551477600400 -> 140551793762512
	140551793762512 [label=AccumulateGrad]
	140551507553248 -> 140551507553152
	140551507553248 [label=ConvolutionBackward0]
	140551507553344 -> 140551507553248
	140551507553344 [label=MulBackward0]
	140551793765104 -> 140551507553344
	140551793765104 [label=SplitWithSizesBackward0]
	140551793765152 -> 140551793765104
	140551793765152 [label=ConvolutionBackward0]
	140551793765248 -> 140551793765152
	140551793765248 [label=PermuteBackward0]
	140551793765440 -> 140551793765248
	140551793765440 [label=NativeLayerNormBackward0]
	140551793765536 -> 140551793765440
	140551793765536 [label=PermuteBackward0]
	140551507553296 -> 140551793765536
	140551793765488 -> 140551793765440
	140550147088272 [label="layers.2.blocks.7.norm1.weight
 (512)" fillcolor=lightblue]
	140550147088272 -> 140551793765488
	140551793765488 [label=AccumulateGrad]
	140551793765344 -> 140551793765440
	140551477600480 [label="layers.2.blocks.7.norm1.bias
 (512)" fillcolor=lightblue]
	140551477600480 -> 140551793765344
	140551793765344 [label=AccumulateGrad]
	140551793765200 -> 140551793765152
	140551477600640 [label="layers.2.blocks.7.modulation.f.weight
 (1028, 512, 1, 1)" fillcolor=lightblue]
	140551477600640 -> 140551793765200
	140551793765200 [label=AccumulateGrad]
	140551793765056 -> 140551793765152
	140551477600720 [label="layers.2.blocks.7.modulation.f.bias
 (1028)" fillcolor=lightblue]
	140551477600720 -> 140551793765056
	140551793765056 [label=AccumulateGrad]
	140551793764960 -> 140551507553344
	140551793764960 [label=ConvolutionBackward0]
	140551793765392 -> 140551793764960
	140551793765392 [label=AddBackward0]
	140551793765632 -> 140551793765392
	140551793765632 [label=AddBackward0]
	140551793765872 -> 140551793765632
	140551793765872 [label=AddBackward0]
	140551793766016 -> 140551793765872
	140551793766016 [label=AddBackward0]
	140551793766160 -> 140551793766016
	140551793766160 [label=MulBackward0]
	140551793766256 -> 140551793766160
	140551793766256 [label=GeluBackward0]
	140551793766352 -> 140551793766256
	140551793766352 [label=ConvolutionBackward0]
	140551793765104 -> 140551793766352
	140551793787040 -> 140551793766352
	140551477601360 [label="layers.2.blocks.7.modulation.focal_layers.0.0.weight
 (512, 1, 3, 3)" fillcolor=lightblue]
	140551477601360 -> 140551793787040
	140551793787040 [label=AccumulateGrad]
	140551793766208 -> 140551793766160
	140551793766208 [label=SliceBackward0]
	140551793766304 -> 140551793766208
	140551793766304 [label=SliceBackward0]
	140551793765104 -> 140551793766304
	140551793765968 -> 140551793765872
	140551793765968 [label=MulBackward0]
	140551793766064 -> 140551793765968
	140551793766064 [label=GeluBackward0]
	140551793786992 -> 140551793766064
	140551793786992 [label=ConvolutionBackward0]
	140551793766256 -> 140551793786992
	140551793787232 -> 140551793786992
	140551477601520 [label="layers.2.blocks.7.modulation.focal_layers.1.0.weight
 (512, 1, 5, 5)" fillcolor=lightblue]
	140551477601520 -> 140551793787232
	140551793787232 [label=AccumulateGrad]
	140551793766112 -> 140551793765968
	140551793766112 [label=SliceBackward0]
	140551793787328 -> 140551793766112
	140551793787328 [label=SliceBackward0]
	140551793765104 -> 140551793787328
	140551793765824 -> 140551793765632
	140551793765824 [label=MulBackward0]
	140551793765920 -> 140551793765824
	140551793765920 [label=GeluBackward0]
	140551793787088 -> 140551793765920
	140551793787088 [label=ConvolutionBackward0]
	140551793766064 -> 140551793787088
	140551793787424 -> 140551793787088
	140551477601680 [label="layers.2.blocks.7.modulation.focal_layers.2.0.weight
 (512, 1, 7, 7)" fillcolor=lightblue]
	140551477601680 -> 140551793787424
	140551793787424 [label=AccumulateGrad]
	140551793787184 -> 140551793765824
	140551793787184 [label=SliceBackward0]
	140551793787520 -> 140551793787184
	140551793787520 [label=SliceBackward0]
	140551793765104 -> 140551793787520
	140551793765680 -> 140551793765392
	140551793765680 [label=MulBackward0]
	140551793765776 -> 140551793765680
	140551793765776 [label=GeluBackward0]
	140551793787280 -> 140551793765776
	140551793787280 [label=MeanBackward1]
	140551793765920 -> 140551793787280
	140551793787376 -> 140551793765680
	140551793787376 [label=SliceBackward0]
	140551793787568 -> 140551793787376
	140551793787568 [label=SliceBackward0]
	140551793765104 -> 140551793787568
	140551793765296 -> 140551793764960
	140551477600880 [label="layers.2.blocks.7.modulation.h.weight
 (512, 512, 1, 1)" fillcolor=lightblue]
	140551477600880 -> 140551793765296
	140551793765296 [label=AccumulateGrad]
	140551793765008 -> 140551793764960
	140551477600960 [label="layers.2.blocks.7.modulation.h.bias
 (512)" fillcolor=lightblue]
	140551477600960 -> 140551793765008
	140551793765008 [label=AccumulateGrad]
	140551793763328 -> 140551507553248
	140551477601120 [label="layers.2.blocks.7.modulation.proj.weight
 (512, 512, 1, 1)" fillcolor=lightblue]
	140551477601120 -> 140551793763328
	140551793763328 [label=AccumulateGrad]
	140551793763136 -> 140551507553248
	140551477601200 [label="layers.2.blocks.7.modulation.proj.bias
 (512)" fillcolor=lightblue]
	140551477601200 -> 140551793763136
	140551793763136 [label=AccumulateGrad]
	140551507553104 -> 140551507553008
	140551507553104 [label=ConvolutionBackward0]
	140551507553200 -> 140551507553104
	140551507553200 [label=GeluBackward0]
	140551793765728 -> 140551507553200
	140551793765728 [label=ConvolutionBackward0]
	140551793786944 -> 140551793765728
	140551793786944 [label=PermuteBackward0]
	140551793787808 -> 140551793786944
	140551793787808 [label=NativeLayerNormBackward0]
	140551793787904 -> 140551793787808
	140551793787904 [label=PermuteBackward0]
	140551507553152 -> 140551793787904
	140551793787856 -> 140551793787808
	140551477601760 [label="layers.2.blocks.7.norm2.weight
 (512)" fillcolor=lightblue]
	140551477601760 -> 140551793787856
	140551793787856 [label=AccumulateGrad]
	140551793787712 -> 140551793787808
	140551477601840 [label="layers.2.blocks.7.norm2.bias
 (512)" fillcolor=lightblue]
	140551477601840 -> 140551793787712
	140551793787712 [label=AccumulateGrad]
	140551793787664 -> 140551793765728
	140551477602080 [label="layers.2.blocks.7.mlp.fc1.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	140551477602080 -> 140551793787664
	140551793787664 [label=AccumulateGrad]
	140551793787136 -> 140551793765728
	140551477602160 [label="layers.2.blocks.7.mlp.fc1.bias
 (2048)" fillcolor=lightblue]
	140551477602160 -> 140551793787136
	140551793787136 [label=AccumulateGrad]
	140551793764768 -> 140551507553104
	140551477602320 [label="layers.2.blocks.7.mlp.fc2.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	140551477602320 -> 140551793764768
	140551793764768 [label=AccumulateGrad]
	140551793764480 -> 140551507553104
	140551477602400 [label="layers.2.blocks.7.mlp.fc2.bias
 (512)" fillcolor=lightblue]
	140551477602400 -> 140551793764480
	140551793764480 [label=AccumulateGrad]
	140551507552960 -> 140551507552864
	140551507552960 [label=ConvolutionBackward0]
	140551793765584 -> 140551507552960
	140551793765584 [label=MulBackward0]
	140551793788096 -> 140551793765584
	140551793788096 [label=SplitWithSizesBackward0]
	140551793788144 -> 140551793788096
	140551793788144 [label=ConvolutionBackward0]
	140551793788240 -> 140551793788144
	140551793788240 [label=PermuteBackward0]
	140551793788432 -> 140551793788240
	140551793788432 [label=NativeLayerNormBackward0]
	140551793788528 -> 140551793788432
	140551793788528 [label=PermuteBackward0]
	140551507553008 -> 140551793788528
	140551793788480 -> 140551793788432
	140551477602240 [label="layers.2.blocks.8.norm1.weight
 (512)" fillcolor=lightblue]
	140551477602240 -> 140551793788480
	140551793788480 [label=AccumulateGrad]
	140551793788336 -> 140551793788432
	140551477602480 [label="layers.2.blocks.8.norm1.bias
 (512)" fillcolor=lightblue]
	140551477602480 -> 140551793788336
	140551793788336 [label=AccumulateGrad]
	140551793788192 -> 140551793788144
	140551477602640 [label="layers.2.blocks.8.modulation.f.weight
 (1028, 512, 1, 1)" fillcolor=lightblue]
	140551477602640 -> 140551793788192
	140551793788192 [label=AccumulateGrad]
	140551793788048 -> 140551793788144
	140551477602720 [label="layers.2.blocks.8.modulation.f.bias
 (1028)" fillcolor=lightblue]
	140551477602720 -> 140551793788048
	140551793788048 [label=AccumulateGrad]
	140551793787952 -> 140551793765584
	140551793787952 [label=ConvolutionBackward0]
	140551793788384 -> 140551793787952
	140551793788384 [label=AddBackward0]
	140551793788624 -> 140551793788384
	140551793788624 [label=AddBackward0]
	140551793788864 -> 140551793788624
	140551793788864 [label=AddBackward0]
	140551793789008 -> 140551793788864
	140551793789008 [label=AddBackward0]
	140551793789152 -> 140551793789008
	140551793789152 [label=MulBackward0]
	140551793789248 -> 140551793789152
	140551793789248 [label=GeluBackward0]
	140551793789392 -> 140551793789248
	140551793789392 [label=ConvolutionBackward0]
	140551793788096 -> 140551793789392
	140551793789488 -> 140551793789392
	140551477603360 [label="layers.2.blocks.8.modulation.focal_layers.0.0.weight
 (512, 1, 3, 3)" fillcolor=lightblue]
	140551477603360 -> 140551793789488
	140551793789488 [label=AccumulateGrad]
	140551793789200 -> 140551793789152
	140551793789200 [label=SliceBackward0]
	140551793789584 -> 140551793789200
	140551793789584 [label=SliceBackward0]
	140551793788096 -> 140551793789584
	140551793788960 -> 140551793788864
	140551793788960 [label=MulBackward0]
	140551793789440 -> 140551793788960
	140551793789440 [label=GeluBackward0]
	140551793789344 -> 140551793789440
	140551793789344 [label=ConvolutionBackward0]
	140551793789248 -> 140551793789344
	140551793789680 -> 140551793789344
	140551477603520 [label="layers.2.blocks.8.modulation.focal_layers.1.0.weight
 (512, 1, 5, 5)" fillcolor=lightblue]
	140551477603520 -> 140551793789680
	140551793789680 [label=AccumulateGrad]
	140551793789056 -> 140551793788960
	140551793789056 [label=SliceBackward0]
	140551793789776 -> 140551793789056
	140551793789776 [label=SliceBackward0]
	140551793788096 -> 140551793789776
	140551793788816 -> 140551793788624
	140551793788816 [label=MulBackward0]
	140551793789632 -> 140551793788816
	140551793789632 [label=GeluBackward0]
	140551793789536 -> 140551793789632
	140551793789536 [label=ConvolutionBackward0]
	140551793789440 -> 140551793789536
	140551793789872 -> 140551793789536
	140551477603680 [label="layers.2.blocks.8.modulation.focal_layers.2.0.weight
 (512, 1, 7, 7)" fillcolor=lightblue]
	140551477603680 -> 140551793789872
	140551793789872 [label=AccumulateGrad]
	140551793789104 -> 140551793788816
	140551793789104 [label=SliceBackward0]
	140551793789968 -> 140551793789104
	140551793789968 [label=SliceBackward0]
	140551793788096 -> 140551793789968
	140551793788672 -> 140551793788384
	140551793788672 [label=MulBackward0]
	140551793789824 -> 140551793788672
	140551793789824 [label=GeluBackward0]
	140551793789728 -> 140551793789824
	140551793789728 [label=MeanBackward1]
	140551793789632 -> 140551793789728
	140551793788912 -> 140551793788672
	140551793788912 [label=SliceBackward0]
	140551793790016 -> 140551793788912
	140551793790016 [label=SliceBackward0]
	140551793788096 -> 140551793790016
	140551793788288 -> 140551793787952
	140551477602880 [label="layers.2.blocks.8.modulation.h.weight
 (512, 512, 1, 1)" fillcolor=lightblue]
	140551477602880 -> 140551793788288
	140551793788288 [label=AccumulateGrad]
	140551793788000 -> 140551793787952
	140551477602960 [label="layers.2.blocks.8.modulation.h.bias
 (512)" fillcolor=lightblue]
	140551477602960 -> 140551793788000
	140551793788000 [label=AccumulateGrad]
	140551507553056 -> 140551507552960
	140551477603120 [label="layers.2.blocks.8.modulation.proj.weight
 (512, 512, 1, 1)" fillcolor=lightblue]
	140551477603120 -> 140551507553056
	140551507553056 [label=AccumulateGrad]
	140551793787616 -> 140551507552960
	140551477603200 [label="layers.2.blocks.8.modulation.proj.bias
 (512)" fillcolor=lightblue]
	140551477603200 -> 140551793787616
	140551793787616 [label=AccumulateGrad]
	140551507552816 -> 140551507552720
	140551507552816 [label=ConvolutionBackward0]
	140551507552912 -> 140551507552816
	140551507552912 [label=GeluBackward0]
	140551793790064 -> 140551507552912
	140551793790064 [label=ConvolutionBackward0]
	140551793789296 -> 140551793790064
	140551793789296 [label=PermuteBackward0]
	140551793790256 -> 140551793789296
	140551793790256 [label=NativeLayerNormBackward0]
	140551793790352 -> 140551793790256
	140551793790352 [label=PermuteBackward0]
	140551507552864 -> 140551793790352
	140551793790304 -> 140551793790256
	140551477603760 [label="layers.2.blocks.8.norm2.weight
 (512)" fillcolor=lightblue]
	140551477603760 -> 140551793790304
	140551793790304 [label=AccumulateGrad]
	140551793790160 -> 140551793790256
	140551477603840 [label="layers.2.blocks.8.norm2.bias
 (512)" fillcolor=lightblue]
	140551477603840 -> 140551793790160
	140551793790160 [label=AccumulateGrad]
	140551793790112 -> 140551793790064
	140551477604080 [label="layers.2.blocks.8.mlp.fc1.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	140551477604080 -> 140551793790112
	140551793790112 [label=AccumulateGrad]
	140551793788720 -> 140551793790064
	140551477604160 [label="layers.2.blocks.8.mlp.fc1.bias
 (2048)" fillcolor=lightblue]
	140551477604160 -> 140551793788720
	140551793788720 [label=AccumulateGrad]
	140551793787760 -> 140551507552816
	140550830014528 [label="layers.2.blocks.8.mlp.fc2.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	140550830014528 -> 140551793787760
	140551793787760 [label=AccumulateGrad]
	140551793787472 -> 140551507552816
	140550830014608 [label="layers.2.blocks.8.mlp.fc2.bias
 (512)" fillcolor=lightblue]
	140550830014608 -> 140551793787472
	140551793787472 [label=AccumulateGrad]
	140551507552672 -> 140551507552576
	140551507552672 [label=ConvolutionBackward0]
	140551507552768 -> 140551507552672
	140551507552768 [label=MulBackward0]
	140551793790544 -> 140551507552768
	140551793790544 [label=SplitWithSizesBackward0]
	140551793790592 -> 140551793790544
	140551793790592 [label=ConvolutionBackward0]
	140551793790688 -> 140551793790592
	140551793790688 [label=PermuteBackward0]
	140551793790880 -> 140551793790688
	140551793790880 [label=NativeLayerNormBackward0]
	140551793790928 -> 140551793790880
	140551793790928 [label=PermuteBackward0]
	140551507552720 -> 140551793790928
	140551793790784 -> 140551793790880
	140551477604240 [label="layers.2.blocks.9.norm1.weight
 (512)" fillcolor=lightblue]
	140551477604240 -> 140551793790784
	140551793790784 [label=AccumulateGrad]
	140551793827904 -> 140551793790880
	140550830014688 [label="layers.2.blocks.9.norm1.bias
 (512)" fillcolor=lightblue]
	140550830014688 -> 140551793827904
	140551793827904 [label=AccumulateGrad]
	140551793790640 -> 140551793790592
	140550830014848 [label="layers.2.blocks.9.modulation.f.weight
 (1028, 512, 1, 1)" fillcolor=lightblue]
	140550830014848 -> 140551793790640
	140551793790640 [label=AccumulateGrad]
	140551793790496 -> 140551793790592
	140550830014928 [label="layers.2.blocks.9.modulation.f.bias
 (1028)" fillcolor=lightblue]
	140550830014928 -> 140551793790496
	140551793790496 [label=AccumulateGrad]
	140551793790400 -> 140551507552768
	140551793790400 [label=ConvolutionBackward0]
	140551793790832 -> 140551793790400
	140551793790832 [label=AddBackward0]
	140551793828000 -> 140551793790832
	140551793828000 [label=AddBackward0]
	140551793828240 -> 140551793828000
	140551793828240 [label=AddBackward0]
	140551793828384 -> 140551793828240
	140551793828384 [label=AddBackward0]
	140551793828528 -> 140551793828384
	140551793828528 [label=MulBackward0]
	140551793828624 -> 140551793828528
	140551793828624 [label=GeluBackward0]
	140551793828768 -> 140551793828624
	140551793828768 [label=ConvolutionBackward0]
	140551793790544 -> 140551793828768
	140551793828864 -> 140551793828768
	140550830015568 [label="layers.2.blocks.9.modulation.focal_layers.0.0.weight
 (512, 1, 3, 3)" fillcolor=lightblue]
	140550830015568 -> 140551793828864
	140551793828864 [label=AccumulateGrad]
	140551793828576 -> 140551793828528
	140551793828576 [label=SliceBackward0]
	140551793828960 -> 140551793828576
	140551793828960 [label=SliceBackward0]
	140551793790544 -> 140551793828960
	140551793828336 -> 140551793828240
	140551793828336 [label=MulBackward0]
	140551793828816 -> 140551793828336
	140551793828816 [label=GeluBackward0]
	140551793828720 -> 140551793828816
	140551793828720 [label=ConvolutionBackward0]
	140551793828624 -> 140551793828720
	140551793829056 -> 140551793828720
	140550830015728 [label="layers.2.blocks.9.modulation.focal_layers.1.0.weight
 (512, 1, 5, 5)" fillcolor=lightblue]
	140550830015728 -> 140551793829056
	140551793829056 [label=AccumulateGrad]
	140551793828432 -> 140551793828336
	140551793828432 [label=SliceBackward0]
	140551793829152 -> 140551793828432
	140551793829152 [label=SliceBackward0]
	140551793790544 -> 140551793829152
	140551793828192 -> 140551793828000
	140551793828192 [label=MulBackward0]
	140551793829008 -> 140551793828192
	140551793829008 [label=GeluBackward0]
	140551793828912 -> 140551793829008
	140551793828912 [label=ConvolutionBackward0]
	140551793828816 -> 140551793828912
	140551793829248 -> 140551793828912
	140550830015888 [label="layers.2.blocks.9.modulation.focal_layers.2.0.weight
 (512, 1, 7, 7)" fillcolor=lightblue]
	140550830015888 -> 140551793829248
	140551793829248 [label=AccumulateGrad]
	140551793828480 -> 140551793828192
	140551793828480 [label=SliceBackward0]
	140551793829344 -> 140551793828480
	140551793829344 [label=SliceBackward0]
	140551793790544 -> 140551793829344
	140551793828048 -> 140551793790832
	140551793828048 [label=MulBackward0]
	140551793829200 -> 140551793828048
	140551793829200 [label=GeluBackward0]
	140551793829104 -> 140551793829200
	140551793829104 [label=MeanBackward1]
	140551793829008 -> 140551793829104
	140551793828288 -> 140551793828048
	140551793828288 [label=SliceBackward0]
	140551793829392 -> 140551793828288
	140551793829392 [label=SliceBackward0]
	140551793790544 -> 140551793829392
	140551793790736 -> 140551793790400
	140550830015088 [label="layers.2.blocks.9.modulation.h.weight
 (512, 512, 1, 1)" fillcolor=lightblue]
	140550830015088 -> 140551793790736
	140551793790736 [label=AccumulateGrad]
	140551793790448 -> 140551793790400
	140550830015168 [label="layers.2.blocks.9.modulation.h.bias
 (512)" fillcolor=lightblue]
	140550830015168 -> 140551793790448
	140551793790448 [label=AccumulateGrad]
	140551793788768 -> 140551507552672
	140550830015328 [label="layers.2.blocks.9.modulation.proj.weight
 (512, 512, 1, 1)" fillcolor=lightblue]
	140550830015328 -> 140551793788768
	140551793788768 [label=AccumulateGrad]
	140551793788576 -> 140551507552672
	140550830015408 [label="layers.2.blocks.9.modulation.proj.bias
 (512)" fillcolor=lightblue]
	140550830015408 -> 140551793788576
	140551793788576 [label=AccumulateGrad]
	140551507552528 -> 140551507552432
	140551507552528 [label=ConvolutionBackward0]
	140551793790208 -> 140551507552528
	140551793790208 [label=GeluBackward0]
	140551793829440 -> 140551793790208
	140551793829440 [label=ConvolutionBackward0]
	140551793828672 -> 140551793829440
	140551793828672 [label=PermuteBackward0]
	140551793829632 -> 140551793828672
	140551793829632 [label=NativeLayerNormBackward0]
	140551793829728 -> 140551793829632
	140551793829728 [label=PermuteBackward0]
	140551507552576 -> 140551793829728
	140551793829680 -> 140551793829632
	140550830015968 [label="layers.2.blocks.9.norm2.weight
 (512)" fillcolor=lightblue]
	140550830015968 -> 140551793829680
	140551793829680 [label=AccumulateGrad]
	140551793829536 -> 140551793829632
	140550830016048 [label="layers.2.blocks.9.norm2.bias
 (512)" fillcolor=lightblue]
	140550830016048 -> 140551793829536
	140551793829536 [label=AccumulateGrad]
	140551793829488 -> 140551793829440
	140550830016288 [label="layers.2.blocks.9.mlp.fc1.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	140550830016288 -> 140551793829488
	140551793829488 [label=AccumulateGrad]
	140551793828096 -> 140551793829440
	140550830016368 [label="layers.2.blocks.9.mlp.fc1.bias
 (2048)" fillcolor=lightblue]
	140550830016368 -> 140551793828096
	140551793828096 [label=AccumulateGrad]
	140551793789920 -> 140551507552528
	140550830016528 [label="layers.2.blocks.9.mlp.fc2.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	140550830016528 -> 140551793789920
	140551793789920 [label=AccumulateGrad]
	140551507552624 -> 140551507552528
	140550830016608 [label="layers.2.blocks.9.mlp.fc2.bias
 (512)" fillcolor=lightblue]
	140550830016608 -> 140551507552624
	140551507552624 [label=AccumulateGrad]
	140551507552384 -> 140551507552288
	140551507552384 [label=ConvolutionBackward0]
	140551507552480 -> 140551507552384
	140551507552480 [label=MulBackward0]
	140551793829920 -> 140551507552480
	140551793829920 [label=SplitWithSizesBackward0]
	140551793829968 -> 140551793829920
	140551793829968 [label=ConvolutionBackward0]
	140551793830064 -> 140551793829968
	140551793830064 [label=PermuteBackward0]
	140551793830256 -> 140551793830064
	140551793830256 [label=NativeLayerNormBackward0]
	140551793830352 -> 140551793830256
	140551793830352 [label=PermuteBackward0]
	140551507552432 -> 140551793830352
	140551793830304 -> 140551793830256
	140550830016448 [label="layers.2.blocks.10.norm1.weight
 (512)" fillcolor=lightblue]
	140550830016448 -> 140551793830304
	140551793830304 [label=AccumulateGrad]
	140551793830160 -> 140551793830256
	140550830016688 [label="layers.2.blocks.10.norm1.bias
 (512)" fillcolor=lightblue]
	140550830016688 -> 140551793830160
	140551793830160 [label=AccumulateGrad]
	140551793830016 -> 140551793829968
	140550830016848 [label="layers.2.blocks.10.modulation.f.weight
 (1028, 512, 1, 1)" fillcolor=lightblue]
	140550830016848 -> 140551793830016
	140551793830016 [label=AccumulateGrad]
	140551793829872 -> 140551793829968
	140550830016928 [label="layers.2.blocks.10.modulation.f.bias
 (1028)" fillcolor=lightblue]
	140550830016928 -> 140551793829872
	140551793829872 [label=AccumulateGrad]
	140551793829776 -> 140551507552480
	140551793829776 [label=ConvolutionBackward0]
	140551793830208 -> 140551793829776
	140551793830208 [label=AddBackward0]
	140551793830448 -> 140551793830208
	140551793830448 [label=AddBackward0]
	140551793830688 -> 140551793830448
	140551793830688 [label=AddBackward0]
	140551793830832 -> 140551793830688
	140551793830832 [label=AddBackward0]
	140551793830976 -> 140551793830832
	140551793830976 [label=MulBackward0]
	140551793831072 -> 140551793830976
	140551793831072 [label=GeluBackward0]
	140551793831216 -> 140551793831072
	140551793831216 [label=ConvolutionBackward0]
	140551793829920 -> 140551793831216
	140551793831312 -> 140551793831216
	140550830017568 [label="layers.2.blocks.10.modulation.focal_layers.0.0.weight
 (512, 1, 3, 3)" fillcolor=lightblue]
	140550830017568 -> 140551793831312
	140551793831312 [label=AccumulateGrad]
	140551793831024 -> 140551793830976
	140551793831024 [label=SliceBackward0]
	140551793831408 -> 140551793831024
	140551793831408 [label=SliceBackward0]
	140551793829920 -> 140551793831408
	140551793830784 -> 140551793830688
	140551793830784 [label=MulBackward0]
	140551793831264 -> 140551793830784
	140551793831264 [label=GeluBackward0]
	140551793831168 -> 140551793831264
	140551793831168 [label=ConvolutionBackward0]
	140551793831072 -> 140551793831168
	140551793831504 -> 140551793831168
	140550830017728 [label="layers.2.blocks.10.modulation.focal_layers.1.0.weight
 (512, 1, 5, 5)" fillcolor=lightblue]
	140550830017728 -> 140551793831504
	140551793831504 [label=AccumulateGrad]
	140551793830880 -> 140551793830784
	140551793830880 [label=SliceBackward0]
	140551793831600 -> 140551793830880
	140551793831600 [label=SliceBackward0]
	140551793829920 -> 140551793831600
	140551793830640 -> 140551793830448
	140551793830640 [label=MulBackward0]
	140551793831456 -> 140551793830640
	140551793831456 [label=GeluBackward0]
	140551793831360 -> 140551793831456
	140551793831360 [label=ConvolutionBackward0]
	140551793831264 -> 140551793831360
	140551793831696 -> 140551793831360
	140550830017888 [label="layers.2.blocks.10.modulation.focal_layers.2.0.weight
 (512, 1, 7, 7)" fillcolor=lightblue]
	140550830017888 -> 140551793831696
	140551793831696 [label=AccumulateGrad]
	140551793830928 -> 140551793830640
	140551793830928 [label=SliceBackward0]
	140551793831792 -> 140551793830928
	140551793831792 [label=SliceBackward0]
	140551793829920 -> 140551793831792
	140551793830496 -> 140551793830208
	140551793830496 [label=MulBackward0]
	140551793831648 -> 140551793830496
	140551793831648 [label=GeluBackward0]
	140551793831552 -> 140551793831648
	140551793831552 [label=MeanBackward1]
	140551793831456 -> 140551793831552
	140551793830736 -> 140551793830496
	140551793830736 [label=SliceBackward0]
	140551793831840 -> 140551793830736
	140551793831840 [label=SliceBackward0]
	140551793829920 -> 140551793831840
	140551793830112 -> 140551793829776
	140550830017088 [label="layers.2.blocks.10.modulation.h.weight
 (512, 512, 1, 1)" fillcolor=lightblue]
	140550830017088 -> 140551793830112
	140551793830112 [label=AccumulateGrad]
	140551793829824 -> 140551793829776
	140550830017168 [label="layers.2.blocks.10.modulation.h.bias
 (512)" fillcolor=lightblue]
	140550830017168 -> 140551793829824
	140551793829824 [label=AccumulateGrad]
	140551793828144 -> 140551507552384
	140550830017328 [label="layers.2.blocks.10.modulation.proj.weight
 (512, 512, 1, 1)" fillcolor=lightblue]
	140550830017328 -> 140551793828144
	140551793828144 [label=AccumulateGrad]
	140551793827952 -> 140551507552384
	140550830017408 [label="layers.2.blocks.10.modulation.proj.bias
 (512)" fillcolor=lightblue]
	140550830017408 -> 140551793827952
	140551793827952 [label=AccumulateGrad]
	140551507552240 -> 140551507552144
	140551507552240 [label=ConvolutionBackward0]
	140551507552336 -> 140551507552240
	140551507552336 [label=GeluBackward0]
	140551793831888 -> 140551507552336
	140551793831888 [label=ConvolutionBackward0]
	140551793831744 -> 140551793831888
	140551793831744 [label=PermuteBackward0]
	140551793852624 -> 140551793831744
	140551793852624 [label=NativeLayerNormBackward0]
	140551793852720 -> 140551793852624
	140551793852720 [label=PermuteBackward0]
	140551507552288 -> 140551793852720
	140551793852672 -> 140551793852624
	140550830017968 [label="layers.2.blocks.10.norm2.weight
 (512)" fillcolor=lightblue]
	140550830017968 -> 140551793852672
	140551793852672 [label=AccumulateGrad]
	140551793852528 -> 140551793852624
	140550830018048 [label="layers.2.blocks.10.norm2.bias
 (512)" fillcolor=lightblue]
	140550830018048 -> 140551793852528
	140551793852528 [label=AccumulateGrad]
	140551793831120 -> 140551793831888
	140550830018288 [label="layers.2.blocks.10.mlp.fc1.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	140550830018288 -> 140551793831120
	140551793831120 [label=AccumulateGrad]
	140551793830544 -> 140551793831888
	140550830018368 [label="layers.2.blocks.10.mlp.fc1.bias
 (2048)" fillcolor=lightblue]
	140550830018368 -> 140551793830544
	140551793830544 [label=AccumulateGrad]
	140551793829584 -> 140551507552240
	140550694174784 [label="layers.2.blocks.10.mlp.fc2.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	140550694174784 -> 140551793829584
	140551793829584 [label=AccumulateGrad]
	140551793829296 -> 140551507552240
	140550694174864 [label="layers.2.blocks.10.mlp.fc2.bias
 (512)" fillcolor=lightblue]
	140550694174864 -> 140551793829296
	140551793829296 [label=AccumulateGrad]
	140551507552096 -> 140551507552000
	140551507552096 [label=ConvolutionBackward0]
	140551793830592 -> 140551507552096
	140551793830592 [label=MulBackward0]
	140551793852912 -> 140551793830592
	140551793852912 [label=SplitWithSizesBackward0]
	140551793852960 -> 140551793852912
	140551793852960 [label=ConvolutionBackward0]
	140551793853056 -> 140551793852960
	140551793853056 [label=PermuteBackward0]
	140551793853248 -> 140551793853056
	140551793853248 [label=NativeLayerNormBackward0]
	140551793853344 -> 140551793853248
	140551793853344 [label=PermuteBackward0]
	140551507552144 -> 140551793853344
	140551793853296 -> 140551793853248
	140550830018448 [label="layers.2.blocks.11.norm1.weight
 (512)" fillcolor=lightblue]
	140550830018448 -> 140551793853296
	140551793853296 [label=AccumulateGrad]
	140551793853152 -> 140551793853248
	140550694174944 [label="layers.2.blocks.11.norm1.bias
 (512)" fillcolor=lightblue]
	140550694174944 -> 140551793853152
	140551793853152 [label=AccumulateGrad]
	140551793853008 -> 140551793852960
	140550694175104 [label="layers.2.blocks.11.modulation.f.weight
 (1028, 512, 1, 1)" fillcolor=lightblue]
	140550694175104 -> 140551793853008
	140551793853008 [label=AccumulateGrad]
	140551793852864 -> 140551793852960
	140550694175184 [label="layers.2.blocks.11.modulation.f.bias
 (1028)" fillcolor=lightblue]
	140550694175184 -> 140551793852864
	140551793852864 [label=AccumulateGrad]
	140551793852768 -> 140551793830592
	140551793852768 [label=ConvolutionBackward0]
	140551793853200 -> 140551793852768
	140551793853200 [label=AddBackward0]
	140551793853440 -> 140551793853200
	140551793853440 [label=AddBackward0]
	140551793853680 -> 140551793853440
	140551793853680 [label=AddBackward0]
	140551793853824 -> 140551793853680
	140551793853824 [label=AddBackward0]
	140551793853968 -> 140551793853824
	140551793853968 [label=MulBackward0]
	140551793854064 -> 140551793853968
	140551793854064 [label=GeluBackward0]
	140551793854208 -> 140551793854064
	140551793854208 [label=ConvolutionBackward0]
	140551793852912 -> 140551793854208
	140551793854304 -> 140551793854208
	140550694175824 [label="layers.2.blocks.11.modulation.focal_layers.0.0.weight
 (512, 1, 3, 3)" fillcolor=lightblue]
	140550694175824 -> 140551793854304
	140551793854304 [label=AccumulateGrad]
	140551793854016 -> 140551793853968
	140551793854016 [label=SliceBackward0]
	140551793854400 -> 140551793854016
	140551793854400 [label=SliceBackward0]
	140551793852912 -> 140551793854400
	140551793853776 -> 140551793853680
	140551793853776 [label=MulBackward0]
	140551793854256 -> 140551793853776
	140551793854256 [label=GeluBackward0]
	140551793854160 -> 140551793854256
	140551793854160 [label=ConvolutionBackward0]
	140551793854064 -> 140551793854160
	140551793854496 -> 140551793854160
	140550694175984 [label="layers.2.blocks.11.modulation.focal_layers.1.0.weight
 (512, 1, 5, 5)" fillcolor=lightblue]
	140550694175984 -> 140551793854496
	140551793854496 [label=AccumulateGrad]
	140551793853872 -> 140551793853776
	140551793853872 [label=SliceBackward0]
	140551793854592 -> 140551793853872
	140551793854592 [label=SliceBackward0]
	140551793852912 -> 140551793854592
	140551793853632 -> 140551793853440
	140551793853632 [label=MulBackward0]
	140551793854448 -> 140551793853632
	140551793854448 [label=GeluBackward0]
	140551793854352 -> 140551793854448
	140551793854352 [label=ConvolutionBackward0]
	140551793854256 -> 140551793854352
	140551793854688 -> 140551793854352
	140550694176144 [label="layers.2.blocks.11.modulation.focal_layers.2.0.weight
 (512, 1, 7, 7)" fillcolor=lightblue]
	140550694176144 -> 140551793854688
	140551793854688 [label=AccumulateGrad]
	140551793853920 -> 140551793853632
	140551793853920 [label=SliceBackward0]
	140551793854784 -> 140551793853920
	140551793854784 [label=SliceBackward0]
	140551793852912 -> 140551793854784
	140551793853488 -> 140551793853200
	140551793853488 [label=MulBackward0]
	140551793854640 -> 140551793853488
	140551793854640 [label=GeluBackward0]
	140551793854544 -> 140551793854640
	140551793854544 [label=MeanBackward1]
	140551793854448 -> 140551793854544
	140551793853728 -> 140551793853488
	140551793853728 [label=SliceBackward0]
	140551793854832 -> 140551793853728
	140551793854832 [label=SliceBackward0]
	140551793852912 -> 140551793854832
	140551793853104 -> 140551793852768
	140550694175344 [label="layers.2.blocks.11.modulation.h.weight
 (512, 512, 1, 1)" fillcolor=lightblue]
	140550694175344 -> 140551793853104
	140551793853104 [label=AccumulateGrad]
	140551793852816 -> 140551793852768
	140550694175424 [label="layers.2.blocks.11.modulation.h.bias
 (512)" fillcolor=lightblue]
	140550694175424 -> 140551793852816
	140551793852816 [label=AccumulateGrad]
	140551793830400 -> 140551507552096
	140550694175584 [label="layers.2.blocks.11.modulation.proj.weight
 (512, 512, 1, 1)" fillcolor=lightblue]
	140550694175584 -> 140551793830400
	140551793830400 [label=AccumulateGrad]
	140551507552192 -> 140551507552096
	140550694175664 [label="layers.2.blocks.11.modulation.proj.bias
 (512)" fillcolor=lightblue]
	140550694175664 -> 140551507552192
	140551507552192 [label=AccumulateGrad]
	140551507551952 -> 140551507551856
	140551507551952 [label=ConvolutionBackward0]
	140551507552048 -> 140551507551952
	140551507552048 [label=GeluBackward0]
	140551793854880 -> 140551507552048
	140551793854880 [label=ConvolutionBackward0]
	140551793854112 -> 140551793854880
	140551793854112 [label=PermuteBackward0]
	140551793855072 -> 140551793854112
	140551793855072 [label=NativeLayerNormBackward0]
	140551793855168 -> 140551793855072
	140551793855168 [label=PermuteBackward0]
	140551507552000 -> 140551793855168
	140551793855120 -> 140551793855072
	140550694176224 [label="layers.2.blocks.11.norm2.weight
 (512)" fillcolor=lightblue]
	140550694176224 -> 140551793855120
	140551793855120 [label=AccumulateGrad]
	140551793854976 -> 140551793855072
	140550694176304 [label="layers.2.blocks.11.norm2.bias
 (512)" fillcolor=lightblue]
	140550694176304 -> 140551793854976
	140551793854976 [label=AccumulateGrad]
	140551793854928 -> 140551793854880
	140550694176544 [label="layers.2.blocks.11.mlp.fc1.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	140550694176544 -> 140551793854928
	140551793854928 [label=AccumulateGrad]
	140551793853536 -> 140551793854880
	140550694176624 [label="layers.2.blocks.11.mlp.fc1.bias
 (2048)" fillcolor=lightblue]
	140550694176624 -> 140551793853536
	140551793853536 [label=AccumulateGrad]
	140551793852576 -> 140551507551952
	140550694176784 [label="layers.2.blocks.11.mlp.fc2.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	140550694176784 -> 140551793852576
	140551793852576 [label=AccumulateGrad]
	140551793852480 -> 140551507551952
	140550694176864 [label="layers.2.blocks.11.mlp.fc2.bias
 (512)" fillcolor=lightblue]
	140550694176864 -> 140551793852480
	140551793852480 [label=AccumulateGrad]
	140551507551808 -> 140551507551712
	140551507551808 [label=ConvolutionBackward0]
	140551507551904 -> 140551507551808
	140551507551904 [label=MulBackward0]
	140551793855360 -> 140551507551904
	140551793855360 [label=SplitWithSizesBackward0]
	140551793855408 -> 140551793855360
	140551793855408 [label=ConvolutionBackward0]
	140551793855504 -> 140551793855408
	140551793855504 [label=PermuteBackward0]
	140551793855696 -> 140551793855504
	140551793855696 [label=NativeLayerNormBackward0]
	140551793855792 -> 140551793855696
	140551793855792 [label=PermuteBackward0]
	140551507551856 -> 140551793855792
	140551793855744 -> 140551793855696
	140550694176944 [label="layers.2.blocks.12.norm1.weight
 (512)" fillcolor=lightblue]
	140550694176944 -> 140551793855744
	140551793855744 [label=AccumulateGrad]
	140551793855600 -> 140551793855696
	140550694177024 [label="layers.2.blocks.12.norm1.bias
 (512)" fillcolor=lightblue]
	140550694177024 -> 140551793855600
	140551793855600 [label=AccumulateGrad]
	140551793855456 -> 140551793855408
	140550694177184 [label="layers.2.blocks.12.modulation.f.weight
 (1028, 512, 1, 1)" fillcolor=lightblue]
	140550694177184 -> 140551793855456
	140551793855456 [label=AccumulateGrad]
	140551793855312 -> 140551793855408
	140550694177264 [label="layers.2.blocks.12.modulation.f.bias
 (1028)" fillcolor=lightblue]
	140550694177264 -> 140551793855312
	140551793855312 [label=AccumulateGrad]
	140551793855216 -> 140551507551904
	140551793855216 [label=ConvolutionBackward0]
	140551793855648 -> 140551793855216
	140551793855648 [label=AddBackward0]
	140551793855888 -> 140551793855648
	140551793855888 [label=AddBackward0]
	140551793856128 -> 140551793855888
	140551793856128 [label=AddBackward0]
	140551793856272 -> 140551793856128
	140551793856272 [label=AddBackward0]
	140551793856416 -> 140551793856272
	140551793856416 [label=MulBackward0]
	140551793856464 -> 140551793856416
	140551793856464 [label=GeluBackward0]
	140551793885392 -> 140551793856464
	140551793885392 [label=ConvolutionBackward0]
	140551793855360 -> 140551793885392
	140551793885488 -> 140551793885392
	140550694177904 [label="layers.2.blocks.12.modulation.focal_layers.0.0.weight
 (512, 1, 3, 3)" fillcolor=lightblue]
	140550694177904 -> 140551793885488
	140551793885488 [label=AccumulateGrad]
	140551793856320 -> 140551793856416
	140551793856320 [label=SliceBackward0]
	140551793885584 -> 140551793856320
	140551793885584 [label=SliceBackward0]
	140551793855360 -> 140551793885584
	140551793856224 -> 140551793856128
	140551793856224 [label=MulBackward0]
	140551793856368 -> 140551793856224
	140551793856368 [label=GeluBackward0]
	140551793885344 -> 140551793856368
	140551793885344 [label=ConvolutionBackward0]
	140551793856464 -> 140551793885344
	140551793885680 -> 140551793885344
	140550694178064 [label="layers.2.blocks.12.modulation.focal_layers.1.0.weight
 (512, 1, 5, 5)" fillcolor=lightblue]
	140550694178064 -> 140551793885680
	140551793885680 [label=AccumulateGrad]
	140551793885440 -> 140551793856224
	140551793885440 [label=SliceBackward0]
	140551793885776 -> 140551793885440
	140551793885776 [label=SliceBackward0]
	140551793855360 -> 140551793885776
	140551793856080 -> 140551793855888
	140551793856080 [label=MulBackward0]
	140551793856176 -> 140551793856080
	140551793856176 [label=GeluBackward0]
	140551793885536 -> 140551793856176
	140551793885536 [label=ConvolutionBackward0]
	140551793856368 -> 140551793885536
	140551793885872 -> 140551793885536
	140550694178224 [label="layers.2.blocks.12.modulation.focal_layers.2.0.weight
 (512, 1, 7, 7)" fillcolor=lightblue]
	140550694178224 -> 140551793885872
	140551793885872 [label=AccumulateGrad]
	140551793885632 -> 140551793856080
	140551793885632 [label=SliceBackward0]
	140551793885968 -> 140551793885632
	140551793885968 [label=SliceBackward0]
	140551793855360 -> 140551793885968
	140551793855936 -> 140551793855648
	140551793855936 [label=MulBackward0]
	140551793856032 -> 140551793855936
	140551793856032 [label=GeluBackward0]
	140551793885728 -> 140551793856032
	140551793885728 [label=MeanBackward1]
	140551793856176 -> 140551793885728
	140551793885824 -> 140551793855936
	140551793885824 [label=SliceBackward0]
	140551793886016 -> 140551793885824
	140551793886016 [label=SliceBackward0]
	140551793855360 -> 140551793886016
	140551793855552 -> 140551793855216
	140550694177424 [label="layers.2.blocks.12.modulation.h.weight
 (512, 512, 1, 1)" fillcolor=lightblue]
	140550694177424 -> 140551793855552
	140551793855552 [label=AccumulateGrad]
	140551793855264 -> 140551793855216
	140550694177504 [label="layers.2.blocks.12.modulation.h.bias
 (512)" fillcolor=lightblue]
	140550694177504 -> 140551793855264
	140551793855264 [label=AccumulateGrad]
	140551793853584 -> 140551507551808
	140550694177664 [label="layers.2.blocks.12.modulation.proj.weight
 (512, 512, 1, 1)" fillcolor=lightblue]
	140550694177664 -> 140551793853584
	140551793853584 [label=AccumulateGrad]
	140551793853392 -> 140551507551808
	140550694177744 [label="layers.2.blocks.12.modulation.proj.bias
 (512)" fillcolor=lightblue]
	140550694177744 -> 140551793853392
	140551793853392 [label=AccumulateGrad]
	140551507551664 -> 140551507551568
	140551507551664 [label=ConvolutionBackward0]
	140551507551760 -> 140551507551664
	140551507551760 [label=GeluBackward0]
	140551793855984 -> 140551507551760
	140551793855984 [label=ConvolutionBackward0]
	140551793885296 -> 140551793855984
	140551793885296 [label=PermuteBackward0]
	140551793886256 -> 140551793885296
	140551793886256 [label=NativeLayerNormBackward0]
	140551793886352 -> 140551793886256
	140551793886352 [label=PermuteBackward0]
	140551507551712 -> 140551793886352
	140551793886304 -> 140551793886256
	140550694178304 [label="layers.2.blocks.12.norm2.weight
 (512)" fillcolor=lightblue]
	140550694178304 -> 140551793886304
	140551793886304 [label=AccumulateGrad]
	140551793886160 -> 140551793886256
	140550694178384 [label="layers.2.blocks.12.norm2.bias
 (512)" fillcolor=lightblue]
	140550694178384 -> 140551793886160
	140551793886160 [label=AccumulateGrad]
	140551793886112 -> 140551793855984
	140550694178624 [label="layers.2.blocks.12.mlp.fc1.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	140550694178624 -> 140551793886112
	140551793886112 [label=AccumulateGrad]
	140551793885248 -> 140551793855984
	140550694178704 [label="layers.2.blocks.12.mlp.fc1.bias
 (2048)" fillcolor=lightblue]
	140550694178704 -> 140551793885248
	140551793885248 [label=AccumulateGrad]
	140551793855024 -> 140551507551664
	140550694412432 [label="layers.2.blocks.12.mlp.fc2.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	140550694412432 -> 140551793855024
	140551793855024 [label=AccumulateGrad]
	140551793854736 -> 140551507551664
	140550694412512 [label="layers.2.blocks.12.mlp.fc2.bias
 (512)" fillcolor=lightblue]
	140550694412512 -> 140551793854736
	140551793854736 [label=AccumulateGrad]
	140551507551520 -> 140551507551424
	140551507551520 [label=ConvolutionBackward0]
	140551793855840 -> 140551507551520
	140551793855840 [label=MulBackward0]
	140551793886544 -> 140551793855840
	140551793886544 [label=SplitWithSizesBackward0]
	140551793886592 -> 140551793886544
	140551793886592 [label=ConvolutionBackward0]
	140551793886688 -> 140551793886592
	140551793886688 [label=PermuteBackward0]
	140551793886880 -> 140551793886688
	140551793886880 [label=NativeLayerNormBackward0]
	140551793886976 -> 140551793886880
	140551793886976 [label=PermuteBackward0]
	140551507551568 -> 140551793886976
	140551793886928 -> 140551793886880
	140550694412352 [label="layers.2.blocks.13.norm1.weight
 (512)" fillcolor=lightblue]
	140550694412352 -> 140551793886928
	140551793886928 [label=AccumulateGrad]
	140551793886784 -> 140551793886880
	140550694412592 [label="layers.2.blocks.13.norm1.bias
 (512)" fillcolor=lightblue]
	140550694412592 -> 140551793886784
	140551793886784 [label=AccumulateGrad]
	140551793886640 -> 140551793886592
	140550694412752 [label="layers.2.blocks.13.modulation.f.weight
 (1028, 512, 1, 1)" fillcolor=lightblue]
	140550694412752 -> 140551793886640
	140551793886640 [label=AccumulateGrad]
	140551793886496 -> 140551793886592
	140550694412832 [label="layers.2.blocks.13.modulation.f.bias
 (1028)" fillcolor=lightblue]
	140550694412832 -> 140551793886496
	140551793886496 [label=AccumulateGrad]
	140551793886400 -> 140551793855840
	140551793886400 [label=ConvolutionBackward0]
	140551793886832 -> 140551793886400
	140551793886832 [label=AddBackward0]
	140551793887072 -> 140551793886832
	140551793887072 [label=AddBackward0]
	140551793887312 -> 140551793887072
	140551793887312 [label=AddBackward0]
	140551793887456 -> 140551793887312
	140551793887456 [label=AddBackward0]
	140551793887600 -> 140551793887456
	140551793887600 [label=MulBackward0]
	140551793887696 -> 140551793887600
	140551793887696 [label=GeluBackward0]
	140551793887840 -> 140551793887696
	140551793887840 [label=ConvolutionBackward0]
	140551793886544 -> 140551793887840
	140551793887936 -> 140551793887840
	140550694413472 [label="layers.2.blocks.13.modulation.focal_layers.0.0.weight
 (512, 1, 3, 3)" fillcolor=lightblue]
	140550694413472 -> 140551793887936
	140551793887936 [label=AccumulateGrad]
	140551793887648 -> 140551793887600
	140551793887648 [label=SliceBackward0]
	140551793888032 -> 140551793887648
	140551793888032 [label=SliceBackward0]
	140551793886544 -> 140551793888032
	140551793887408 -> 140551793887312
	140551793887408 [label=MulBackward0]
	140551793887888 -> 140551793887408
	140551793887888 [label=GeluBackward0]
	140551793887792 -> 140551793887888
	140551793887792 [label=ConvolutionBackward0]
	140551793887696 -> 140551793887792
	140551793888128 -> 140551793887792
	140550694413632 [label="layers.2.blocks.13.modulation.focal_layers.1.0.weight
 (512, 1, 5, 5)" fillcolor=lightblue]
	140550694413632 -> 140551793888128
	140551793888128 [label=AccumulateGrad]
	140551793887504 -> 140551793887408
	140551793887504 [label=SliceBackward0]
	140551793888224 -> 140551793887504
	140551793888224 [label=SliceBackward0]
	140551793886544 -> 140551793888224
	140551793887264 -> 140551793887072
	140551793887264 [label=MulBackward0]
	140551793888080 -> 140551793887264
	140551793888080 [label=GeluBackward0]
	140551793887984 -> 140551793888080
	140551793887984 [label=ConvolutionBackward0]
	140551793887888 -> 140551793887984
	140551793888320 -> 140551793887984
	140550694413792 [label="layers.2.blocks.13.modulation.focal_layers.2.0.weight
 (512, 1, 7, 7)" fillcolor=lightblue]
	140550694413792 -> 140551793888320
	140551793888320 [label=AccumulateGrad]
	140551793887552 -> 140551793887264
	140551793887552 [label=SliceBackward0]
	140551793888416 -> 140551793887552
	140551793888416 [label=SliceBackward0]
	140551793886544 -> 140551793888416
	140551793887120 -> 140551793886832
	140551793887120 [label=MulBackward0]
	140551793888272 -> 140551793887120
	140551793888272 [label=GeluBackward0]
	140551793888176 -> 140551793888272
	140551793888176 [label=MeanBackward1]
	140551793888080 -> 140551793888176
	140551793887360 -> 140551793887120
	140551793887360 [label=SliceBackward0]
	140551793888464 -> 140551793887360
	140551793888464 [label=SliceBackward0]
	140551793886544 -> 140551793888464
	140551793886736 -> 140551793886400
	140550694412992 [label="layers.2.blocks.13.modulation.h.weight
 (512, 512, 1, 1)" fillcolor=lightblue]
	140550694412992 -> 140551793886736
	140551793886736 [label=AccumulateGrad]
	140551793886448 -> 140551793886400
	140550694413072 [label="layers.2.blocks.13.modulation.h.bias
 (512)" fillcolor=lightblue]
	140550694413072 -> 140551793886448
	140551793886448 [label=AccumulateGrad]
	140551507551616 -> 140551507551520
	140550694413232 [label="layers.2.blocks.13.modulation.proj.weight
 (512, 512, 1, 1)" fillcolor=lightblue]
	140550694413232 -> 140551507551616
	140551507551616 [label=AccumulateGrad]
	140551793886064 -> 140551507551520
	140550694413312 [label="layers.2.blocks.13.modulation.proj.bias
 (512)" fillcolor=lightblue]
	140550694413312 -> 140551793886064
	140551793886064 [label=AccumulateGrad]
	140551507551376 -> 140551507551280
	140551507551376 [label=ConvolutionBackward0]
	140551507551472 -> 140551507551376
	140551507551472 [label=GeluBackward0]
	140551793888512 -> 140551507551472
	140551793888512 [label=ConvolutionBackward0]
	140551793887744 -> 140551793888512
	140551793887744 [label=PermuteBackward0]
	140551793888704 -> 140551793887744
	140551793888704 [label=NativeLayerNormBackward0]
	140551793888800 -> 140551793888704
	140551793888800 [label=PermuteBackward0]
	140551507551424 -> 140551793888800
	140551793888752 -> 140551793888704
	140550694413872 [label="layers.2.blocks.13.norm2.weight
 (512)" fillcolor=lightblue]
	140550694413872 -> 140551793888752
	140551793888752 [label=AccumulateGrad]
	140551793888608 -> 140551793888704
	140550694413952 [label="layers.2.blocks.13.norm2.bias
 (512)" fillcolor=lightblue]
	140550694413952 -> 140551793888608
	140551793888608 [label=AccumulateGrad]
	140551793888560 -> 140551793888512
	140550694414192 [label="layers.2.blocks.13.mlp.fc1.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	140550694414192 -> 140551793888560
	140551793888560 [label=AccumulateGrad]
	140551793887168 -> 140551793888512
	140550694414272 [label="layers.2.blocks.13.mlp.fc1.bias
 (2048)" fillcolor=lightblue]
	140550694414272 -> 140551793887168
	140551793887168 [label=AccumulateGrad]
	140551793886208 -> 140551507551376
	140550694414432 [label="layers.2.blocks.13.mlp.fc2.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	140550694414432 -> 140551793886208
	140551793886208 [label=AccumulateGrad]
	140551793885920 -> 140551507551376
	140550694414512 [label="layers.2.blocks.13.mlp.fc2.bias
 (512)" fillcolor=lightblue]
	140550694414512 -> 140551793885920
	140551793885920 [label=AccumulateGrad]
	140551507551232 -> 140551507551136
	140551507551232 [label=ConvolutionBackward0]
	140551507551328 -> 140551507551232
	140551507551328 [label=MulBackward0]
	140551793888992 -> 140551507551328
	140551793888992 [label=SplitWithSizesBackward0]
	140551793889040 -> 140551793888992
	140551793889040 [label=ConvolutionBackward0]
	140551793889136 -> 140551793889040
	140551793889136 [label=PermuteBackward0]
	140551793889232 -> 140551793889136
	140551793889232 [label=NativeLayerNormBackward0]
	140551793918160 -> 140551793889232
	140551793918160 [label=PermuteBackward0]
	140551507551280 -> 140551793918160
	140551793918112 -> 140551793889232
	140550694414352 [label="layers.2.blocks.14.norm1.weight
 (512)" fillcolor=lightblue]
	140550694414352 -> 140551793918112
	140551793918112 [label=AccumulateGrad]
	140551793918016 -> 140551793889232
	140550694414592 [label="layers.2.blocks.14.norm1.bias
 (512)" fillcolor=lightblue]
	140550694414592 -> 140551793918016
	140551793918016 [label=AccumulateGrad]
	140551793889088 -> 140551793889040
	140550694414752 [label="layers.2.blocks.14.modulation.f.weight
 (1028, 512, 1, 1)" fillcolor=lightblue]
	140550694414752 -> 140551793889088
	140551793889088 [label=AccumulateGrad]
	140551793888944 -> 140551793889040
	140550694414832 [label="layers.2.blocks.14.modulation.f.bias
 (1028)" fillcolor=lightblue]
	140550694414832 -> 140551793888944
	140551793888944 [label=AccumulateGrad]
	140551793888848 -> 140551507551328
	140551793888848 [label=ConvolutionBackward0]
	140551793889184 -> 140551793888848
	140551793889184 [label=AddBackward0]
	140551793918256 -> 140551793889184
	140551793918256 [label=AddBackward0]
	140551793918496 -> 140551793918256
	140551793918496 [label=AddBackward0]
	140551793918640 -> 140551793918496
	140551793918640 [label=AddBackward0]
	140551793918784 -> 140551793918640
	140551793918784 [label=MulBackward0]
	140551793918880 -> 140551793918784
	140551793918880 [label=GeluBackward0]
	140551793919024 -> 140551793918880
	140551793919024 [label=ConvolutionBackward0]
	140551793888992 -> 140551793919024
	140551793919120 -> 140551793919024
	140550694415472 [label="layers.2.blocks.14.modulation.focal_layers.0.0.weight
 (512, 1, 3, 3)" fillcolor=lightblue]
	140550694415472 -> 140551793919120
	140551793919120 [label=AccumulateGrad]
	140551793918832 -> 140551793918784
	140551793918832 [label=SliceBackward0]
	140551793919216 -> 140551793918832
	140551793919216 [label=SliceBackward0]
	140551793888992 -> 140551793919216
	140551793918592 -> 140551793918496
	140551793918592 [label=MulBackward0]
	140551793919072 -> 140551793918592
	140551793919072 [label=GeluBackward0]
	140551793918976 -> 140551793919072
	140551793918976 [label=ConvolutionBackward0]
	140551793918880 -> 140551793918976
	140551793919312 -> 140551793918976
	140550694415632 [label="layers.2.blocks.14.modulation.focal_layers.1.0.weight
 (512, 1, 5, 5)" fillcolor=lightblue]
	140550694415632 -> 140551793919312
	140551793919312 [label=AccumulateGrad]
	140551793918688 -> 140551793918592
	140551793918688 [label=SliceBackward0]
	140551793919408 -> 140551793918688
	140551793919408 [label=SliceBackward0]
	140551793888992 -> 140551793919408
	140551793918448 -> 140551793918256
	140551793918448 [label=MulBackward0]
	140551793919264 -> 140551793918448
	140551793919264 [label=GeluBackward0]
	140551793919168 -> 140551793919264
	140551793919168 [label=ConvolutionBackward0]
	140551793919072 -> 140551793919168
	140551793919504 -> 140551793919168
	140550694415792 [label="layers.2.blocks.14.modulation.focal_layers.2.0.weight
 (512, 1, 7, 7)" fillcolor=lightblue]
	140550694415792 -> 140551793919504
	140551793919504 [label=AccumulateGrad]
	140551793918736 -> 140551793918448
	140551793918736 [label=SliceBackward0]
	140551793919600 -> 140551793918736
	140551793919600 [label=SliceBackward0]
	140551793888992 -> 140551793919600
	140551793918304 -> 140551793889184
	140551793918304 [label=MulBackward0]
	140551793919456 -> 140551793918304
	140551793919456 [label=GeluBackward0]
	140551793919360 -> 140551793919456
	140551793919360 [label=MeanBackward1]
	140551793919264 -> 140551793919360
	140551793918544 -> 140551793918304
	140551793918544 [label=SliceBackward0]
	140551793919648 -> 140551793918544
	140551793919648 [label=SliceBackward0]
	140551793888992 -> 140551793919648
	140551793888896 -> 140551793888848
	140550694414992 [label="layers.2.blocks.14.modulation.h.weight
 (512, 512, 1, 1)" fillcolor=lightblue]
	140550694414992 -> 140551793888896
	140551793888896 [label=AccumulateGrad]
	140551793918064 -> 140551793888848
	140550694415072 [label="layers.2.blocks.14.modulation.h.bias
 (512)" fillcolor=lightblue]
	140550694415072 -> 140551793918064
	140551793918064 [label=AccumulateGrad]
	140551793887216 -> 140551507551232
	140550694415232 [label="layers.2.blocks.14.modulation.proj.weight
 (512, 512, 1, 1)" fillcolor=lightblue]
	140550694415232 -> 140551793887216
	140551793887216 [label=AccumulateGrad]
	140551793887024 -> 140551507551232
	140550694415312 [label="layers.2.blocks.14.modulation.proj.bias
 (512)" fillcolor=lightblue]
	140550694415312 -> 140551793887024
	140551793887024 [label=AccumulateGrad]
	140551507551088 -> 140551507550992
	140551507551088 [label=ConvolutionBackward0]
	140551793888656 -> 140551507551088
	140551793888656 [label=GeluBackward0]
	140551793919696 -> 140551793888656
	140551793919696 [label=ConvolutionBackward0]
	140551793918928 -> 140551793919696
	140551793918928 [label=PermuteBackward0]
	140551793919888 -> 140551793918928
	140551793919888 [label=NativeLayerNormBackward0]
	140551793919984 -> 140551793919888
	140551793919984 [label=PermuteBackward0]
	140551507551136 -> 140551793919984
	140551793919936 -> 140551793919888
	140550694415872 [label="layers.2.blocks.14.norm2.weight
 (512)" fillcolor=lightblue]
	140550694415872 -> 140551793919936
	140551793919936 [label=AccumulateGrad]
	140551793919792 -> 140551793919888
	140550694415952 [label="layers.2.blocks.14.norm2.bias
 (512)" fillcolor=lightblue]
	140550694415952 -> 140551793919792
	140551793919792 [label=AccumulateGrad]
	140551793919744 -> 140551793919696
	140550694416192 [label="layers.2.blocks.14.mlp.fc1.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	140550694416192 -> 140551793919744
	140551793919744 [label=AccumulateGrad]
	140551793918352 -> 140551793919696
	140550694416272 [label="layers.2.blocks.14.mlp.fc1.bias
 (2048)" fillcolor=lightblue]
	140550694416272 -> 140551793918352
	140551793918352 [label=AccumulateGrad]
	140551793888368 -> 140551507551088
	140550947655824 [label="layers.2.blocks.14.mlp.fc2.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	140550947655824 -> 140551793888368
	140551793888368 [label=AccumulateGrad]
	140551507551184 -> 140551507551088
	140550947655904 [label="layers.2.blocks.14.mlp.fc2.bias
 (512)" fillcolor=lightblue]
	140550947655904 -> 140551507551184
	140551507551184 [label=AccumulateGrad]
	140551507550944 -> 140551507550848
	140551507550944 [label=ConvolutionBackward0]
	140551507551040 -> 140551507550944
	140551507551040 [label=MulBackward0]
	140551793920176 -> 140551507551040
	140551793920176 [label=SplitWithSizesBackward0]
	140551793920224 -> 140551793920176
	140551793920224 [label=ConvolutionBackward0]
	140551793920320 -> 140551793920224
	140551793920320 [label=PermuteBackward0]
	140551793920512 -> 140551793920320
	140551793920512 [label=NativeLayerNormBackward0]
	140551793920608 -> 140551793920512
	140551793920608 [label=PermuteBackward0]
	140551507550992 -> 140551793920608
	140551793920560 -> 140551793920512
	140550947655744 [label="layers.2.blocks.15.norm1.weight
 (512)" fillcolor=lightblue]
	140550947655744 -> 140551793920560
	140551793920560 [label=AccumulateGrad]
	140551793920416 -> 140551793920512
	140550947655984 [label="layers.2.blocks.15.norm1.bias
 (512)" fillcolor=lightblue]
	140550947655984 -> 140551793920416
	140551793920416 [label=AccumulateGrad]
	140551793920272 -> 140551793920224
	140550947656144 [label="layers.2.blocks.15.modulation.f.weight
 (1028, 512, 1, 1)" fillcolor=lightblue]
	140550947656144 -> 140551793920272
	140551793920272 [label=AccumulateGrad]
	140551793920128 -> 140551793920224
	140550947656224 [label="layers.2.blocks.15.modulation.f.bias
 (1028)" fillcolor=lightblue]
	140550947656224 -> 140551793920128
	140551793920128 [label=AccumulateGrad]
	140551793920032 -> 140551507551040
	140551793920032 [label=ConvolutionBackward0]
	140551793920464 -> 140551793920032
	140551793920464 [label=AddBackward0]
	140551793920704 -> 140551793920464
	140551793920704 [label=AddBackward0]
	140551793920944 -> 140551793920704
	140551793920944 [label=AddBackward0]
	140551793921088 -> 140551793920944
	140551793921088 [label=AddBackward0]
	140551793921232 -> 140551793921088
	140551793921232 [label=MulBackward0]
	140551793921328 -> 140551793921232
	140551793921328 [label=GeluBackward0]
	140551793921472 -> 140551793921328
	140551793921472 [label=ConvolutionBackward0]
	140551793920176 -> 140551793921472
	140551793921568 -> 140551793921472
	140550947656864 [label="layers.2.blocks.15.modulation.focal_layers.0.0.weight
 (512, 1, 3, 3)" fillcolor=lightblue]
	140550947656864 -> 140551793921568
	140551793921568 [label=AccumulateGrad]
	140551793921280 -> 140551793921232
	140551793921280 [label=SliceBackward0]
	140551793921664 -> 140551793921280
	140551793921664 [label=SliceBackward0]
	140551793920176 -> 140551793921664
	140551793921040 -> 140551793920944
	140551793921040 [label=MulBackward0]
	140551793921520 -> 140551793921040
	140551793921520 [label=GeluBackward0]
	140551793921424 -> 140551793921520
	140551793921424 [label=ConvolutionBackward0]
	140551793921328 -> 140551793921424
	140551793921760 -> 140551793921424
	140550947657024 [label="layers.2.blocks.15.modulation.focal_layers.1.0.weight
 (512, 1, 5, 5)" fillcolor=lightblue]
	140550947657024 -> 140551793921760
	140551793921760 [label=AccumulateGrad]
	140551793921136 -> 140551793921040
	140551793921136 [label=SliceBackward0]
	140551793921856 -> 140551793921136
	140551793921856 [label=SliceBackward0]
	140551793920176 -> 140551793921856
	140551793920896 -> 140551793920704
	140551793920896 [label=MulBackward0]
	140551793921712 -> 140551793920896
	140551793921712 [label=GeluBackward0]
	140551793921616 -> 140551793921712
	140551793921616 [label=ConvolutionBackward0]
	140551793921520 -> 140551793921616
	140551793921952 -> 140551793921616
	140550947657184 [label="layers.2.blocks.15.modulation.focal_layers.2.0.weight
 (512, 1, 7, 7)" fillcolor=lightblue]
	140550947657184 -> 140551793921952
	140551793921952 [label=AccumulateGrad]
	140551793921184 -> 140551793920896
	140551793921184 [label=SliceBackward0]
	140551793922000 -> 140551793921184
	140551793922000 [label=SliceBackward0]
	140551793920176 -> 140551793922000
	140551793920752 -> 140551793920464
	140551793920752 [label=MulBackward0]
	140551793921904 -> 140551793920752
	140551793921904 [label=GeluBackward0]
	140551793921808 -> 140551793921904
	140551793921808 [label=MeanBackward1]
	140551793921712 -> 140551793921808
	140551793920992 -> 140551793920752
	140551793920992 [label=SliceBackward0]
	140551793921376 -> 140551793920992
	140551793921376 [label=SliceBackward0]
	140551793920176 -> 140551793921376
	140551793920368 -> 140551793920032
	140550947656384 [label="layers.2.blocks.15.modulation.h.weight
 (512, 512, 1, 1)" fillcolor=lightblue]
	140550947656384 -> 140551793920368
	140551793920368 [label=AccumulateGrad]
	140551793920080 -> 140551793920032
	140550947656464 [label="layers.2.blocks.15.modulation.h.bias
 (512)" fillcolor=lightblue]
	140550947656464 -> 140551793920080
	140551793920080 [label=AccumulateGrad]
	140551793918400 -> 140551507550944
	140550947656624 [label="layers.2.blocks.15.modulation.proj.weight
 (512, 512, 1, 1)" fillcolor=lightblue]
	140550947656624 -> 140551793918400
	140551793918400 [label=AccumulateGrad]
	140551793918208 -> 140551507550944
	140550947656704 [label="layers.2.blocks.15.modulation.proj.bias
 (512)" fillcolor=lightblue]
	140550947656704 -> 140551793918208
	140551793918208 [label=AccumulateGrad]
	140551507550800 -> 140551507550704
	140551507550800 [label=ConvolutionBackward0]
	140551507550896 -> 140551507550800
	140551507550896 [label=GeluBackward0]
	140551793920848 -> 140551507550896
	140551793920848 [label=ConvolutionBackward0]
	140550199455808 -> 140551793920848
	140550199455808 [label=PermuteBackward0]
	140550199456096 -> 140550199455808
	140550199456096 [label=NativeLayerNormBackward0]
	140550199456192 -> 140550199456096
	140550199456192 [label=PermuteBackward0]
	140551507550848 -> 140550199456192
	140550199456144 -> 140550199456096
	140550947657264 [label="layers.2.blocks.15.norm2.weight
 (512)" fillcolor=lightblue]
	140550947657264 -> 140550199456144
	140550199456144 [label=AccumulateGrad]
	140550199456000 -> 140550199456096
	140550947657344 [label="layers.2.blocks.15.norm2.bias
 (512)" fillcolor=lightblue]
	140550947657344 -> 140550199456000
	140550199456000 [label=AccumulateGrad]
	140550199455952 -> 140551793920848
	140550947657584 [label="layers.2.blocks.15.mlp.fc1.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	140550947657584 -> 140550199455952
	140550199455952 [label=AccumulateGrad]
	140550199455856 -> 140551793920848
	140550947657664 [label="layers.2.blocks.15.mlp.fc1.bias
 (2048)" fillcolor=lightblue]
	140550947657664 -> 140550199455856
	140550199455856 [label=AccumulateGrad]
	140551793919840 -> 140551507550800
	140550947657824 [label="layers.2.blocks.15.mlp.fc2.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	140550947657824 -> 140551793919840
	140551793919840 [label=AccumulateGrad]
	140551793919552 -> 140551507550800
	140550947657904 [label="layers.2.blocks.15.mlp.fc2.bias
 (512)" fillcolor=lightblue]
	140550947657904 -> 140551793919552
	140551793919552 [label=AccumulateGrad]
	140551507550656 -> 140551507550560
	140551507550656 [label=ConvolutionBackward0]
	140551793920800 -> 140551507550656
	140551793920800 [label=MulBackward0]
	140550199456384 -> 140551793920800
	140550199456384 [label=SplitWithSizesBackward0]
	140550199456432 -> 140550199456384
	140550199456432 [label=ConvolutionBackward0]
	140550199456528 -> 140550199456432
	140550199456528 [label=PermuteBackward0]
	140550199456720 -> 140550199456528
	140550199456720 [label=NativeLayerNormBackward0]
	140550199456816 -> 140550199456720
	140550199456816 [label=PermuteBackward0]
	140551507550704 -> 140550199456816
	140550199456768 -> 140550199456720
	140550947657744 [label="layers.2.blocks.16.norm1.weight
 (512)" fillcolor=lightblue]
	140550947657744 -> 140550199456768
	140550199456768 [label=AccumulateGrad]
	140550199456624 -> 140550199456720
	140550947657984 [label="layers.2.blocks.16.norm1.bias
 (512)" fillcolor=lightblue]
	140550947657984 -> 140550199456624
	140550199456624 [label=AccumulateGrad]
	140550199456480 -> 140550199456432
	140550947658144 [label="layers.2.blocks.16.modulation.f.weight
 (1028, 512, 1, 1)" fillcolor=lightblue]
	140550947658144 -> 140550199456480
	140550199456480 [label=AccumulateGrad]
	140550199456336 -> 140550199456432
	140550947658224 [label="layers.2.blocks.16.modulation.f.bias
 (1028)" fillcolor=lightblue]
	140550947658224 -> 140550199456336
	140550199456336 [label=AccumulateGrad]
	140550199456240 -> 140551793920800
	140550199456240 [label=ConvolutionBackward0]
	140550199456672 -> 140550199456240
	140550199456672 [label=AddBackward0]
	140550199456912 -> 140550199456672
	140550199456912 [label=AddBackward0]
	140550199457152 -> 140550199456912
	140550199457152 [label=AddBackward0]
	140550199457296 -> 140550199457152
	140550199457296 [label=AddBackward0]
	140550199457440 -> 140550199457296
	140550199457440 [label=MulBackward0]
	140550199457536 -> 140550199457440
	140550199457536 [label=GeluBackward0]
	140550199457680 -> 140550199457536
	140550199457680 [label=ConvolutionBackward0]
	140550199456384 -> 140550199457680
	140550199457776 -> 140550199457680
	140550947658864 [label="layers.2.blocks.16.modulation.focal_layers.0.0.weight
 (512, 1, 3, 3)" fillcolor=lightblue]
	140550947658864 -> 140550199457776
	140550199457776 [label=AccumulateGrad]
	140550199457488 -> 140550199457440
	140550199457488 [label=SliceBackward0]
	140550199457872 -> 140550199457488
	140550199457872 [label=SliceBackward0]
	140550199456384 -> 140550199457872
	140550199457248 -> 140550199457152
	140550199457248 [label=MulBackward0]
	140550199457728 -> 140550199457248
	140550199457728 [label=GeluBackward0]
	140550199457632 -> 140550199457728
	140550199457632 [label=ConvolutionBackward0]
	140550199457536 -> 140550199457632
	140550199457968 -> 140550199457632
	140550947659024 [label="layers.2.blocks.16.modulation.focal_layers.1.0.weight
 (512, 1, 5, 5)" fillcolor=lightblue]
	140550947659024 -> 140550199457968
	140550199457968 [label=AccumulateGrad]
	140550199457344 -> 140550199457248
	140550199457344 [label=SliceBackward0]
	140550199458064 -> 140550199457344
	140550199458064 [label=SliceBackward0]
	140550199456384 -> 140550199458064
	140550199457104 -> 140550199456912
	140550199457104 [label=MulBackward0]
	140550199457920 -> 140550199457104
	140550199457920 [label=GeluBackward0]
	140550199457824 -> 140550199457920
	140550199457824 [label=ConvolutionBackward0]
	140550199457728 -> 140550199457824
	140550199458160 -> 140550199457824
	140550947659184 [label="layers.2.blocks.16.modulation.focal_layers.2.0.weight
 (512, 1, 7, 7)" fillcolor=lightblue]
	140550947659184 -> 140550199458160
	140550199458160 [label=AccumulateGrad]
	140550199457392 -> 140550199457104
	140550199457392 [label=SliceBackward0]
	140550199458256 -> 140550199457392
	140550199458256 [label=SliceBackward0]
	140550199456384 -> 140550199458256
	140550199456960 -> 140550199456672
	140550199456960 [label=MulBackward0]
	140550199458112 -> 140550199456960
	140550199458112 [label=GeluBackward0]
	140550199458016 -> 140550199458112
	140550199458016 [label=MeanBackward1]
	140550199457920 -> 140550199458016
	140550199457200 -> 140550199456960
	140550199457200 [label=SliceBackward0]
	140550199458304 -> 140550199457200
	140550199458304 [label=SliceBackward0]
	140550199456384 -> 140550199458304
	140550199456576 -> 140550199456240
	140550947658384 [label="layers.2.blocks.16.modulation.h.weight
 (512, 512, 1, 1)" fillcolor=lightblue]
	140550947658384 -> 140550199456576
	140550199456576 [label=AccumulateGrad]
	140550199456288 -> 140550199456240
	140550947658464 [label="layers.2.blocks.16.modulation.h.bias
 (512)" fillcolor=lightblue]
	140550947658464 -> 140550199456288
	140550199456288 [label=AccumulateGrad]
	140551793920656 -> 140551507550656
	140550947658624 [label="layers.2.blocks.16.modulation.proj.weight
 (512, 512, 1, 1)" fillcolor=lightblue]
	140550947658624 -> 140551793920656
	140551793920656 [label=AccumulateGrad]
	140551507550752 -> 140551507550656
	140550947658704 [label="layers.2.blocks.16.modulation.proj.bias
 (512)" fillcolor=lightblue]
	140550947658704 -> 140551507550752
	140551507550752 [label=AccumulateGrad]
	140551507550512 -> 140551507550416
	140551507550512 [label=ConvolutionBackward0]
	140551507550608 -> 140551507550512
	140551507550608 [label=GeluBackward0]
	140550199458352 -> 140551507550608
	140550199458352 [label=ConvolutionBackward0]
	140550199457584 -> 140550199458352
	140550199457584 [label=PermuteBackward0]
	140550199458544 -> 140550199457584
	140550199458544 [label=NativeLayerNormBackward0]
	140550199458640 -> 140550199458544
	140550199458640 [label=PermuteBackward0]
	140551507550560 -> 140550199458640
	140550199458592 -> 140550199458544
	140550947659264 [label="layers.2.blocks.16.norm2.weight
 (512)" fillcolor=lightblue]
	140550947659264 -> 140550199458592
	140550199458592 [label=AccumulateGrad]
	140550199458448 -> 140550199458544
	140550947659344 [label="layers.2.blocks.16.norm2.bias
 (512)" fillcolor=lightblue]
	140550947659344 -> 140550199458448
	140550199458448 [label=AccumulateGrad]
	140550199458400 -> 140550199458352
	140550947659584 [label="layers.2.blocks.16.mlp.fc1.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	140550947659584 -> 140550199458400
	140550199458400 [label=AccumulateGrad]
	140550199457008 -> 140550199458352
	140550947659664 [label="layers.2.blocks.16.mlp.fc1.bias
 (2048)" fillcolor=lightblue]
	140550947659664 -> 140550199457008
	140550199457008 [label=AccumulateGrad]
	140550199456048 -> 140551507550512
	140550841004176 [label="layers.2.blocks.16.mlp.fc2.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	140550841004176 -> 140550199456048
	140550199456048 [label=AccumulateGrad]
	140550199455904 -> 140551507550512
	140550841004256 [label="layers.2.blocks.16.mlp.fc2.bias
 (512)" fillcolor=lightblue]
	140550841004256 -> 140550199455904
	140550199455904 [label=AccumulateGrad]
	140551507550368 -> 140550708244432
	140551507550368 [label=ConvolutionBackward0]
	140551507550464 -> 140551507550368
	140551507550464 [label=MulBackward0]
	140550199458832 -> 140551507550464
	140550199458832 [label=SplitWithSizesBackward0]
	140550199458880 -> 140550199458832
	140550199458880 [label=ConvolutionBackward0]
	140550199458976 -> 140550199458880
	140550199458976 [label=PermuteBackward0]
	140550199459168 -> 140550199458976
	140550199459168 [label=NativeLayerNormBackward0]
	140550199459264 -> 140550199459168
	140550199459264 [label=PermuteBackward0]
	140551507550416 -> 140550199459264
	140550199459216 -> 140550199459168
	140550841004096 [label="layers.2.blocks.17.norm1.weight
 (512)" fillcolor=lightblue]
	140550841004096 -> 140550199459216
	140550199459216 [label=AccumulateGrad]
	140550199459072 -> 140550199459168
	140550841004336 [label="layers.2.blocks.17.norm1.bias
 (512)" fillcolor=lightblue]
	140550841004336 -> 140550199459072
	140550199459072 [label=AccumulateGrad]
	140550199458928 -> 140550199458880
	140550841004496 [label="layers.2.blocks.17.modulation.f.weight
 (1028, 512, 1, 1)" fillcolor=lightblue]
	140550841004496 -> 140550199458928
	140550199458928 [label=AccumulateGrad]
	140550199458784 -> 140550199458880
	140550841004576 [label="layers.2.blocks.17.modulation.f.bias
 (1028)" fillcolor=lightblue]
	140550841004576 -> 140550199458784
	140550199458784 [label=AccumulateGrad]
	140550199458688 -> 140551507550464
	140550199458688 [label=ConvolutionBackward0]
	140550199459120 -> 140550199458688
	140550199459120 [label=AddBackward0]
	140550199459360 -> 140550199459120
	140550199459360 [label=AddBackward0]
	140550199459600 -> 140550199459360
	140550199459600 [label=AddBackward0]
	140550199459744 -> 140550199459600
	140550199459744 [label=AddBackward0]
	140550199459792 -> 140550199459744
	140550199459792 [label=MulBackward0]
	140550199488720 -> 140550199459792
	140550199488720 [label=GeluBackward0]
	140550199488864 -> 140550199488720
	140550199488864 [label=ConvolutionBackward0]
	140550199458832 -> 140550199488864
	140550199488960 -> 140550199488864
	140550841005216 [label="layers.2.blocks.17.modulation.focal_layers.0.0.weight
 (512, 1, 3, 3)" fillcolor=lightblue]
	140550841005216 -> 140550199488960
	140550199488960 [label=AccumulateGrad]
	140550199488672 -> 140550199459792
	140550199488672 [label=SliceBackward0]
	140550199489056 -> 140550199488672
	140550199489056 [label=SliceBackward0]
	140550199458832 -> 140550199489056
	140550199459696 -> 140550199459600
	140550199459696 [label=MulBackward0]
	140550199488912 -> 140550199459696
	140550199488912 [label=GeluBackward0]
	140550199488816 -> 140550199488912
	140550199488816 [label=ConvolutionBackward0]
	140550199488720 -> 140550199488816
	140550199489152 -> 140550199488816
	140550841005376 [label="layers.2.blocks.17.modulation.focal_layers.1.0.weight
 (512, 1, 5, 5)" fillcolor=lightblue]
	140550841005376 -> 140550199489152
	140550199489152 [label=AccumulateGrad]
	140550199488576 -> 140550199459696
	140550199488576 [label=SliceBackward0]
	140550199489248 -> 140550199488576
	140550199489248 [label=SliceBackward0]
	140550199458832 -> 140550199489248
	140550199459552 -> 140550199459360
	140550199459552 [label=MulBackward0]
	140550199459648 -> 140550199459552
	140550199459648 [label=GeluBackward0]
	140550199489008 -> 140550199459648
	140550199489008 [label=ConvolutionBackward0]
	140550199488912 -> 140550199489008
	140550199489344 -> 140550199489008
	140550841005536 [label="layers.2.blocks.17.modulation.focal_layers.2.0.weight
 (512, 1, 7, 7)" fillcolor=lightblue]
	140550841005536 -> 140550199489344
	140550199489344 [label=AccumulateGrad]
	140550199489104 -> 140550199459552
	140550199489104 [label=SliceBackward0]
	140550199489440 -> 140550199489104
	140550199489440 [label=SliceBackward0]
	140550199458832 -> 140550199489440
	140550199459408 -> 140550199459120
	140550199459408 [label=MulBackward0]
	140550199459504 -> 140550199459408
	140550199459504 [label=GeluBackward0]
	140550199489200 -> 140550199459504
	140550199489200 [label=MeanBackward1]
	140550199459648 -> 140550199489200
	140550199489296 -> 140550199459408
	140550199489296 [label=SliceBackward0]
	140550199489488 -> 140550199489296
	140550199489488 [label=SliceBackward0]
	140550199458832 -> 140550199489488
	140550199459024 -> 140550199458688
	140550841004736 [label="layers.2.blocks.17.modulation.h.weight
 (512, 512, 1, 1)" fillcolor=lightblue]
	140550841004736 -> 140550199459024
	140550199459024 [label=AccumulateGrad]
	140550199458736 -> 140550199458688
	140550841004816 [label="layers.2.blocks.17.modulation.h.bias
 (512)" fillcolor=lightblue]
	140550841004816 -> 140550199458736
	140550199458736 [label=AccumulateGrad]
	140550199457056 -> 140551507550368
	140550841004976 [label="layers.2.blocks.17.modulation.proj.weight
 (512, 512, 1, 1)" fillcolor=lightblue]
	140550841004976 -> 140550199457056
	140550199457056 [label=AccumulateGrad]
	140550199456864 -> 140551507550368
	140550841005056 [label="layers.2.blocks.17.modulation.proj.bias
 (512)" fillcolor=lightblue]
	140550841005056 -> 140550199456864
	140550199456864 [label=AccumulateGrad]
	140550708244384 -> 140550708244288
	140550708244384 [label=ConvolutionBackward0]
	140551507550320 -> 140550708244384
	140551507550320 [label=GeluBackward0]
	140550199459456 -> 140551507550320
	140550199459456 [label=ConvolutionBackward0]
	140550199488768 -> 140550199459456
	140550199488768 [label=PermuteBackward0]
	140550199489728 -> 140550199488768
	140550199489728 [label=NativeLayerNormBackward0]
	140550199489824 -> 140550199489728
	140550199489824 [label=PermuteBackward0]
	140550708244432 -> 140550199489824
	140550199489776 -> 140550199489728
	140550841005616 [label="layers.2.blocks.17.norm2.weight
 (512)" fillcolor=lightblue]
	140550841005616 -> 140550199489776
	140550199489776 [label=AccumulateGrad]
	140550199489632 -> 140550199489728
	140550841005696 [label="layers.2.blocks.17.norm2.bias
 (512)" fillcolor=lightblue]
	140550841005696 -> 140550199489632
	140550199489632 [label=AccumulateGrad]
	140550199489584 -> 140550199459456
	140550841005936 [label="layers.2.blocks.17.mlp.fc1.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	140550841005936 -> 140550199489584
	140550199489584 [label=AccumulateGrad]
	140550199488624 -> 140550199459456
	140550841006016 [label="layers.2.blocks.17.mlp.fc1.bias
 (2048)" fillcolor=lightblue]
	140550841006016 -> 140550199488624
	140550199488624 [label=AccumulateGrad]
	140550199458496 -> 140550708244384
	140550841006176 [label="layers.2.blocks.17.mlp.fc2.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	140550841006176 -> 140550199458496
	140550199458496 [label=AccumulateGrad]
	140550199458208 -> 140550708244384
	140550841006256 [label="layers.2.blocks.17.mlp.fc2.bias
 (512)" fillcolor=lightblue]
	140550841006256 -> 140550199458208
	140550199458208 [label=AccumulateGrad]
	140550708244240 -> 140550708244144
	140550841006336 [label="layers.3.downsample.proj.weight
 (1024, 512, 2, 2)" fillcolor=lightblue]
	140550841006336 -> 140550708244240
	140550708244240 [label=AccumulateGrad]
	140550708243904 -> 140550708244144
	140550841006416 [label="layers.3.downsample.proj.bias
 (1024)" fillcolor=lightblue]
	140550841006416 -> 140550708243904
	140550708243904 [label=AccumulateGrad]
	140550708243472 -> 140550708243520
	140550841006496 [label="layers.3.downsample.norm.weight
 (1024)" fillcolor=lightblue]
	140550841006496 -> 140550708243472
	140550708243472 [label=AccumulateGrad]
	140550708244096 -> 140550708243520
	140550841006576 [label="layers.3.downsample.norm.bias
 (1024)" fillcolor=lightblue]
	140550841006576 -> 140550708244096
	140550708244096 [label=AccumulateGrad]
	140550708243424 -> 140550708243616
	140550708243424 [label=ConvolutionBackward0]
	140550708244192 -> 140550708243424
	140550708244192 [label=MulBackward0]
	140550199459312 -> 140550708244192
	140550199459312 [label=SplitWithSizesBackward0]
	140550199489872 -> 140550199459312
	140550199489872 [label=ConvolutionBackward0]
	140550199489968 -> 140550199489872
	140550199489968 [label=PermuteBackward0]
	140550199490160 -> 140550199489968
	140550199490160 [label=NativeLayerNormBackward0]
	140550199490256 -> 140550199490160
	140550199490256 [label=PermuteBackward0]
	140550708242944 -> 140550199490256
	140550199490208 -> 140550199490160
	140550841006656 [label="layers.3.blocks.0.norm1.weight
 (1024)" fillcolor=lightblue]
	140550841006656 -> 140550199490208
	140550199490208 [label=AccumulateGrad]
	140550199490064 -> 140550199490160
	140550841006736 [label="layers.3.blocks.0.norm1.bias
 (1024)" fillcolor=lightblue]
	140550841006736 -> 140550199490064
	140550199490064 [label=AccumulateGrad]
	140550199490016 -> 140550199489872
	140550841006896 [label="layers.3.blocks.0.modulation.f.weight
 (2052, 1024, 1, 1)" fillcolor=lightblue]
	140550841006896 -> 140550199490016
	140550199490016 [label=AccumulateGrad]
	140550199489392 -> 140550199489872
	140550841006976 [label="layers.3.blocks.0.modulation.f.bias
 (2052)" fillcolor=lightblue]
	140550841006976 -> 140550199489392
	140550199489392 [label=AccumulateGrad]
	140551507550272 -> 140550708244192
	140551507550272 [label=ConvolutionBackward0]
	140550199490112 -> 140551507550272
	140550199490112 [label=AddBackward0]
	140550199490352 -> 140550199490112
	140550199490352 [label=AddBackward0]
	140550199490592 -> 140550199490352
	140550199490592 [label=AddBackward0]
	140550199490736 -> 140550199490592
	140550199490736 [label=AddBackward0]
	140550199490880 -> 140550199490736
	140550199490880 [label=MulBackward0]
	140550199490976 -> 140550199490880
	140550199490976 [label=GeluBackward0]
	140550199491120 -> 140550199490976
	140550199491120 [label=ConvolutionBackward0]
	140550199459312 -> 140550199491120
	140550199491216 -> 140550199491120
	140550841007616 [label="layers.3.blocks.0.modulation.focal_layers.0.0.weight
 (1024, 1, 3, 3)" fillcolor=lightblue]
	140550841007616 -> 140550199491216
	140550199491216 [label=AccumulateGrad]
	140550199490928 -> 140550199490880
	140550199490928 [label=SliceBackward0]
	140550199491312 -> 140550199490928
	140550199491312 [label=SliceBackward0]
	140550199459312 -> 140550199491312
	140550199490688 -> 140550199490592
	140550199490688 [label=MulBackward0]
	140550199491168 -> 140550199490688
	140550199491168 [label=GeluBackward0]
	140550199491072 -> 140550199491168
	140550199491072 [label=ConvolutionBackward0]
	140550199490976 -> 140550199491072
	140550199491408 -> 140550199491072
	140550841007776 [label="layers.3.blocks.0.modulation.focal_layers.1.0.weight
 (1024, 1, 5, 5)" fillcolor=lightblue]
	140550841007776 -> 140550199491408
	140550199491408 [label=AccumulateGrad]
	140550199490784 -> 140550199490688
	140550199490784 [label=SliceBackward0]
	140550199491504 -> 140550199490784
	140550199491504 [label=SliceBackward0]
	140550199459312 -> 140550199491504
	140550199490544 -> 140550199490352
	140550199490544 [label=MulBackward0]
	140550199491360 -> 140550199490544
	140550199491360 [label=GeluBackward0]
	140550199491264 -> 140550199491360
	140550199491264 [label=ConvolutionBackward0]
	140550199491168 -> 140550199491264
	140550199491600 -> 140550199491264
	140550841007936 [label="layers.3.blocks.0.modulation.focal_layers.2.0.weight
 (1024, 1, 7, 7)" fillcolor=lightblue]
	140550841007936 -> 140550199491600
	140550199491600 [label=AccumulateGrad]
	140550199490832 -> 140550199490544
	140550199490832 [label=SliceBackward0]
	140550199491696 -> 140550199490832
	140550199491696 [label=SliceBackward0]
	140550199459312 -> 140550199491696
	140550199490400 -> 140550199490112
	140550199490400 [label=MulBackward0]
	140550199491552 -> 140550199490400
	140550199491552 [label=GeluBackward0]
	140550199491456 -> 140550199491552
	140550199491456 [label=MeanBackward1]
	140550199491360 -> 140550199491456
	140550199490640 -> 140550199490400
	140550199490640 [label=SliceBackward0]
	140550199491744 -> 140550199490640
	140550199491744 [label=SliceBackward0]
	140550199459312 -> 140550199491744
	140550199489920 -> 140551507550272
	140550841007136 [label="layers.3.blocks.0.modulation.h.weight
 (1024, 1024, 1, 1)" fillcolor=lightblue]
	140550841007136 -> 140550199489920
	140550199489920 [label=AccumulateGrad]
	140550199489680 -> 140551507550272
	140550841007216 [label="layers.3.blocks.0.modulation.h.bias
 (1024)" fillcolor=lightblue]
	140550841007216 -> 140550199489680
	140550199489680 [label=AccumulateGrad]
	140550708244048 -> 140550708243424
	140550841007376 [label="layers.3.blocks.0.modulation.proj.weight
 (1024, 1024, 1, 1)" fillcolor=lightblue]
	140550841007376 -> 140550708244048
	140550708244048 [label=AccumulateGrad]
	140550708244000 -> 140550708243424
	140550841007456 [label="layers.3.blocks.0.modulation.proj.bias
 (1024)" fillcolor=lightblue]
	140550841007456 -> 140550708244000
	140550708244000 [label=AccumulateGrad]
	140550708243568 -> 140550708243712
	140550708243568 [label=ConvolutionBackward0]
	140550708244336 -> 140550708243568
	140550708244336 [label=GeluBackward0]
	140550199491792 -> 140550708244336
	140550199491792 [label=ConvolutionBackward0]
	140550199491024 -> 140550199491792
	140550199491024 [label=PermuteBackward0]
	140550199491984 -> 140550199491024
	140550199491984 [label=NativeLayerNormBackward0]
	140550199492080 -> 140550199491984
	140550199492080 [label=PermuteBackward0]
	140550708243616 -> 140550199492080
	140550199492032 -> 140550199491984
	140550841008016 [label="layers.3.blocks.0.norm2.weight
 (1024)" fillcolor=lightblue]
	140550841008016 -> 140550199492032
	140550199492032 [label=AccumulateGrad]
	140550199491888 -> 140550199491984
	140550429638720 [label="layers.3.blocks.0.norm2.bias
 (1024)" fillcolor=lightblue]
	140550429638720 -> 140550199491888
	140550199491888 [label=AccumulateGrad]
	140550199491840 -> 140550199491792
	140550429638960 [label="layers.3.blocks.0.mlp.fc1.weight
 (4096, 1024, 1, 1)" fillcolor=lightblue]
	140550429638960 -> 140550199491840
	140550199491840 [label=AccumulateGrad]
	140550199490448 -> 140550199491792
	140550429639040 [label="layers.3.blocks.0.mlp.fc1.bias
 (4096)" fillcolor=lightblue]
	140550429639040 -> 140550199490448
	140550199490448 [label=AccumulateGrad]
	140550708243376 -> 140550708243568
	140550429639200 [label="layers.3.blocks.0.mlp.fc2.weight
 (1024, 4096, 1, 1)" fillcolor=lightblue]
	140550429639200 -> 140550708243376
	140550708243376 [label=AccumulateGrad]
	140550199489536 -> 140550708243568
	140550429639280 [label="layers.3.blocks.0.mlp.fc2.bias
 (1024)" fillcolor=lightblue]
	140550429639280 -> 140550199489536
	140550199489536 [label=AccumulateGrad]
	140550708243232 -> 140550708243328
	140550708243232 [label=ConvolutionBackward0]
	140550708243808 -> 140550708243232
	140550708243808 [label=MulBackward0]
	140550199492272 -> 140550708243808
	140550199492272 [label=SplitWithSizesBackward0]
	140550199492320 -> 140550199492272
	140550199492320 [label=ConvolutionBackward0]
	140550199492416 -> 140550199492320
	140550199492416 [label=PermuteBackward0]
	140550199492560 -> 140550199492416
	140550199492560 [label=NativeLayerNormBackward0]
	140550199517344 -> 140550199492560
	140550199517344 [label=PermuteBackward0]
	140550708243712 -> 140550199517344
	140550199517296 -> 140550199492560
	140550429639120 [label="layers.3.blocks.1.norm1.weight
 (1024)" fillcolor=lightblue]
	140550429639120 -> 140550199517296
	140550199517296 [label=AccumulateGrad]
	140550199517248 -> 140550199492560
	140550429639360 [label="layers.3.blocks.1.norm1.bias
 (1024)" fillcolor=lightblue]
	140550429639360 -> 140550199517248
	140550199517248 [label=AccumulateGrad]
	140550199492368 -> 140550199492320
	140550429639520 [label="layers.3.blocks.1.modulation.f.weight
 (2052, 1024, 1, 1)" fillcolor=lightblue]
	140550429639520 -> 140550199492368
	140550199492368 [label=AccumulateGrad]
	140550199492224 -> 140550199492320
	140550429639600 [label="layers.3.blocks.1.modulation.f.bias
 (2052)" fillcolor=lightblue]
	140550429639600 -> 140550199492224
	140550199492224 [label=AccumulateGrad]
	140550199492128 -> 140550708243808
	140550199492128 [label=ConvolutionBackward0]
	140550199492512 -> 140550199492128
	140550199492512 [label=AddBackward0]
	140550199517440 -> 140550199492512
	140550199517440 [label=AddBackward0]
	140550199517680 -> 140550199517440
	140550199517680 [label=AddBackward0]
	140550199517824 -> 140550199517680
	140550199517824 [label=AddBackward0]
	140550199517968 -> 140550199517824
	140550199517968 [label=MulBackward0]
	140550199518064 -> 140550199517968
	140550199518064 [label=GeluBackward0]
	140550199518208 -> 140550199518064
	140550199518208 [label=ConvolutionBackward0]
	140550199492272 -> 140550199518208
	140550199518304 -> 140550199518208
	140550429640240 [label="layers.3.blocks.1.modulation.focal_layers.0.0.weight
 (1024, 1, 3, 3)" fillcolor=lightblue]
	140550429640240 -> 140550199518304
	140550199518304 [label=AccumulateGrad]
	140550199518016 -> 140550199517968
	140550199518016 [label=SliceBackward0]
	140550199518400 -> 140550199518016
	140550199518400 [label=SliceBackward0]
	140550199492272 -> 140550199518400
	140550199517776 -> 140550199517680
	140550199517776 [label=MulBackward0]
	140550199518256 -> 140550199517776
	140550199518256 [label=GeluBackward0]
	140550199518160 -> 140550199518256
	140550199518160 [label=ConvolutionBackward0]
	140550199518064 -> 140550199518160
	140550199518496 -> 140550199518160
	140550429640400 [label="layers.3.blocks.1.modulation.focal_layers.1.0.weight
 (1024, 1, 5, 5)" fillcolor=lightblue]
	140550429640400 -> 140550199518496
	140550199518496 [label=AccumulateGrad]
	140550199517872 -> 140550199517776
	140550199517872 [label=SliceBackward0]
	140550199518592 -> 140550199517872
	140550199518592 [label=SliceBackward0]
	140550199492272 -> 140550199518592
	140550199517632 -> 140550199517440
	140550199517632 [label=MulBackward0]
	140550199518448 -> 140550199517632
	140550199518448 [label=GeluBackward0]
	140550199518352 -> 140550199518448
	140550199518352 [label=ConvolutionBackward0]
	140550199518256 -> 140550199518352
	140550199518688 -> 140550199518352
	140550429640560 [label="layers.3.blocks.1.modulation.focal_layers.2.0.weight
 (1024, 1, 7, 7)" fillcolor=lightblue]
	140550429640560 -> 140550199518688
	140550199518688 [label=AccumulateGrad]
	140550199517920 -> 140550199517632
	140550199517920 [label=SliceBackward0]
	140550199518784 -> 140550199517920
	140550199518784 [label=SliceBackward0]
	140550199492272 -> 140550199518784
	140550199517488 -> 140550199492512
	140550199517488 [label=MulBackward0]
	140550199518640 -> 140550199517488
	140550199518640 [label=GeluBackward0]
	140550199518544 -> 140550199518640
	140550199518544 [label=MeanBackward1]
	140550199518448 -> 140550199518544
	140550199517728 -> 140550199517488
	140550199517728 [label=SliceBackward0]
	140550199518832 -> 140550199517728
	140550199518832 [label=SliceBackward0]
	140550199492272 -> 140550199518832
	140550199492464 -> 140550199492128
	140550429639760 [label="layers.3.blocks.1.modulation.h.weight
 (1024, 1024, 1, 1)" fillcolor=lightblue]
	140550429639760 -> 140550199492464
	140550199492464 [label=AccumulateGrad]
	140550199492176 -> 140550199492128
	140550429639840 [label="layers.3.blocks.1.modulation.h.bias
 (1024)" fillcolor=lightblue]
	140550429639840 -> 140550199492176
	140550199492176 [label=AccumulateGrad]
	140550199490496 -> 140550708243232
	140550429640000 [label="layers.3.blocks.1.modulation.proj.weight
 (1024, 1024, 1, 1)" fillcolor=lightblue]
	140550429640000 -> 140550199490496
	140550199490496 [label=AccumulateGrad]
	140550199490304 -> 140550708243232
	140550429640080 [label="layers.3.blocks.1.modulation.proj.bias
 (1024)" fillcolor=lightblue]
	140550429640080 -> 140550199490304
	140550199490304 [label=AccumulateGrad]
	140550708243280 -> 140550708243136
	140550708243280 [label=ConvolutionBackward0]
	140550199491936 -> 140550708243280
	140550199491936 [label=GeluBackward0]
	140550199518880 -> 140550199491936
	140550199518880 [label=ConvolutionBackward0]
	140550199518112 -> 140550199518880
	140550199518112 [label=PermuteBackward0]
	140550199519072 -> 140550199518112
	140550199519072 [label=NativeLayerNormBackward0]
	140550199519168 -> 140550199519072
	140550199519168 [label=PermuteBackward0]
	140550708243328 -> 140550199519168
	140550199519120 -> 140550199519072
	140550429640640 [label="layers.3.blocks.1.norm2.weight
 (1024)" fillcolor=lightblue]
	140550429640640 -> 140550199519120
	140550199519120 [label=AccumulateGrad]
	140550199518976 -> 140550199519072
	140550429640720 [label="layers.3.blocks.1.norm2.bias
 (1024)" fillcolor=lightblue]
	140550429640720 -> 140550199518976
	140550199518976 [label=AccumulateGrad]
	140550199518928 -> 140550199518880
	140550429640960 [label="layers.3.blocks.1.mlp.fc1.weight
 (4096, 1024, 1, 1)" fillcolor=lightblue]
	140550429640960 -> 140550199518928
	140550199518928 [label=AccumulateGrad]
	140550199517536 -> 140550199518880
	140550429641040 [label="layers.3.blocks.1.mlp.fc1.bias
 (4096)" fillcolor=lightblue]
	140550429641040 -> 140550199517536
	140550199517536 [label=AccumulateGrad]
	140550199491648 -> 140550708243280
	140550429641200 [label="layers.3.blocks.1.mlp.fc2.weight
 (1024, 4096, 1, 1)" fillcolor=lightblue]
	140550429641200 -> 140550199491648
	140550199491648 [label=AccumulateGrad]
	140550708243184 -> 140550708243280
	140550429641280 [label="layers.3.blocks.1.mlp.fc2.bias
 (1024)" fillcolor=lightblue]
	140550429641280 -> 140550708243184
	140550708243184 [label=AccumulateGrad]
	140550707780432 -> 140550707779184
	140550429641120 [label="norm.weight
 (1024)" fillcolor=lightblue]
	140550429641120 -> 140550707780432
	140550707780432 [label=AccumulateGrad]
	140550708243952 -> 140550707779184
	140550429641360 [label="norm.bias
 (1024)" fillcolor=lightblue]
	140550429641360 -> 140550708243952
	140550708243952 [label=AccumulateGrad]
	140550707781056 -> 140550708217888
	140550707781056 [label=TBackward0]
	140550707781200 -> 140550707781056
	140550429641440 [label="head.fc.weight
 (1000, 1024)" fillcolor=lightblue]
	140550429641440 -> 140550707781200
	140550707781200 [label=AccumulateGrad]
	140550708217888 -> 140551507433248
}
