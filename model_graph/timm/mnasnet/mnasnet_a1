digraph {
	graph [size="168.9,168.9"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	140670817532320 [label="
 (1, 1000)" fillcolor=darkolivegreen1]
	140670811373520 [label=AddmmBackward0]
	140670811371600 -> 140670811373520
	140670811334592 [label="classifier.bias
 (1000)" fillcolor=lightblue]
	140670811334592 -> 140670811371600
	140670811371600 [label=AccumulateGrad]
	140670811371792 -> 140670811373520
	140670811371792 [label=ReshapeAliasBackward0]
	140670811373328 -> 140670811371792
	140670811373328 [label=MeanBackward1]
	140670811370592 -> 140670811373328
	140670811370592 [label=ReluBackward0]
	140670811370736 -> 140670811370592
	140670811370736 [label=NativeBatchNormBackward0]
	140670811373088 -> 140670811370736
	140670811373088 [label=ConvolutionBackward0]
	140670811370544 -> 140670811373088
	140670811370544 [label=NativeBatchNormBackward0]
	140670817431712 -> 140670811370544
	140670817431712 [label=ConvolutionBackward0]
	140670817431904 -> 140670817431712
	140670817431904 [label=ReluBackward0]
	140670817431664 -> 140670817431904
	140670817431664 [label=NativeBatchNormBackward0]
	140670817431616 -> 140670817431664
	140670817431616 [label=ConvolutionBackward0]
	140670817432384 -> 140670817431616
	140670817432384 [label=ReluBackward0]
	140670817432528 -> 140670817432384
	140670817432528 [label=NativeBatchNormBackward0]
	140670817432624 -> 140670817432528
	140670817432624 [label=ConvolutionBackward0]
	140670817432816 -> 140670817432624
	140670817432816 [label=AddBackward0]
	140670817432960 -> 140670817432816
	140670817432960 [label=NativeBatchNormBackward0]
	140670817433104 -> 140670817432960
	140670817433104 [label=ConvolutionBackward0]
	140670817433296 -> 140670817433104
	140670817433296 [label=MulBackward0]
	140670817433440 -> 140670817433296
	140670817433440 [label=ReluBackward0]
	140670817433584 -> 140670817433440
	140670817433584 [label=NativeBatchNormBackward0]
	140670817433680 -> 140670817433584
	140670817433680 [label=ConvolutionBackward0]
	140670817433872 -> 140670817433680
	140670817433872 [label=ReluBackward0]
	140670817434016 -> 140670817433872
	140670817434016 [label=NativeBatchNormBackward0]
	140670817434112 -> 140670817434016
	140670817434112 [label=ConvolutionBackward0]
	140670817432912 -> 140670817434112
	140670817432912 [label=AddBackward0]
	140670817434400 -> 140670817432912
	140670817434400 [label=NativeBatchNormBackward0]
	140670817434544 -> 140670817434400
	140670817434544 [label=ConvolutionBackward0]
	140670817434736 -> 140670817434544
	140670817434736 [label=MulBackward0]
	140670817434880 -> 140670817434736
	140670817434880 [label=ReluBackward0]
	140670817435024 -> 140670817434880
	140670817435024 [label=NativeBatchNormBackward0]
	140670817435120 -> 140670817435024
	140670817435120 [label=ConvolutionBackward0]
	140670817435312 -> 140670817435120
	140670817435312 [label=ReluBackward0]
	140670817435456 -> 140670817435312
	140670817435456 [label=NativeBatchNormBackward0]
	140670817435552 -> 140670817435456
	140670817435552 [label=ConvolutionBackward0]
	140670817434352 -> 140670817435552
	140670817434352 [label=NativeBatchNormBackward0]
	140670822678784 -> 140670817434352
	140670822678784 [label=ConvolutionBackward0]
	140670822678976 -> 140670822678784
	140670822678976 [label=MulBackward0]
	140670822679120 -> 140670822678976
	140670822679120 [label=ReluBackward0]
	140670822679264 -> 140670822679120
	140670822679264 [label=NativeBatchNormBackward0]
	140670822679360 -> 140670822679264
	140670822679360 [label=ConvolutionBackward0]
	140670822679552 -> 140670822679360
	140670822679552 [label=ReluBackward0]
	140670822679696 -> 140670822679552
	140670822679696 [label=NativeBatchNormBackward0]
	140670822679792 -> 140670822679696
	140670822679792 [label=ConvolutionBackward0]
	140670822679984 -> 140670822679792
	140670822679984 [label=AddBackward0]
	140670822680128 -> 140670822679984
	140670822680128 [label=NativeBatchNormBackward0]
	140670822680272 -> 140670822680128
	140670822680272 [label=ConvolutionBackward0]
	140670822680464 -> 140670822680272
	140670822680464 [label=MulBackward0]
	140670822680608 -> 140670822680464
	140670822680608 [label=ReluBackward0]
	140670822680752 -> 140670822680608
	140670822680752 [label=NativeBatchNormBackward0]
	140670822680848 -> 140670822680752
	140670822680848 [label=ConvolutionBackward0]
	140670822681040 -> 140670822680848
	140670822681040 [label=ReluBackward0]
	140670822681184 -> 140670822681040
	140670822681184 [label=NativeBatchNormBackward0]
	140670822681280 -> 140670822681184
	140670822681280 [label=ConvolutionBackward0]
	140670822680080 -> 140670822681280
	140670822680080 [label=NativeBatchNormBackward0]
	140670822681568 -> 140670822680080
	140670822681568 [label=ConvolutionBackward0]
	140670822681760 -> 140670822681568
	140670822681760 [label=MulBackward0]
	140670822681904 -> 140670822681760
	140670822681904 [label=ReluBackward0]
	140670822682048 -> 140670822681904
	140670822682048 [label=NativeBatchNormBackward0]
	140670822682144 -> 140670822682048
	140670822682144 [label=ConvolutionBackward0]
	140670822682336 -> 140670822682144
	140670822682336 [label=ReluBackward0]
	140670822682480 -> 140670822682336
	140670822682480 [label=NativeBatchNormBackward0]
	140670822682576 -> 140670822682480
	140670822682576 [label=ConvolutionBackward0]
	140670822760656 -> 140670822682576
	140670822760656 [label=AddBackward0]
	140670822760800 -> 140670822760656
	140670822760800 [label=NativeBatchNormBackward0]
	140670822760944 -> 140670822760800
	140670822760944 [label=ConvolutionBackward0]
	140670822761136 -> 140670822760944
	140670822761136 [label=ReluBackward0]
	140670822761280 -> 140670822761136
	140670822761280 [label=NativeBatchNormBackward0]
	140670822761376 -> 140670822761280
	140670822761376 [label=ConvolutionBackward0]
	140670822761568 -> 140670822761376
	140670822761568 [label=ReluBackward0]
	140670822761712 -> 140670822761568
	140670822761712 [label=NativeBatchNormBackward0]
	140670822761808 -> 140670822761712
	140670822761808 [label=ConvolutionBackward0]
	140670822760752 -> 140670822761808
	140670822760752 [label=AddBackward0]
	140670822762096 -> 140670822760752
	140670822762096 [label=NativeBatchNormBackward0]
	140670822762240 -> 140670822762096
	140670822762240 [label=ConvolutionBackward0]
	140670822762432 -> 140670822762240
	140670822762432 [label=ReluBackward0]
	140670822762576 -> 140670822762432
	140670822762576 [label=NativeBatchNormBackward0]
	140670822762624 -> 140670822762576
	140670822762624 [label=ConvolutionBackward0]
	140670822762912 -> 140670822762624
	140670822762912 [label=ReluBackward0]
	140670822763056 -> 140670822762912
	140670822763056 [label=NativeBatchNormBackward0]
	140670822763104 -> 140670822763056
	140670822763104 [label=ConvolutionBackward0]
	140670822762048 -> 140670822763104
	140670822762048 [label=AddBackward0]
	140670822763488 -> 140670822762048
	140670822763488 [label=NativeBatchNormBackward0]
	140670822763632 -> 140670822763488
	140670822763632 [label=ConvolutionBackward0]
	140670822763824 -> 140670822763632
	140670822763824 [label=ReluBackward0]
	140670822763968 -> 140670822763824
	140670822763968 [label=NativeBatchNormBackward0]
	140670822764016 -> 140670822763968
	140670822764016 [label=ConvolutionBackward0]
	140670822764304 -> 140670822764016
	140670822764304 [label=ReluBackward0]
	140670822764448 -> 140670822764304
	140670822764448 [label=NativeBatchNormBackward0]
	140670822764352 -> 140670822764448
	140670822764352 [label=ConvolutionBackward0]
	140670822763440 -> 140670822764352
	140670822763440 [label=NativeBatchNormBackward0]
	140670822773136 -> 140670822763440
	140670822773136 [label=ConvolutionBackward0]
	140670822773328 -> 140670822773136
	140670822773328 [label=ReluBackward0]
	140670822773472 -> 140670822773328
	140670822773472 [label=NativeBatchNormBackward0]
	140670822773520 -> 140670822773472
	140670822773520 [label=ConvolutionBackward0]
	140670822773808 -> 140670822773520
	140670822773808 [label=ReluBackward0]
	140670822773952 -> 140670822773808
	140670822773952 [label=NativeBatchNormBackward0]
	140670822774000 -> 140670822773952
	140670822774000 [label=ConvolutionBackward0]
	140670822774288 -> 140670822774000
	140670822774288 [label=AddBackward0]
	140670822774432 -> 140670822774288
	140670822774432 [label=NativeBatchNormBackward0]
	140670822774576 -> 140670822774432
	140670822774576 [label=ConvolutionBackward0]
	140670822774768 -> 140670822774576
	140670822774768 [label=MulBackward0]
	140670822774912 -> 140670822774768
	140670822774912 [label=ReluBackward0]
	140670822775056 -> 140670822774912
	140670822775056 [label=NativeBatchNormBackward0]
	140670822775104 -> 140670822775056
	140670822775104 [label=ConvolutionBackward0]
	140670822775392 -> 140670822775104
	140670822775392 [label=ReluBackward0]
	140670822775536 -> 140670822775392
	140670822775536 [label=NativeBatchNormBackward0]
	140670822775584 -> 140670822775536
	140670822775584 [label=ConvolutionBackward0]
	140670822774384 -> 140670822775584
	140670822774384 [label=AddBackward0]
	140670822775968 -> 140670822774384
	140670822775968 [label=NativeBatchNormBackward0]
	140670822776112 -> 140670822775968
	140670822776112 [label=ConvolutionBackward0]
	140670822776304 -> 140670822776112
	140670822776304 [label=MulBackward0]
	140670822776448 -> 140670822776304
	140670822776448 [label=ReluBackward0]
	140670822776592 -> 140670822776448
	140670822776592 [label=NativeBatchNormBackward0]
	140670822776640 -> 140670822776592
	140670822776640 [label=ConvolutionBackward0]
	140670822785088 -> 140670822776640
	140670822785088 [label=ReluBackward0]
	140670822785328 -> 140670822785088
	140670822785328 [label=NativeBatchNormBackward0]
	140670822785376 -> 140670822785328
	140670822785376 [label=ConvolutionBackward0]
	140670822775920 -> 140670822785376
	140670822775920 [label=NativeBatchNormBackward0]
	140670822785760 -> 140670822775920
	140670822785760 [label=ConvolutionBackward0]
	140670822785952 -> 140670822785760
	140670822785952 [label=MulBackward0]
	140670822786096 -> 140670822785952
	140670822786096 [label=ReluBackward0]
	140670822786240 -> 140670822786096
	140670822786240 [label=NativeBatchNormBackward0]
	140670822786288 -> 140670822786240
	140670822786288 [label=ConvolutionBackward0]
	140670822786576 -> 140670822786288
	140670822786576 [label=ReluBackward0]
	140670822786720 -> 140670822786576
	140670822786720 [label=NativeBatchNormBackward0]
	140670822786768 -> 140670822786720
	140670822786768 [label=ConvolutionBackward0]
	140670822787056 -> 140670822786768
	140670822787056 [label=AddBackward0]
	140670822787200 -> 140670822787056
	140670822787200 [label=NativeBatchNormBackward0]
	140670822787344 -> 140670822787200
	140670822787344 [label=ConvolutionBackward0]
	140670822787536 -> 140670822787344
	140670822787536 [label=ReluBackward0]
	140670822787680 -> 140670822787536
	140670822787680 [label=NativeBatchNormBackward0]
	140670822787728 -> 140670822787680
	140670822787728 [label=ConvolutionBackward0]
	140670822788016 -> 140670822787728
	140670822788016 [label=ReluBackward0]
	140670822788160 -> 140670822788016
	140670822788160 [label=NativeBatchNormBackward0]
	140670822788208 -> 140670822788160
	140670822788208 [label=ConvolutionBackward0]
	140670822787152 -> 140670822788208
	140670822787152 [label=NativeBatchNormBackward0]
	140670822788592 -> 140670822787152
	140670822788592 [label=ConvolutionBackward0]
	140670822788784 -> 140670822788592
	140670822788784 [label=ReluBackward0]
	140670822788928 -> 140670822788784
	140670822788928 [label=NativeBatchNormBackward0]
	140670822788976 -> 140670822788928
	140670822788976 [label=ConvolutionBackward0]
	140670822797520 -> 140670822788976
	140670822797520 [label=ReluBackward0]
	140670822797664 -> 140670822797520
	140670822797664 [label=NativeBatchNormBackward0]
	140670822797712 -> 140670822797664
	140670822797712 [label=ConvolutionBackward0]
	140670822798000 -> 140670822797712
	140670822798000 [label=NativeBatchNormBackward0]
	140670822798144 -> 140670822798000
	140670822798144 [label=ConvolutionBackward0]
	140670822798336 -> 140670822798144
	140670822798336 [label=ReluBackward0]
	140670822798480 -> 140670822798336
	140670822798480 [label=NativeBatchNormBackward0]
	140670822798528 -> 140670822798480
	140670822798528 [label=ConvolutionBackward0]
	140670822798816 -> 140670822798528
	140670822798816 [label=ReluBackward0]
	140670822798960 -> 140670822798816
	140670822798960 [label=NativeBatchNormBackward0]
	140670822799008 -> 140670822798960
	140670822799008 [label=ConvolutionBackward0]
	140670822799296 -> 140670822799008
	140671334962976 [label="conv_stem.weight
 (32, 3, 3, 3)" fillcolor=lightblue]
	140671334962976 -> 140670822799296
	140670822799296 [label=AccumulateGrad]
	140670822798864 -> 140670822798960
	140671334963056 [label="bn1.weight
 (32)" fillcolor=lightblue]
	140671334963056 -> 140670822798864
	140670822798864 [label=AccumulateGrad]
	140670822799104 -> 140670822798960
	140671334963136 [label="bn1.bias
 (32)" fillcolor=lightblue]
	140671334963136 -> 140670822799104
	140670822799104 [label=AccumulateGrad]
	140670822798768 -> 140670822798528
	140671334963696 [label="blocks.0.0.conv_dw.weight
 (32, 1, 3, 3)" fillcolor=lightblue]
	140671334963696 -> 140670822798768
	140670822798768 [label=AccumulateGrad]
	140670822798384 -> 140670822798480
	140671334963776 [label="blocks.0.0.bn1.weight
 (32)" fillcolor=lightblue]
	140671334963776 -> 140670822798384
	140670822798384 [label=AccumulateGrad]
	140670822798624 -> 140670822798480
	140671334963856 [label="blocks.0.0.bn1.bias
 (32)" fillcolor=lightblue]
	140671334963856 -> 140670822798624
	140670822798624 [label=AccumulateGrad]
	140670822798288 -> 140670822798144
	140671334964416 [label="blocks.0.0.conv_pw.weight
 (16, 32, 1, 1)" fillcolor=lightblue]
	140671334964416 -> 140670822798288
	140670822798288 [label=AccumulateGrad]
	140670822798096 -> 140670822798000
	140671334964496 [label="blocks.0.0.bn2.weight
 (16)" fillcolor=lightblue]
	140671334964496 -> 140670822798096
	140670822798096 [label=AccumulateGrad]
	140670822798048 -> 140670822798000
	140671334964576 [label="blocks.0.0.bn2.bias
 (16)" fillcolor=lightblue]
	140671334964576 -> 140670822798048
	140670822798048 [label=AccumulateGrad]
	140670822797952 -> 140670822797712
	140671334964976 [label="blocks.1.0.conv_pw.weight
 (96, 16, 1, 1)" fillcolor=lightblue]
	140671334964976 -> 140670822797952
	140670822797952 [label=AccumulateGrad]
	140670822797568 -> 140670822797664
	140671334965056 [label="blocks.1.0.bn1.weight
 (96)" fillcolor=lightblue]
	140671334965056 -> 140670822797568
	140670822797568 [label=AccumulateGrad]
	140670822797808 -> 140670822797664
	140671334965136 [label="blocks.1.0.bn1.bias
 (96)" fillcolor=lightblue]
	140671334965136 -> 140670822797808
	140670822797808 [label=AccumulateGrad]
	140670822797472 -> 140670822788976
	140671734485456 [label="blocks.1.0.conv_dw.weight
 (96, 1, 3, 3)" fillcolor=lightblue]
	140671734485456 -> 140670822797472
	140670822797472 [label=AccumulateGrad]
	140670822788832 -> 140670822788928
	140671734485376 [label="blocks.1.0.bn2.weight
 (96)" fillcolor=lightblue]
	140671734485376 -> 140670822788832
	140670822788832 [label=AccumulateGrad]
	140670822789072 -> 140670822788928
	140671734485536 [label="blocks.1.0.bn2.bias
 (96)" fillcolor=lightblue]
	140671734485536 -> 140670822789072
	140670822789072 [label=AccumulateGrad]
	140670822788736 -> 140670822788592
	140671734485936 [label="blocks.1.0.conv_pwl.weight
 (24, 96, 1, 1)" fillcolor=lightblue]
	140671734485936 -> 140670822788736
	140670822788736 [label=AccumulateGrad]
	140670822788544 -> 140670822787152
	140671734486016 [label="blocks.1.0.bn3.weight
 (24)" fillcolor=lightblue]
	140671734486016 -> 140670822788544
	140670822788544 [label=AccumulateGrad]
	140670822788400 -> 140670822787152
	140671734486096 [label="blocks.1.0.bn3.bias
 (24)" fillcolor=lightblue]
	140671734486096 -> 140670822788400
	140670822788400 [label=AccumulateGrad]
	140670822788496 -> 140670822788208
	140671734486496 [label="blocks.1.1.conv_pw.weight
 (144, 24, 1, 1)" fillcolor=lightblue]
	140671734486496 -> 140670822788496
	140670822788496 [label=AccumulateGrad]
	140670822788064 -> 140670822788160
	140671734486576 [label="blocks.1.1.bn1.weight
 (144)" fillcolor=lightblue]
	140671734486576 -> 140670822788064
	140670822788064 [label=AccumulateGrad]
	140670822788304 -> 140670822788160
	140671734486656 [label="blocks.1.1.bn1.bias
 (144)" fillcolor=lightblue]
	140671734486656 -> 140670822788304
	140670822788304 [label=AccumulateGrad]
	140670822787968 -> 140670822787728
	140671734487136 [label="blocks.1.1.conv_dw.weight
 (144, 1, 3, 3)" fillcolor=lightblue]
	140671734487136 -> 140670822787968
	140670822787968 [label=AccumulateGrad]
	140670822787584 -> 140670822787680
	140671734487056 [label="blocks.1.1.bn2.weight
 (144)" fillcolor=lightblue]
	140671734487056 -> 140670822787584
	140670822787584 [label=AccumulateGrad]
	140670822787824 -> 140670822787680
	140671734487216 [label="blocks.1.1.bn2.bias
 (144)" fillcolor=lightblue]
	140671734487216 -> 140670822787824
	140670822787824 [label=AccumulateGrad]
	140670822787488 -> 140670822787344
	140671734487616 [label="blocks.1.1.conv_pwl.weight
 (24, 144, 1, 1)" fillcolor=lightblue]
	140671734487616 -> 140670822787488
	140670822787488 [label=AccumulateGrad]
	140670822787296 -> 140670822787200
	140671734487696 [label="blocks.1.1.bn3.weight
 (24)" fillcolor=lightblue]
	140671734487696 -> 140670822787296
	140670822787296 [label=AccumulateGrad]
	140670822787248 -> 140670822787200
	140671734487776 [label="blocks.1.1.bn3.bias
 (24)" fillcolor=lightblue]
	140671734487776 -> 140670822787248
	140670822787248 [label=AccumulateGrad]
	140670822787152 -> 140670822787056
	140670822787008 -> 140670822786768
	140671734488256 [label="blocks.2.0.conv_pw.weight
 (72, 24, 1, 1)" fillcolor=lightblue]
	140671734488256 -> 140670822787008
	140670822787008 [label=AccumulateGrad]
	140670822786624 -> 140670822786720
	140671734488336 [label="blocks.2.0.bn1.weight
 (72)" fillcolor=lightblue]
	140671734488336 -> 140670822786624
	140670822786624 [label=AccumulateGrad]
	140670822786864 -> 140670822786720
	140671734488416 [label="blocks.2.0.bn1.bias
 (72)" fillcolor=lightblue]
	140671734488416 -> 140670822786864
	140670822786864 [label=AccumulateGrad]
	140670822786528 -> 140670822786288
	140671734488896 [label="blocks.2.0.conv_dw.weight
 (72, 1, 5, 5)" fillcolor=lightblue]
	140671734488896 -> 140670822786528
	140670822786528 [label=AccumulateGrad]
	140670822786144 -> 140670822786240
	140671734488816 [label="blocks.2.0.bn2.weight
 (72)" fillcolor=lightblue]
	140671734488816 -> 140670822786144
	140670822786144 [label=AccumulateGrad]
	140670822786384 -> 140670822786240
	140671734488976 [label="blocks.2.0.bn2.bias
 (72)" fillcolor=lightblue]
	140671734488976 -> 140670822786384
	140670822786384 [label=AccumulateGrad]
	140670822786048 -> 140670822785952
	140670822786048 [label=SigmoidBackward0]
	140670822786480 -> 140670822786048
	140670822786480 [label=ConvolutionBackward0]
	140670822786912 -> 140670822786480
	140670822786912 [label=ReluBackward0]
	140670822787104 -> 140670822786912
	140670822787104 [label=ConvolutionBackward0]
	140670822787872 -> 140670822787104
	140670822787872 [label=MeanBackward1]
	140670822786096 -> 140670822787872
	140670822787920 -> 140670822787104
	140671734612352 [label="blocks.2.0.se.conv_reduce.weight
 (6, 72, 1, 1)" fillcolor=lightblue]
	140671734612352 -> 140670822787920
	140670822787920 [label=AccumulateGrad]
	140670822787440 -> 140670822787104
	140671734612432 [label="blocks.2.0.se.conv_reduce.bias
 (6)" fillcolor=lightblue]
	140671734612432 -> 140670822787440
	140670822787440 [label=AccumulateGrad]
	140670822786960 -> 140670822786480
	140671734612592 [label="blocks.2.0.se.conv_expand.weight
 (72, 6, 1, 1)" fillcolor=lightblue]
	140671734612592 -> 140670822786960
	140670822786960 [label=AccumulateGrad]
	140670822786192 -> 140670822786480
	140671734612672 [label="blocks.2.0.se.conv_expand.bias
 (72)" fillcolor=lightblue]
	140671734612672 -> 140670822786192
	140670822786192 [label=AccumulateGrad]
	140670822785904 -> 140670822785760
	140671734612832 [label="blocks.2.0.conv_pwl.weight
 (40, 72, 1, 1)" fillcolor=lightblue]
	140671734612832 -> 140670822785904
	140670822785904 [label=AccumulateGrad]
	140670822785712 -> 140670822775920
	140671734612912 [label="blocks.2.0.bn3.weight
 (40)" fillcolor=lightblue]
	140671734612912 -> 140670822785712
	140670822785712 [label=AccumulateGrad]
	140670822785568 -> 140670822775920
	140671734612992 [label="blocks.2.0.bn3.bias
 (40)" fillcolor=lightblue]
	140671734612992 -> 140670822785568
	140670822785568 [label=AccumulateGrad]
	140670822785664 -> 140670822785376
	140671734613472 [label="blocks.2.1.conv_pw.weight
 (120, 40, 1, 1)" fillcolor=lightblue]
	140671734613472 -> 140670822785664
	140670822785664 [label=AccumulateGrad]
	140670822785232 -> 140670822785328
	140671734613552 [label="blocks.2.1.bn1.weight
 (120)" fillcolor=lightblue]
	140671734613552 -> 140670822785232
	140670822785232 [label=AccumulateGrad]
	140670822785472 -> 140670822785328
	140671734613632 [label="blocks.2.1.bn1.bias
 (120)" fillcolor=lightblue]
	140671734613632 -> 140670822785472
	140670822785472 [label=AccumulateGrad]
	140670822785136 -> 140670822776640
	140671734614112 [label="blocks.2.1.conv_dw.weight
 (120, 1, 5, 5)" fillcolor=lightblue]
	140671734614112 -> 140670822785136
	140670822785136 [label=AccumulateGrad]
	140670822776496 -> 140670822776592
	140671734614032 [label="blocks.2.1.bn2.weight
 (120)" fillcolor=lightblue]
	140671734614032 -> 140670822776496
	140670822776496 [label=AccumulateGrad]
	140670822776736 -> 140670822776592
	140671734614192 [label="blocks.2.1.bn2.bias
 (120)" fillcolor=lightblue]
	140671734614192 -> 140670822776736
	140670822776736 [label=AccumulateGrad]
	140670822776400 -> 140670822776304
	140670822776400 [label=SigmoidBackward0]
	140670822776784 -> 140670822776400
	140670822776784 [label=ConvolutionBackward0]
	140670822785520 -> 140670822776784
	140670822785520 [label=ReluBackward0]
	140670822785808 -> 140670822785520
	140670822785808 [label=ConvolutionBackward0]
	140670822787392 -> 140670822785808
	140670822787392 [label=MeanBackward1]
	140670822776448 -> 140670822787392
	140670822786000 -> 140670822785808
	140671734614592 [label="blocks.2.1.se.conv_reduce.weight
 (10, 120, 1, 1)" fillcolor=lightblue]
	140671734614592 -> 140670822786000
	140670822786000 [label=AccumulateGrad]
	140670822786432 -> 140670822785808
	140671734614672 [label="blocks.2.1.se.conv_reduce.bias
 (10)" fillcolor=lightblue]
	140671734614672 -> 140670822786432
	140670822786432 [label=AccumulateGrad]
	140670822785616 -> 140670822776784
	140671734614832 [label="blocks.2.1.se.conv_expand.weight
 (120, 10, 1, 1)" fillcolor=lightblue]
	140671734614832 -> 140670822785616
	140670822785616 [label=AccumulateGrad]
	140670822785184 -> 140670822776784
	140671734614912 [label="blocks.2.1.se.conv_expand.bias
 (120)" fillcolor=lightblue]
	140671734614912 -> 140670822785184
	140670822785184 [label=AccumulateGrad]
	140670822776256 -> 140670822776112
	140671734615072 [label="blocks.2.1.conv_pwl.weight
 (40, 120, 1, 1)" fillcolor=lightblue]
	140671734615072 -> 140670822776256
	140670822776256 [label=AccumulateGrad]
	140670822776064 -> 140670822775968
	140671734615152 [label="blocks.2.1.bn3.weight
 (40)" fillcolor=lightblue]
	140671734615152 -> 140670822776064
	140670822776064 [label=AccumulateGrad]
	140670822776016 -> 140670822775968
	140671734615232 [label="blocks.2.1.bn3.bias
 (40)" fillcolor=lightblue]
	140671734615232 -> 140670822776016
	140670822776016 [label=AccumulateGrad]
	140670822775920 -> 140670822774384
	140670822775872 -> 140670822775584
	140671734615712 [label="blocks.2.2.conv_pw.weight
 (120, 40, 1, 1)" fillcolor=lightblue]
	140671734615712 -> 140670822775872
	140670822775872 [label=AccumulateGrad]
	140670822775440 -> 140670822775536
	140671734615792 [label="blocks.2.2.bn1.weight
 (120)" fillcolor=lightblue]
	140671734615792 -> 140670822775440
	140670822775440 [label=AccumulateGrad]
	140670822775680 -> 140670822775536
	140671734615872 [label="blocks.2.2.bn1.bias
 (120)" fillcolor=lightblue]
	140671734615872 -> 140670822775680
	140670822775680 [label=AccumulateGrad]
	140670822775344 -> 140670822775104
	140671734739328 [label="blocks.2.2.conv_dw.weight
 (120, 1, 5, 5)" fillcolor=lightblue]
	140671734739328 -> 140670822775344
	140670822775344 [label=AccumulateGrad]
	140670822774960 -> 140670822775056
	140671734739248 [label="blocks.2.2.bn2.weight
 (120)" fillcolor=lightblue]
	140671734739248 -> 140670822774960
	140670822774960 [label=AccumulateGrad]
	140670822775200 -> 140670822775056
	140671734739408 [label="blocks.2.2.bn2.bias
 (120)" fillcolor=lightblue]
	140671734739408 -> 140670822775200
	140670822775200 [label=AccumulateGrad]
	140670822774864 -> 140670822774768
	140670822774864 [label=SigmoidBackward0]
	140670822775296 -> 140670822774864
	140670822775296 [label=ConvolutionBackward0]
	140670822775728 -> 140670822775296
	140670822775728 [label=ReluBackward0]
	140670822775776 -> 140670822775728
	140670822775776 [label=ConvolutionBackward0]
	140670822776352 -> 140670822775776
	140670822776352 [label=MeanBackward1]
	140670822774912 -> 140670822776352
	140670822776544 -> 140670822775776
	140671734739808 [label="blocks.2.2.se.conv_reduce.weight
 (10, 120, 1, 1)" fillcolor=lightblue]
	140671734739808 -> 140670822776544
	140670822776544 [label=AccumulateGrad]
	140670822776208 -> 140670822775776
	140671734739888 [label="blocks.2.2.se.conv_reduce.bias
 (10)" fillcolor=lightblue]
	140671734739888 -> 140670822776208
	140670822776208 [label=AccumulateGrad]
	140670822775824 -> 140670822775296
	140671734740048 [label="blocks.2.2.se.conv_expand.weight
 (120, 10, 1, 1)" fillcolor=lightblue]
	140671734740048 -> 140670822775824
	140670822775824 [label=AccumulateGrad]
	140670822775008 -> 140670822775296
	140671734740128 [label="blocks.2.2.se.conv_expand.bias
 (120)" fillcolor=lightblue]
	140671734740128 -> 140670822775008
	140670822775008 [label=AccumulateGrad]
	140670822774720 -> 140670822774576
	140671734740288 [label="blocks.2.2.conv_pwl.weight
 (40, 120, 1, 1)" fillcolor=lightblue]
	140671734740288 -> 140670822774720
	140670822774720 [label=AccumulateGrad]
	140670822774528 -> 140670822774432
	140671734740368 [label="blocks.2.2.bn3.weight
 (40)" fillcolor=lightblue]
	140671734740368 -> 140670822774528
	140670822774528 [label=AccumulateGrad]
	140670822774480 -> 140670822774432
	140671734740448 [label="blocks.2.2.bn3.bias
 (40)" fillcolor=lightblue]
	140671734740448 -> 140670822774480
	140670822774480 [label=AccumulateGrad]
	140670822774384 -> 140670822774288
	140670822774240 -> 140670822774000
	140671734740848 [label="blocks.3.0.conv_pw.weight
 (240, 40, 1, 1)" fillcolor=lightblue]
	140671734740848 -> 140670822774240
	140670822774240 [label=AccumulateGrad]
	140670822773856 -> 140670822773952
	140671734740928 [label="blocks.3.0.bn1.weight
 (240)" fillcolor=lightblue]
	140671734740928 -> 140670822773856
	140670822773856 [label=AccumulateGrad]
	140670822774096 -> 140670822773952
	140671734741008 [label="blocks.3.0.bn1.bias
 (240)" fillcolor=lightblue]
	140671734741008 -> 140670822774096
	140670822774096 [label=AccumulateGrad]
	140670822773760 -> 140670822773520
	140671734741488 [label="blocks.3.0.conv_dw.weight
 (240, 1, 3, 3)" fillcolor=lightblue]
	140671734741488 -> 140670822773760
	140670822773760 [label=AccumulateGrad]
	140670822773376 -> 140670822773472
	140671734741408 [label="blocks.3.0.bn2.weight
 (240)" fillcolor=lightblue]
	140671734741408 -> 140670822773376
	140670822773376 [label=AccumulateGrad]
	140670822773616 -> 140670822773472
	140671734741568 [label="blocks.3.0.bn2.bias
 (240)" fillcolor=lightblue]
	140671734741568 -> 140670822773616
	140670822773616 [label=AccumulateGrad]
	140670822773280 -> 140670822773136
	140671734741968 [label="blocks.3.0.conv_pwl.weight
 (80, 240, 1, 1)" fillcolor=lightblue]
	140671734741968 -> 140670822773280
	140670822773280 [label=AccumulateGrad]
	140670822773088 -> 140670822763440
	140671734742048 [label="blocks.3.0.bn3.weight
 (80)" fillcolor=lightblue]
	140671734742048 -> 140670822773088
	140670822773088 [label=AccumulateGrad]
	140670822772944 -> 140670822763440
	140671734742128 [label="blocks.3.0.bn3.bias
 (80)" fillcolor=lightblue]
	140671734742128 -> 140670822772944
	140670822772944 [label=AccumulateGrad]
	140670822773040 -> 140670822764352
	140671734742528 [label="blocks.3.1.conv_pw.weight
 (480, 80, 1, 1)" fillcolor=lightblue]
	140671734742528 -> 140670822773040
	140670822773040 [label=AccumulateGrad]
	140670822772800 -> 140670822764448
	140671734742608 [label="blocks.3.1.bn1.weight
 (480)" fillcolor=lightblue]
	140671734742608 -> 140670822772800
	140670822772800 [label=AccumulateGrad]
	140670822772848 -> 140670822764448
	140671734742688 [label="blocks.3.1.bn1.bias
 (480)" fillcolor=lightblue]
	140671734742688 -> 140670822772848
	140670822772848 [label=AccumulateGrad]
	140670822764256 -> 140670822764016
	140671734882528 [label="blocks.3.1.conv_dw.weight
 (480, 1, 3, 3)" fillcolor=lightblue]
	140671734882528 -> 140670822764256
	140670822764256 [label=AccumulateGrad]
	140670822763872 -> 140670822763968
	140671734882448 [label="blocks.3.1.bn2.weight
 (480)" fillcolor=lightblue]
	140671734882448 -> 140670822763872
	140670822763872 [label=AccumulateGrad]
	140670822764112 -> 140670822763968
	140671734882608 [label="blocks.3.1.bn2.bias
 (480)" fillcolor=lightblue]
	140671734882608 -> 140670822764112
	140670822764112 [label=AccumulateGrad]
	140670822763776 -> 140670822763632
	140671734883008 [label="blocks.3.1.conv_pwl.weight
 (80, 480, 1, 1)" fillcolor=lightblue]
	140671734883008 -> 140670822763776
	140670822763776 [label=AccumulateGrad]
	140670822763584 -> 140670822763488
	140671734883088 [label="blocks.3.1.bn3.weight
 (80)" fillcolor=lightblue]
	140671734883088 -> 140670822763584
	140670822763584 [label=AccumulateGrad]
	140670822763536 -> 140670822763488
	140671734883168 [label="blocks.3.1.bn3.bias
 (80)" fillcolor=lightblue]
	140671734883168 -> 140670822763536
	140670822763536 [label=AccumulateGrad]
	140670822763440 -> 140670822762048
	140670822763392 -> 140670822763104
	140671734883568 [label="blocks.3.2.conv_pw.weight
 (480, 80, 1, 1)" fillcolor=lightblue]
	140671734883568 -> 140670822763392
	140670822763392 [label=AccumulateGrad]
	140670822762960 -> 140670822763056
	140671734883648 [label="blocks.3.2.bn1.weight
 (480)" fillcolor=lightblue]
	140671734883648 -> 140670822762960
	140670822762960 [label=AccumulateGrad]
	140670822763200 -> 140670822763056
	140671734883728 [label="blocks.3.2.bn1.bias
 (480)" fillcolor=lightblue]
	140671734883728 -> 140670822763200
	140670822763200 [label=AccumulateGrad]
	140670822762864 -> 140670822762624
	140671734884208 [label="blocks.3.2.conv_dw.weight
 (480, 1, 3, 3)" fillcolor=lightblue]
	140671734884208 -> 140670822762864
	140670822762864 [label=AccumulateGrad]
	140670822762480 -> 140670822762576
	140671734884128 [label="blocks.3.2.bn2.weight
 (480)" fillcolor=lightblue]
	140671734884128 -> 140670822762480
	140670822762480 [label=AccumulateGrad]
	140670822762720 -> 140670822762576
	140671734884288 [label="blocks.3.2.bn2.bias
 (480)" fillcolor=lightblue]
	140671734884288 -> 140670822762720
	140670822762720 [label=AccumulateGrad]
	140670822762384 -> 140670822762240
	140671734884688 [label="blocks.3.2.conv_pwl.weight
 (80, 480, 1, 1)" fillcolor=lightblue]
	140671734884688 -> 140670822762384
	140670822762384 [label=AccumulateGrad]
	140670822762192 -> 140670822762096
	140671734884768 [label="blocks.3.2.bn3.weight
 (80)" fillcolor=lightblue]
	140671734884768 -> 140670822762192
	140670822762192 [label=AccumulateGrad]
	140670822762144 -> 140670822762096
	140671734884848 [label="blocks.3.2.bn3.bias
 (80)" fillcolor=lightblue]
	140671734884848 -> 140670822762144
	140670822762144 [label=AccumulateGrad]
	140670822762048 -> 140670822760752
	140670822762000 -> 140670822761808
	140671734885248 [label="blocks.3.3.conv_pw.weight
 (480, 80, 1, 1)" fillcolor=lightblue]
	140671734885248 -> 140670822762000
	140670822762000 [label=AccumulateGrad]
	140670822761760 -> 140670822761712
	140671734885328 [label="blocks.3.3.bn1.weight
 (480)" fillcolor=lightblue]
	140671734885328 -> 140670822761760
	140670822761760 [label=AccumulateGrad]
	140670822761616 -> 140670822761712
	140671734885408 [label="blocks.3.3.bn1.bias
 (480)" fillcolor=lightblue]
	140671734885408 -> 140670822761616
	140670822761616 [label=AccumulateGrad]
	140670822761520 -> 140670822761376
	140671734885888 [label="blocks.3.3.conv_dw.weight
 (480, 1, 3, 3)" fillcolor=lightblue]
	140671734885888 -> 140670822761520
	140670822761520 [label=AccumulateGrad]
	140670822761328 -> 140670822761280
	140671734885808 [label="blocks.3.3.bn2.weight
 (480)" fillcolor=lightblue]
	140671734885808 -> 140670822761328
	140670822761328 [label=AccumulateGrad]
	140670822761184 -> 140670822761280
	140671734885968 [label="blocks.3.3.bn2.bias
 (480)" fillcolor=lightblue]
	140671734885968 -> 140670822761184
	140670822761184 [label=AccumulateGrad]
	140670822761088 -> 140670822760944
	140670809342016 [label="blocks.3.3.conv_pwl.weight
 (80, 480, 1, 1)" fillcolor=lightblue]
	140670809342016 -> 140670822761088
	140670822761088 [label=AccumulateGrad]
	140670822760896 -> 140670822760800
	140670809342096 [label="blocks.3.3.bn3.weight
 (80)" fillcolor=lightblue]
	140670809342096 -> 140670822760896
	140670822760896 [label=AccumulateGrad]
	140670822760848 -> 140670822760800
	140670809342176 [label="blocks.3.3.bn3.bias
 (80)" fillcolor=lightblue]
	140670809342176 -> 140670822760848
	140670822760848 [label=AccumulateGrad]
	140670822760752 -> 140670822760656
	140670822760608 -> 140670822682576
	140670809342656 [label="blocks.4.0.conv_pw.weight
 (480, 80, 1, 1)" fillcolor=lightblue]
	140670809342656 -> 140670822760608
	140670822760608 [label=AccumulateGrad]
	140670822682528 -> 140670822682480
	140670809342736 [label="blocks.4.0.bn1.weight
 (480)" fillcolor=lightblue]
	140670809342736 -> 140670822682528
	140670822682528 [label=AccumulateGrad]
	140670822682384 -> 140670822682480
	140670809342816 [label="blocks.4.0.bn1.bias
 (480)" fillcolor=lightblue]
	140670809342816 -> 140670822682384
	140670822682384 [label=AccumulateGrad]
	140670822682288 -> 140670822682144
	140670809343296 [label="blocks.4.0.conv_dw.weight
 (480, 1, 3, 3)" fillcolor=lightblue]
	140670809343296 -> 140670822682288
	140670822682288 [label=AccumulateGrad]
	140670822682096 -> 140670822682048
	140670809343216 [label="blocks.4.0.bn2.weight
 (480)" fillcolor=lightblue]
	140670809343216 -> 140670822682096
	140670822682096 [label=AccumulateGrad]
	140670822681952 -> 140670822682048
	140670809343376 [label="blocks.4.0.bn2.bias
 (480)" fillcolor=lightblue]
	140670809343376 -> 140670822681952
	140670822681952 [label=AccumulateGrad]
	140670822681856 -> 140670822681760
	140670822681856 [label=SigmoidBackward0]
	140670822682240 -> 140670822681856
	140670822682240 [label=ConvolutionBackward0]
	140670822682432 -> 140670822682240
	140670822682432 [label=ReluBackward0]
	140670822760704 -> 140670822682432
	140670822760704 [label=ConvolutionBackward0]
	140670822761424 -> 140670822760704
	140670822761424 [label=MeanBackward1]
	140670822681904 -> 140670822761424
	140670822761472 -> 140670822760704
	140670809343776 [label="blocks.4.0.se.conv_reduce.weight
 (20, 480, 1, 1)" fillcolor=lightblue]
	140670809343776 -> 140670822761472
	140670822761472 [label=AccumulateGrad]
	140670822761040 -> 140670822760704
	140670809343856 [label="blocks.4.0.se.conv_reduce.bias
 (20)" fillcolor=lightblue]
	140670809343856 -> 140670822761040
	140670822761040 [label=AccumulateGrad]
	140670822682000 -> 140670822682240
	140670809344016 [label="blocks.4.0.se.conv_expand.weight
 (480, 20, 1, 1)" fillcolor=lightblue]
	140670809344016 -> 140670822682000
	140670822682000 [label=AccumulateGrad]
	140670822760560 -> 140670822682240
	140670809344096 [label="blocks.4.0.se.conv_expand.bias
 (480)" fillcolor=lightblue]
	140670809344096 -> 140670822760560
	140670822760560 [label=AccumulateGrad]
	140670822681712 -> 140670822681568
	140670809344256 [label="blocks.4.0.conv_pwl.weight
 (112, 480, 1, 1)" fillcolor=lightblue]
	140670809344256 -> 140670822681712
	140670822681712 [label=AccumulateGrad]
	140670822681520 -> 140670822680080
	140670809344336 [label="blocks.4.0.bn3.weight
 (112)" fillcolor=lightblue]
	140670809344336 -> 140670822681520
	140670822681520 [label=AccumulateGrad]
	140670822681376 -> 140670822680080
	140670809344416 [label="blocks.4.0.bn3.bias
 (112)" fillcolor=lightblue]
	140670809344416 -> 140670822681376
	140670822681376 [label=AccumulateGrad]
	140670822681472 -> 140670822681280
	140670809344896 [label="blocks.4.1.conv_pw.weight
 (672, 112, 1, 1)" fillcolor=lightblue]
	140670809344896 -> 140670822681472
	140670822681472 [label=AccumulateGrad]
	140670822681232 -> 140670822681184
	140670809344976 [label="blocks.4.1.bn1.weight
 (672)" fillcolor=lightblue]
	140670809344976 -> 140670822681232
	140670822681232 [label=AccumulateGrad]
	140670822681088 -> 140670822681184
	140670809345056 [label="blocks.4.1.bn1.bias
 (672)" fillcolor=lightblue]
	140670809345056 -> 140670822681088
	140670822681088 [label=AccumulateGrad]
	140670822680992 -> 140670822680848
	140670809345536 [label="blocks.4.1.conv_dw.weight
 (672, 1, 3, 3)" fillcolor=lightblue]
	140670809345536 -> 140670822680992
	140670822680992 [label=AccumulateGrad]
	140670822680800 -> 140670822680752
	140670809345456 [label="blocks.4.1.bn2.weight
 (672)" fillcolor=lightblue]
	140670809345456 -> 140670822680800
	140670822680800 [label=AccumulateGrad]
	140670822680656 -> 140670822680752
	140670809345616 [label="blocks.4.1.bn2.bias
 (672)" fillcolor=lightblue]
	140670809345616 -> 140670822680656
	140670822680656 [label=AccumulateGrad]
	140670822680560 -> 140670822680464
	140670822680560 [label=SigmoidBackward0]
	140670822680944 -> 140670822680560
	140670822680944 [label=ConvolutionBackward0]
	140670822681328 -> 140670822680944
	140670822681328 [label=ReluBackward0]
	140670822681616 -> 140670822681328
	140670822681616 [label=ConvolutionBackward0]
	140670822681808 -> 140670822681616
	140670822681808 [label=MeanBackward1]
	140670822680608 -> 140670822681808
	140670822682192 -> 140670822681616
	140670809477184 [label="blocks.4.1.se.conv_reduce.weight
 (28, 672, 1, 1)" fillcolor=lightblue]
	140670809477184 -> 140670822682192
	140670822682192 [label=AccumulateGrad]
	140670822760512 -> 140670822681616
	140670809477264 [label="blocks.4.1.se.conv_reduce.bias
 (28)" fillcolor=lightblue]
	140670809477264 -> 140670822760512
	140670822760512 [label=AccumulateGrad]
	140670822681424 -> 140670822680944
	140670809477424 [label="blocks.4.1.se.conv_expand.weight
 (672, 28, 1, 1)" fillcolor=lightblue]
	140670809477424 -> 140670822681424
	140670822681424 [label=AccumulateGrad]
	140670822680704 -> 140670822680944
	140670809477504 [label="blocks.4.1.se.conv_expand.bias
 (672)" fillcolor=lightblue]
	140670809477504 -> 140670822680704
	140670822680704 [label=AccumulateGrad]
	140670822680416 -> 140670822680272
	140670809477664 [label="blocks.4.1.conv_pwl.weight
 (112, 672, 1, 1)" fillcolor=lightblue]
	140670809477664 -> 140670822680416
	140670822680416 [label=AccumulateGrad]
	140670822680224 -> 140670822680128
	140670809477744 [label="blocks.4.1.bn3.weight
 (112)" fillcolor=lightblue]
	140670809477744 -> 140670822680224
	140670822680224 [label=AccumulateGrad]
	140670822680176 -> 140670822680128
	140670809477824 [label="blocks.4.1.bn3.bias
 (112)" fillcolor=lightblue]
	140670809477824 -> 140670822680176
	140670822680176 [label=AccumulateGrad]
	140670822680080 -> 140670822679984
	140670822679936 -> 140670822679792
	140670809478304 [label="blocks.5.0.conv_pw.weight
 (672, 112, 1, 1)" fillcolor=lightblue]
	140670809478304 -> 140670822679936
	140670822679936 [label=AccumulateGrad]
	140670822679744 -> 140670822679696
	140670809478384 [label="blocks.5.0.bn1.weight
 (672)" fillcolor=lightblue]
	140670809478384 -> 140670822679744
	140670822679744 [label=AccumulateGrad]
	140670822679600 -> 140670822679696
	140670809478464 [label="blocks.5.0.bn1.bias
 (672)" fillcolor=lightblue]
	140670809478464 -> 140670822679600
	140670822679600 [label=AccumulateGrad]
	140670822679504 -> 140670822679360
	140670809478944 [label="blocks.5.0.conv_dw.weight
 (672, 1, 5, 5)" fillcolor=lightblue]
	140670809478944 -> 140670822679504
	140670822679504 [label=AccumulateGrad]
	140670822679312 -> 140670822679264
	140670809478864 [label="blocks.5.0.bn2.weight
 (672)" fillcolor=lightblue]
	140670809478864 -> 140670822679312
	140670822679312 [label=AccumulateGrad]
	140670822679168 -> 140670822679264
	140670809479024 [label="blocks.5.0.bn2.bias
 (672)" fillcolor=lightblue]
	140670809479024 -> 140670822679168
	140670822679168 [label=AccumulateGrad]
	140670822679072 -> 140670822678976
	140670822679072 [label=SigmoidBackward0]
	140670822679456 -> 140670822679072
	140670822679456 [label=ConvolutionBackward0]
	140670822679840 -> 140670822679456
	140670822679840 [label=ReluBackward0]
	140670822680032 -> 140670822679840
	140670822680032 [label=ConvolutionBackward0]
	140670822680896 -> 140670822680032
	140670822680896 [label=MeanBackward1]
	140670822679120 -> 140670822680896
	140670822681136 -> 140670822680032
	140670809479424 [label="blocks.5.0.se.conv_reduce.weight
 (28, 672, 1, 1)" fillcolor=lightblue]
	140670809479424 -> 140670822681136
	140670822681136 [label=AccumulateGrad]
	140670822680368 -> 140670822680032
	140670809479504 [label="blocks.5.0.se.conv_reduce.bias
 (28)" fillcolor=lightblue]
	140670809479504 -> 140670822680368
	140670822680368 [label=AccumulateGrad]
	140670822679888 -> 140670822679456
	140670809479664 [label="blocks.5.0.se.conv_expand.weight
 (672, 28, 1, 1)" fillcolor=lightblue]
	140670809479664 -> 140670822679888
	140670822679888 [label=AccumulateGrad]
	140670822679216 -> 140670822679456
	140670809479744 [label="blocks.5.0.se.conv_expand.bias
 (672)" fillcolor=lightblue]
	140670809479744 -> 140670822679216
	140670822679216 [label=AccumulateGrad]
	140670822678928 -> 140670822678784
	140670809479904 [label="blocks.5.0.conv_pwl.weight
 (160, 672, 1, 1)" fillcolor=lightblue]
	140670809479904 -> 140670822678928
	140670822678928 [label=AccumulateGrad]
	140670822678736 -> 140670817434352
	140670809479984 [label="blocks.5.0.bn3.weight
 (160)" fillcolor=lightblue]
	140670809479984 -> 140670822678736
	140670822678736 [label=AccumulateGrad]
	140670822678688 -> 140670817434352
	140670809480064 [label="blocks.5.0.bn3.bias
 (160)" fillcolor=lightblue]
	140670809480064 -> 140670822678688
	140670822678688 [label=AccumulateGrad]
	140670822678592 -> 140670817435552
	140670809480544 [label="blocks.5.1.conv_pw.weight
 (960, 160, 1, 1)" fillcolor=lightblue]
	140670809480544 -> 140670822678592
	140670822678592 [label=AccumulateGrad]
	140670817435504 -> 140670817435456
	140670809480624 [label="blocks.5.1.bn1.weight
 (960)" fillcolor=lightblue]
	140670809480624 -> 140670817435504
	140670817435504 [label=AccumulateGrad]
	140670817435360 -> 140670817435456
	140670809480704 [label="blocks.5.1.bn1.bias
 (960)" fillcolor=lightblue]
	140670809480704 -> 140670817435360
	140670817435360 [label=AccumulateGrad]
	140670817435264 -> 140670817435120
	140670811213888 [label="blocks.5.1.conv_dw.weight
 (960, 1, 5, 5)" fillcolor=lightblue]
	140670811213888 -> 140670817435264
	140670817435264 [label=AccumulateGrad]
	140670817435072 -> 140670817435024
	140670809481104 [label="blocks.5.1.bn2.weight
 (960)" fillcolor=lightblue]
	140670809481104 -> 140670817435072
	140670817435072 [label=AccumulateGrad]
	140670817434928 -> 140670817435024
	140670811213968 [label="blocks.5.1.bn2.bias
 (960)" fillcolor=lightblue]
	140670811213968 -> 140670817434928
	140670817434928 [label=AccumulateGrad]
	140670817434832 -> 140670817434736
	140670817434832 [label=SigmoidBackward0]
	140670817435216 -> 140670817434832
	140670817435216 [label=ConvolutionBackward0]
	140670817435408 -> 140670817435216
	140670817435408 [label=ReluBackward0]
	140670822678832 -> 140670817435408
	140670822678832 [label=ConvolutionBackward0]
	140670822679024 -> 140670822678832
	140670822679024 [label=MeanBackward1]
	140670817434880 -> 140670822679024
	140670822679408 -> 140670822678832
	140670811214368 [label="blocks.5.1.se.conv_reduce.weight
 (40, 960, 1, 1)" fillcolor=lightblue]
	140670811214368 -> 140670822679408
	140670822679408 [label=AccumulateGrad]
	140670822679648 -> 140670822678832
	140670811214448 [label="blocks.5.1.se.conv_reduce.bias
 (40)" fillcolor=lightblue]
	140670811214448 -> 140670822679648
	140670822679648 [label=AccumulateGrad]
	140670817435600 -> 140670817435216
	140670811214608 [label="blocks.5.1.se.conv_expand.weight
 (960, 40, 1, 1)" fillcolor=lightblue]
	140670811214608 -> 140670817435600
	140670817435600 [label=AccumulateGrad]
	140670817434976 -> 140670817435216
	140670811214688 [label="blocks.5.1.se.conv_expand.bias
 (960)" fillcolor=lightblue]
	140670811214688 -> 140670817434976
	140670817434976 [label=AccumulateGrad]
	140670817434688 -> 140670817434544
	140670811214848 [label="blocks.5.1.conv_pwl.weight
 (160, 960, 1, 1)" fillcolor=lightblue]
	140670811214848 -> 140670817434688
	140670817434688 [label=AccumulateGrad]
	140670817434496 -> 140670817434400
	140670811214928 [label="blocks.5.1.bn3.weight
 (160)" fillcolor=lightblue]
	140670811214928 -> 140670817434496
	140670817434496 [label=AccumulateGrad]
	140670817434448 -> 140670817434400
	140670811215008 [label="blocks.5.1.bn3.bias
 (160)" fillcolor=lightblue]
	140670811215008 -> 140670817434448
	140670817434448 [label=AccumulateGrad]
	140670817434352 -> 140670817432912
	140670817434304 -> 140670817434112
	140670811215488 [label="blocks.5.2.conv_pw.weight
 (960, 160, 1, 1)" fillcolor=lightblue]
	140670811215488 -> 140670817434304
	140670817434304 [label=AccumulateGrad]
	140670817434064 -> 140670817434016
	140670811215568 [label="blocks.5.2.bn1.weight
 (960)" fillcolor=lightblue]
	140670811215568 -> 140670817434064
	140670817434064 [label=AccumulateGrad]
	140670817433920 -> 140670817434016
	140670811215648 [label="blocks.5.2.bn1.bias
 (960)" fillcolor=lightblue]
	140670811215648 -> 140670817433920
	140670817433920 [label=AccumulateGrad]
	140670817433824 -> 140670817433680
	140670811216128 [label="blocks.5.2.conv_dw.weight
 (960, 1, 5, 5)" fillcolor=lightblue]
	140670811216128 -> 140670817433824
	140670817433824 [label=AccumulateGrad]
	140670817433632 -> 140670817433584
	140670811216048 [label="blocks.5.2.bn2.weight
 (960)" fillcolor=lightblue]
	140670811216048 -> 140670817433632
	140670817433632 [label=AccumulateGrad]
	140670817433488 -> 140670817433584
	140670811216208 [label="blocks.5.2.bn2.bias
 (960)" fillcolor=lightblue]
	140670811216208 -> 140670817433488
	140670817433488 [label=AccumulateGrad]
	140670817433392 -> 140670817433296
	140670817433392 [label=SigmoidBackward0]
	140670817433776 -> 140670817433392
	140670817433776 [label=ConvolutionBackward0]
	140670817434160 -> 140670817433776
	140670817434160 [label=ReluBackward0]
	140670817434592 -> 140670817434160
	140670817434592 [label=ConvolutionBackward0]
	140670817434784 -> 140670817434592
	140670817434784 [label=MeanBackward1]
	140670817433440 -> 140670817434784
	140670817435168 -> 140670817434592
	140670811216608 [label="blocks.5.2.se.conv_reduce.weight
 (40, 960, 1, 1)" fillcolor=lightblue]
	140670811216608 -> 140670817435168
	140670817435168 [label=AccumulateGrad]
	140670822678640 -> 140670817434592
	140670811216688 [label="blocks.5.2.se.conv_reduce.bias
 (40)" fillcolor=lightblue]
	140670811216688 -> 140670822678640
	140670822678640 [label=AccumulateGrad]
	140670817434256 -> 140670817433776
	140670811216848 [label="blocks.5.2.se.conv_expand.weight
 (960, 40, 1, 1)" fillcolor=lightblue]
	140670811216848 -> 140670817434256
	140670817434256 [label=AccumulateGrad]
	140670817433536 -> 140670817433776
	140670811216928 [label="blocks.5.2.se.conv_expand.bias
 (960)" fillcolor=lightblue]
	140670811216928 -> 140670817433536
	140670817433536 [label=AccumulateGrad]
	140670817433248 -> 140670817433104
	140670811217088 [label="blocks.5.2.conv_pwl.weight
 (160, 960, 1, 1)" fillcolor=lightblue]
	140670811217088 -> 140670817433248
	140670817433248 [label=AccumulateGrad]
	140670817433056 -> 140670817432960
	140670811217168 [label="blocks.5.2.bn3.weight
 (160)" fillcolor=lightblue]
	140670811217168 -> 140670817433056
	140670817433056 [label=AccumulateGrad]
	140670817433008 -> 140670817432960
	140670811217248 [label="blocks.5.2.bn3.bias
 (160)" fillcolor=lightblue]
	140670811217248 -> 140670817433008
	140670817433008 [label=AccumulateGrad]
	140670817432912 -> 140670817432816
	140670817432768 -> 140670817432624
	140670811217648 [label="blocks.6.0.conv_pw.weight
 (960, 160, 1, 1)" fillcolor=lightblue]
	140670811217648 -> 140670817432768
	140670817432768 [label=AccumulateGrad]
	140670817432576 -> 140670817432528
	140670811217728 [label="blocks.6.0.bn1.weight
 (960)" fillcolor=lightblue]
	140670811217728 -> 140670817432576
	140670817432576 [label=AccumulateGrad]
	140670817432432 -> 140670817432528
	140670811217808 [label="blocks.6.0.bn1.bias
 (960)" fillcolor=lightblue]
	140670811217808 -> 140670817432432
	140670817432432 [label=AccumulateGrad]
	140670817432336 -> 140670817431616
	140670811333072 [label="blocks.6.0.conv_dw.weight
 (960, 1, 3, 3)" fillcolor=lightblue]
	140670811333072 -> 140670817432336
	140670817432336 [label=AccumulateGrad]
	140670817432144 -> 140670817431664
	140670811332992 [label="blocks.6.0.bn2.weight
 (960)" fillcolor=lightblue]
	140670811332992 -> 140670817432144
	140670817432144 [label=AccumulateGrad]
	140670817431952 -> 140670817431664
	140670811333152 [label="blocks.6.0.bn2.bias
 (960)" fillcolor=lightblue]
	140670811333152 -> 140670817431952
	140670817431952 [label=AccumulateGrad]
	140670817432000 -> 140670817431712
	140670811333552 [label="blocks.6.0.conv_pwl.weight
 (320, 960, 1, 1)" fillcolor=lightblue]
	140670811333552 -> 140670817432000
	140670817432000 [label=AccumulateGrad]
	140670817431808 -> 140670811370544
	140670811333632 [label="blocks.6.0.bn3.weight
 (320)" fillcolor=lightblue]
	140670811333632 -> 140670817431808
	140670817431808 [label=AccumulateGrad]
	140670817431760 -> 140670811370544
	140670811333712 [label="blocks.6.0.bn3.bias
 (320)" fillcolor=lightblue]
	140670811333712 -> 140670817431760
	140670817431760 [label=AccumulateGrad]
	140670811370784 -> 140670811373088
	140670811334032 [label="conv_head.weight
 (1280, 320, 1, 1)" fillcolor=lightblue]
	140670811334032 -> 140670811370784
	140670811370784 [label=AccumulateGrad]
	140670811372656 -> 140670811370736
	140670811334112 [label="bn2.weight
 (1280)" fillcolor=lightblue]
	140670811334112 -> 140670811372656
	140670811372656 [label=AccumulateGrad]
	140670811371648 -> 140670811370736
	140670811334192 [label="bn2.bias
 (1280)" fillcolor=lightblue]
	140670811334192 -> 140670811371648
	140670811371648 [label=AccumulateGrad]
	140670811370832 -> 140670811373520
	140670811370832 [label=TBackward0]
	140670811371840 -> 140670811370832
	140670811334512 [label="classifier.weight
 (1000, 1280)" fillcolor=lightblue]
	140670811334512 -> 140670811371840
	140670811371840 [label=AccumulateGrad]
	140670811373520 -> 140670817532320
}
