digraph {
	graph [size="191.1,191.1"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	140671366002384 [label="
 (1, 1000)" fillcolor=darkolivegreen1]
	140671366131088 [label=DivBackward0]
	140671366130704 -> 140671366131088
	140671366130704 [label=AddBackward0]
	140671366130944 -> 140671366130704
	140671366130944 [label=AddmmBackward0]
	140671366130656 -> 140671366130944
	140671360648912 [label="head.bias
 (1000)" fillcolor=lightblue]
	140671360648912 -> 140671366130656
	140671366130656 [label=AccumulateGrad]
	140671366130464 -> 140671366130944
	140671366130464 [label=MeanBackward1]
	140671366130416 -> 140671366130464
	140671366130416 [label=NativeLayerNormBackward0]
	140671366130032 -> 140671366130416
	140671366130032 [label=AddBackward0]
	140671366129648 -> 140671366130032
	140671366129648 [label=AddBackward0]
	140671366129504 -> 140671366129648
	140671366129504 [label=TransposeBackward0]
	140671366129264 -> 140671366129504
	140671366129264 [label=ReshapeAliasBackward0]
	140671366129072 -> 140671366129264
	140671366129072 [label=AddBackward0]
	140671366128880 -> 140671366129072
	140671366128880 [label=AddBackward0]
	140671366128544 -> 140671366128880
	140671366128544 [label=AddBackward0]
	140671366128400 -> 140671366128544
	140671366128400 [label=AddBackward0]
	140671366128064 -> 140671366128400
	140671366128064 [label=AddBackward0]
	140671366127920 -> 140671366128064
	140671366127920 [label=AddBackward0]
	140671366127728 -> 140671366127920
	140671366127728 [label=NativeBatchNormBackward0]
	140671366102704 -> 140671366127728
	140671366102704 [label=ConvolutionBackward0]
	140671366102320 -> 140671366102704
	140671366102320 [label=AddBackward0]
	140671366102128 -> 140671366102320
	140671366102128 [label=AddBackward0]
	140671366101792 -> 140671366102128
	140671366101792 [label=AddBackward0]
	140671366101552 -> 140671366101792
	140671366101552 [label=AddBackward0]
	140671366101216 -> 140671366101552
	140671366101216 [label=AddBackward0]
	140671366101072 -> 140671366101216
	140671366101072 [label=AddBackward0]
	140671366100736 -> 140671366101072
	140671366100736 [label=AddBackward0]
	140671366100592 -> 140671366100736
	140671366100592 [label=AddBackward0]
	140671366100256 -> 140671366100592
	140671366100256 [label=AddBackward0]
	140671366100016 -> 140671366100256
	140671366100016 [label=AddBackward0]
	140671366099680 -> 140671366100016
	140671366099680 [label=AddBackward0]
	140671366099536 -> 140671366099680
	140671366099536 [label=AddBackward0]
	140671366099296 -> 140671366099536
	140671366099296 [label=NativeBatchNormBackward0]
	140671366099056 -> 140671366099296
	140671366099056 [label=ConvolutionBackward0]
	140671366078128 -> 140671366099056
	140671366078128 [label=AddBackward0]
	140671366077840 -> 140671366078128
	140671366077840 [label=AddBackward0]
	140671366077600 -> 140671366077840
	140671366077600 [label=AddBackward0]
	140671366077360 -> 140671366077600
	140671366077360 [label=AddBackward0]
	140671366077024 -> 140671366077360
	140671366077024 [label=NativeBatchNormBackward0]
	140671366076784 -> 140671366077024
	140671366076784 [label=ConvolutionBackward0]
	140671366076496 -> 140671366076784
	140671366076496 [label=AddBackward0]
	140671366076208 -> 140671366076496
	140671366076208 [label=AddBackward0]
	140671366075872 -> 140671366076208
	140671366075872 [label=AddBackward0]
	140671366075632 -> 140671366075872
	140671366075632 [label=AddBackward0]
	140671366075296 -> 140671366075632
	140671366075296 [label=AddBackward0]
	140671366075152 -> 140671366075296
	140671366075152 [label=AddBackward0]
	140671366074912 -> 140671366075152
	140671366074912 [label=ReluBackward0]
	140671366074672 -> 140671366074912
	140671366074672 [label=NativeBatchNormBackward0]
	140671366074480 -> 140671366074672
	140671366074480 [label=ConvolutionBackward0]
	140671366053552 -> 140671366074480
	140671366053552 [label=ReluBackward0]
	140671366053264 -> 140671366053552
	140671366053264 [label=NativeBatchNormBackward0]
	140671366053072 -> 140671366053264
	140671366053072 [label=ConvolutionBackward0]
	140671366052784 -> 140671366053072
	140671352553936 [label="stem.conv1.weight
 (24, 3, 3, 3)" fillcolor=lightblue]
	140671352553936 -> 140671366052784
	140671366052784 [label=AccumulateGrad]
	140671366052832 -> 140671366053072
	140671352554016 [label="stem.conv1.bias
 (24)" fillcolor=lightblue]
	140671352554016 -> 140671366052832
	140671366052832 [label=AccumulateGrad]
	140671366053120 -> 140671366053264
	140671352554096 [label="stem.norm1.weight
 (24)" fillcolor=lightblue]
	140671352554096 -> 140671366053120
	140671366053120 [label=AccumulateGrad]
	140671366053456 -> 140671366053264
	140671352554176 [label="stem.norm1.bias
 (24)" fillcolor=lightblue]
	140671352554176 -> 140671366053456
	140671366053456 [label=AccumulateGrad]
	140671366053600 -> 140671366074480
	140671352554656 [label="stem.conv2.weight
 (48, 24, 3, 3)" fillcolor=lightblue]
	140671352554656 -> 140671366053600
	140671366053600 [label=AccumulateGrad]
	140671366053744 -> 140671366074480
	140671352554736 [label="stem.conv2.bias
 (48)" fillcolor=lightblue]
	140671352554736 -> 140671366053744
	140671366053744 [label=AccumulateGrad]
	140671366074528 -> 140671366074672
	140671352554816 [label="stem.norm2.weight
 (48)" fillcolor=lightblue]
	140671352554816 -> 140671366074528
	140671366074528 [label=AccumulateGrad]
	140671366074864 -> 140671366074672
	140671352554896 [label="stem.norm2.bias
 (48)" fillcolor=lightblue]
	140671352554896 -> 140671366074864
	140671366074864 [label=AccumulateGrad]
	140671366075056 -> 140671366075152
	140671366075056 [label=MulBackward0]
	140671366074720 -> 140671366075056
	140671366074720 [label=SubBackward0]
	140671366052976 -> 140671366074720
	140671366052976 [label=AvgPool2DBackward0]
	140671366074912 -> 140671366052976
	140671366074912 -> 140671366074720
	140671366053504 -> 140671366075056
	140671366053504 [label=ViewBackward0]
	140671366052640 -> 140671366053504
	140671352555136 [label="stages.0.blocks.0.ls1.gamma
 (48)" fillcolor=lightblue]
	140671352555136 -> 140671366052640
	140671366052640 [label=AccumulateGrad]
	140671366075200 -> 140671366075296
	140671366075200 [label=MulBackward0]
	140671366075104 -> 140671366075200
	140671366075104 [label=NativeBatchNormBackward0]
	140671366053024 -> 140671366075104
	140671366053024 [label=ConvolutionBackward0]
	140671366052064 -> 140671366053024
	140671366052064 [label=GeluBackward0]
	140671366051872 -> 140671366052064
	140671366051872 [label=NativeBatchNormBackward0]
	140671366051680 -> 140671366051872
	140671366051680 [label=ConvolutionBackward0]
	140671366075152 -> 140671366051680
	140671366051296 -> 140671366051680
	140671352555376 [label="stages.0.blocks.0.mlp.fc1.weight
 (192, 48, 1, 1)" fillcolor=lightblue]
	140671352555376 -> 140671366051296
	140671366051296 [label=AccumulateGrad]
	140671366051440 -> 140671366051680
	140671352555536 [label="stages.0.blocks.0.mlp.fc1.bias
 (192)" fillcolor=lightblue]
	140671352555536 -> 140671366051440
	140671366051440 [label=AccumulateGrad]
	140671366051824 -> 140671366051872
	140671352555616 [label="stages.0.blocks.0.mlp.norm1.weight
 (192)" fillcolor=lightblue]
	140671352555616 -> 140671366051824
	140671366051824 [label=AccumulateGrad]
	140671366051968 -> 140671366051872
	140671352555696 [label="stages.0.blocks.0.mlp.norm1.bias
 (192)" fillcolor=lightblue]
	140671352555696 -> 140671366051968
	140671366051968 [label=AccumulateGrad]
	140671366052208 -> 140671366053024
	140671352556096 [label="stages.0.blocks.0.mlp.fc2.weight
 (48, 192, 1, 1)" fillcolor=lightblue]
	140671352556096 -> 140671366052208
	140671366052208 [label=AccumulateGrad]
	140671366052256 -> 140671366053024
	140671352556176 [label="stages.0.blocks.0.mlp.fc2.bias
 (48)" fillcolor=lightblue]
	140671352556176 -> 140671366052256
	140671366052256 [label=AccumulateGrad]
	140671366053312 -> 140671366075104
	140671352556256 [label="stages.0.blocks.0.mlp.norm2.weight
 (48)" fillcolor=lightblue]
	140671352556256 -> 140671366053312
	140671366053312 [label=AccumulateGrad]
	140671366052448 -> 140671366075104
	140671352556336 [label="stages.0.blocks.0.mlp.norm2.bias
 (48)" fillcolor=lightblue]
	140671352556336 -> 140671366052448
	140671366052448 [label=AccumulateGrad]
	140671366052592 -> 140671366075200
	140671366052592 [label=ViewBackward0]
	140671366051920 -> 140671366052592
	140671352556576 [label="stages.0.blocks.0.ls2.gamma
 (48)" fillcolor=lightblue]
	140671352556576 -> 140671366051920
	140671366051920 [label=AccumulateGrad]
	140671366075440 -> 140671366075632
	140671366075440 [label=MulBackward0]
	140671366075248 -> 140671366075440
	140671366075248 [label=SubBackward0]
	140671366052400 -> 140671366075248
	140671366052400 [label=AvgPool2DBackward0]
	140671366075296 -> 140671366052400
	140671366075296 -> 140671366075248
	140671366052016 -> 140671366075440
	140671366052016 [label=ViewBackward0]
	140671366051248 -> 140671366052016
	140671352556656 [label="stages.0.blocks.1.ls1.gamma
 (48)" fillcolor=lightblue]
	140671352556656 -> 140671366051248
	140671366051248 [label=AccumulateGrad]
	140671366075680 -> 140671366075872
	140671366075680 [label=MulBackward0]
	140671366075488 -> 140671366075680
	140671366075488 [label=NativeBatchNormBackward0]
	140671366051632 -> 140671366075488
	140671366051632 [label=ConvolutionBackward0]
	140671366050768 -> 140671366051632
	140671366050768 [label=GeluBackward0]
	140671366050384 -> 140671366050768
	140671366050384 [label=NativeBatchNormBackward0]
	140671366050288 -> 140671366050384
	140671366050288 [label=ConvolutionBackward0]
	140671366075632 -> 140671366050288
	140671366049904 -> 140671366050288
	140671352556896 [label="stages.0.blocks.1.mlp.fc1.weight
 (192, 48, 1, 1)" fillcolor=lightblue]
	140671352556896 -> 140671366049904
	140671366049904 [label=AccumulateGrad]
	140671366049952 -> 140671366050288
	140671352556976 [label="stages.0.blocks.1.mlp.fc1.bias
 (192)" fillcolor=lightblue]
	140671352556976 -> 140671366049952
	140671366049952 [label=AccumulateGrad]
	140671366050336 -> 140671366050384
	140671352557056 [label="stages.0.blocks.1.mlp.norm1.weight
 (192)" fillcolor=lightblue]
	140671352557056 -> 140671366050336
	140671366050336 [label=AccumulateGrad]
	140671366050576 -> 140671366050384
	140671352557136 [label="stages.0.blocks.1.mlp.norm1.bias
 (192)" fillcolor=lightblue]
	140671352557136 -> 140671366050576
	140671366050576 [label=AccumulateGrad]
	140671366050816 -> 140671366051632
	140671352676416 [label="stages.0.blocks.1.mlp.fc2.weight
 (48, 192, 1, 1)" fillcolor=lightblue]
	140671352676416 -> 140671366050816
	140671366050816 [label=AccumulateGrad]
	140671366050864 -> 140671366051632
	140671352676496 [label="stages.0.blocks.1.mlp.fc2.bias
 (48)" fillcolor=lightblue]
	140671352676496 -> 140671366050864
	140671366050864 [label=AccumulateGrad]
	140671366051488 -> 140671366075488
	140671352676576 [label="stages.0.blocks.1.mlp.norm2.weight
 (48)" fillcolor=lightblue]
	140671352676576 -> 140671366051488
	140671366051488 [label=AccumulateGrad]
	140671366051056 -> 140671366075488
	140671352676656 [label="stages.0.blocks.1.mlp.norm2.bias
 (48)" fillcolor=lightblue]
	140671352676656 -> 140671366051056
	140671366051056 [label=AccumulateGrad]
	140671366051104 -> 140671366075680
	140671366051104 [label=ViewBackward0]
	140671366050432 -> 140671366051104
	140671352676896 [label="stages.0.blocks.1.ls2.gamma
 (48)" fillcolor=lightblue]
	140671352676896 -> 140671366050432
	140671366050432 [label=AccumulateGrad]
	140671366076016 -> 140671366076208
	140671366076016 [label=MulBackward0]
	140671366075824 -> 140671366076016
	140671366075824 [label=SubBackward0]
	140671366050912 -> 140671366075824
	140671366050912 [label=AvgPool2DBackward0]
	140671366075872 -> 140671366050912
	140671366075872 -> 140671366075824
	140671366050624 -> 140671366076016
	140671366050624 [label=ViewBackward0]
	140671366050096 -> 140671366050624
	140671352676976 [label="stages.0.blocks.2.ls1.gamma
 (48)" fillcolor=lightblue]
	140671352676976 -> 140671366050096
	140671366050096 [label=AccumulateGrad]
	140671366076256 -> 140671366076496
	140671366076256 [label=MulBackward0]
	140671366050144 -> 140671366076256
	140671366050144 [label=NativeBatchNormBackward0]
	140671366020976 -> 140671366050144
	140671366020976 [label=ConvolutionBackward0]
	140671366020544 -> 140671366020976
	140671366020544 [label=GeluBackward0]
	140671366020256 -> 140671366020544
	140671366020256 [label=NativeBatchNormBackward0]
	140671366020064 -> 140671366020256
	140671366020064 [label=ConvolutionBackward0]
	140671366076208 -> 140671366020064
	140671366019680 -> 140671366020064
	140671352677216 [label="stages.0.blocks.2.mlp.fc1.weight
 (192, 48, 1, 1)" fillcolor=lightblue]
	140671352677216 -> 140671366019680
	140671366019680 [label=AccumulateGrad]
	140671366019824 -> 140671366020064
	140671352677296 [label="stages.0.blocks.2.mlp.fc1.bias
 (192)" fillcolor=lightblue]
	140671352677296 -> 140671366019824
	140671366019824 [label=AccumulateGrad]
	140671366020208 -> 140671366020256
	140671352677376 [label="stages.0.blocks.2.mlp.norm1.weight
 (192)" fillcolor=lightblue]
	140671352677376 -> 140671366020208
	140671366020208 [label=AccumulateGrad]
	140671366020448 -> 140671366020256
	140671352677456 [label="stages.0.blocks.2.mlp.norm1.bias
 (192)" fillcolor=lightblue]
	140671352677456 -> 140671366020448
	140671366020448 [label=AccumulateGrad]
	140671366020592 -> 140671366020976
	140671352677856 [label="stages.0.blocks.2.mlp.fc2.weight
 (48, 192, 1, 1)" fillcolor=lightblue]
	140671352677856 -> 140671366020592
	140671366020592 [label=AccumulateGrad]
	140671366020640 -> 140671366020976
	140671352677936 [label="stages.0.blocks.2.mlp.fc2.bias
 (48)" fillcolor=lightblue]
	140671352677936 -> 140671366020640
	140671366020640 [label=AccumulateGrad]
	140671366021024 -> 140671366050144
	140671352678016 [label="stages.0.blocks.2.mlp.norm2.weight
 (48)" fillcolor=lightblue]
	140671352678016 -> 140671366021024
	140671366021024 [label=AccumulateGrad]
	140671366020832 -> 140671366050144
	140671352678096 [label="stages.0.blocks.2.mlp.norm2.bias
 (48)" fillcolor=lightblue]
	140671352678096 -> 140671366020832
	140671366020832 [label=AccumulateGrad]
	140671366053792 -> 140671366076256
	140671366053792 [label=ViewBackward0]
	140671366020400 -> 140671366053792
	140671352678336 [label="stages.0.blocks.2.ls2.gamma
 (48)" fillcolor=lightblue]
	140671352678336 -> 140671366020400
	140671366020400 [label=AccumulateGrad]
	140671366076544 -> 140671366076784
	140671352678496 [label="stages.1.downsample.conv.weight
 (96, 48, 3, 3)" fillcolor=lightblue]
	140671352678496 -> 140671366076544
	140671366076544 [label=AccumulateGrad]
	140671366076592 -> 140671366076784
	140671352678576 [label="stages.1.downsample.conv.bias
 (96)" fillcolor=lightblue]
	140671352678576 -> 140671366076592
	140671366076592 [label=AccumulateGrad]
	140671366076832 -> 140671366077024
	140671352678656 [label="stages.1.downsample.norm.weight
 (96)" fillcolor=lightblue]
	140671352678656 -> 140671366076832
	140671366076832 [label=AccumulateGrad]
	140671366076976 -> 140671366077024
	140671352678736 [label="stages.1.downsample.norm.bias
 (96)" fillcolor=lightblue]
	140671352678736 -> 140671366076976
	140671366076976 [label=AccumulateGrad]
	140671366077168 -> 140671366077360
	140671366077168 [label=MulBackward0]
	140671366076304 -> 140671366077168
	140671366076304 [label=SubBackward0]
	140671366076064 -> 140671366076304
	140671366076064 [label=AvgPool2DBackward0]
	140671366077024 -> 140671366076064
	140671366077024 -> 140671366076304
	140671366076352 -> 140671366077168
	140671366076352 [label=ViewBackward0]
	140671366020016 -> 140671366076352
	140671352679056 [label="stages.1.blocks.0.ls1.gamma
 (96)" fillcolor=lightblue]
	140671352679056 -> 140671366020016
	140671366020016 [label=AccumulateGrad]
	140671366077408 -> 140671366077600
	140671366077408 [label=MulBackward0]
	140671366076640 -> 140671366077408
	140671366076640 [label=NativeBatchNormBackward0]
	140671366019872 -> 140671366076640
	140671366019872 [label=ConvolutionBackward0]
	140671366019344 -> 140671366019872
	140671366019344 [label=GeluBackward0]
	140671366018960 -> 140671366019344
	140671366018960 [label=NativeBatchNormBackward0]
	140671366018864 -> 140671366018960
	140671366018864 [label=ConvolutionBackward0]
	140671366077360 -> 140671366018864
	140671366018480 -> 140671366018864
	140671352679296 [label="stages.1.blocks.0.mlp.fc1.weight
 (384, 96, 1, 1)" fillcolor=lightblue]
	140671352679296 -> 140671366018480
	140671366018480 [label=AccumulateGrad]
	140671366018528 -> 140671366018864
	140671352679376 [label="stages.1.blocks.0.mlp.fc1.bias
 (384)" fillcolor=lightblue]
	140671352679376 -> 140671366018528
	140671366018528 [label=AccumulateGrad]
	140671366018912 -> 140671366018960
	140671352679456 [label="stages.1.blocks.0.mlp.norm1.weight
 (384)" fillcolor=lightblue]
	140671352679456 -> 140671366018912
	140671366018912 [label=AccumulateGrad]
	140671366019152 -> 140671366018960
	140671352679536 [label="stages.1.blocks.0.mlp.norm1.bias
 (384)" fillcolor=lightblue]
	140671352679536 -> 140671366019152
	140671366019152 [label=AccumulateGrad]
	140671366019392 -> 140671366019872
	140671352679936 [label="stages.1.blocks.0.mlp.fc2.weight
 (96, 384, 1, 1)" fillcolor=lightblue]
	140671352679936 -> 140671366019392
	140671366019392 [label=AccumulateGrad]
	140671366019440 -> 140671366019872
	140671352680016 [label="stages.1.blocks.0.mlp.fc2.bias
 (96)" fillcolor=lightblue]
	140671352680016 -> 140671366019440
	140671366019440 [label=AccumulateGrad]
	140671366020496 -> 140671366076640
	140671352680096 [label="stages.1.blocks.0.mlp.norm2.weight
 (96)" fillcolor=lightblue]
	140671352680096 -> 140671366020496
	140671366020496 [label=AccumulateGrad]
	140671366019632 -> 140671366076640
	140671352680176 [label="stages.1.blocks.0.mlp.norm2.bias
 (96)" fillcolor=lightblue]
	140671352680176 -> 140671366019632
	140671366019632 [label=AccumulateGrad]
	140671366077216 -> 140671366077408
	140671366077216 [label=ViewBackward0]
	140671366019008 -> 140671366077216
	140671352803392 [label="stages.1.blocks.0.ls2.gamma
 (96)" fillcolor=lightblue]
	140671352803392 -> 140671366019008
	140671366019008 [label=AccumulateGrad]
	140671366077648 -> 140671366077840
	140671366077648 [label=MulBackward0]
	140671366077552 -> 140671366077648
	140671366077552 [label=SubBackward0]
	140671366019488 -> 140671366077552
	140671366019488 [label=AvgPool2DBackward0]
	140671366077600 -> 140671366019488
	140671366077600 -> 140671366077552
	140671366019200 -> 140671366077648
	140671366019200 [label=ViewBackward0]
	140671366018336 -> 140671366019200
	140671352803472 [label="stages.1.blocks.1.ls1.gamma
 (96)" fillcolor=lightblue]
	140671352803472 -> 140671366018336
	140671366018336 [label=AccumulateGrad]
	140671366077888 -> 140671366078128
	140671366077888 [label=MulBackward0]
	140671366077696 -> 140671366077888
	140671366077696 [label=NativeBatchNormBackward0]
	140671366018720 -> 140671366077696
	140671366018720 [label=ConvolutionBackward0]
	140671366017856 -> 140671366018720
	140671366017856 [label=GeluBackward0]
	140671366017568 -> 140671366017856
	140671366017568 [label=NativeBatchNormBackward0]
	140671366017376 -> 140671366017568
	140671366017376 [label=ConvolutionBackward0]
	140671366077840 -> 140671366017376
	140671366017136 -> 140671366017376
	140671352803712 [label="stages.1.blocks.1.mlp.fc1.weight
 (384, 96, 1, 1)" fillcolor=lightblue]
	140671352803712 -> 140671366017136
	140671366017136 [label=AccumulateGrad]
	140671366017184 -> 140671366017376
	140671352803792 [label="stages.1.blocks.1.mlp.fc1.bias
 (384)" fillcolor=lightblue]
	140671352803792 -> 140671366017184
	140671366017184 [label=AccumulateGrad]
	140671366017520 -> 140671366017568
	140671352803872 [label="stages.1.blocks.1.mlp.norm1.weight
 (384)" fillcolor=lightblue]
	140671352803872 -> 140671366017520
	140671366017520 [label=AccumulateGrad]
	140671366017760 -> 140671366017568
	140671352803952 [label="stages.1.blocks.1.mlp.norm1.bias
 (384)" fillcolor=lightblue]
	140671352803952 -> 140671366017760
	140671366017760 [label=AccumulateGrad]
	140671366017904 -> 140671366018720
	140671352804352 [label="stages.1.blocks.1.mlp.fc2.weight
 (96, 384, 1, 1)" fillcolor=lightblue]
	140671352804352 -> 140671366017904
	140671366017904 [label=AccumulateGrad]
	140671366017952 -> 140671366018720
	140671352804432 [label="stages.1.blocks.1.mlp.fc2.bias
 (96)" fillcolor=lightblue]
	140671352804432 -> 140671366017952
	140671366017952 [label=AccumulateGrad]
	140671366018672 -> 140671366077696
	140671352804512 [label="stages.1.blocks.1.mlp.norm2.weight
 (96)" fillcolor=lightblue]
	140671352804512 -> 140671366018672
	140671366018672 [label=AccumulateGrad]
	140671366018144 -> 140671366077696
	140671352804592 [label="stages.1.blocks.1.mlp.norm2.bias
 (96)" fillcolor=lightblue]
	140671352804592 -> 140671366018144
	140671366018144 [label=AccumulateGrad]
	140671366018288 -> 140671366077888
	140671366018288 [label=ViewBackward0]
	140671366017712 -> 140671366018288
	140671352804832 [label="stages.1.blocks.1.ls2.gamma
 (96)" fillcolor=lightblue]
	140671352804832 -> 140671366017712
	140671366017712 [label=AccumulateGrad]
	140671366078176 -> 140671366099056
	140671352804992 [label="stages.2.downsample.conv.weight
 (224, 96, 3, 3)" fillcolor=lightblue]
	140671352804992 -> 140671366078176
	140671366078176 [label=AccumulateGrad]
	140671366078320 -> 140671366099056
	140671352805072 [label="stages.2.downsample.conv.bias
 (224)" fillcolor=lightblue]
	140671352805072 -> 140671366078320
	140671366078320 [label=AccumulateGrad]
	140671366099104 -> 140671366099296
	140671352805152 [label="stages.2.downsample.norm.weight
 (224)" fillcolor=lightblue]
	140671352805152 -> 140671366099104
	140671366099104 [label=AccumulateGrad]
	140671366099248 -> 140671366099296
	140671352805232 [label="stages.2.downsample.norm.bias
 (224)" fillcolor=lightblue]
	140671352805232 -> 140671366099248
	140671366099248 [label=AccumulateGrad]
	140671366099440 -> 140671366099536
	140671366099440 [label=MulBackward0]
	140671366077936 -> 140671366099440
	140671366077936 [label=SubBackward0]
	140671366017328 -> 140671366077936
	140671366017328 [label=AvgPool2DBackward0]
	140671366099296 -> 140671366017328
	140671366099296 -> 140671366077936
	140671366077984 -> 140671366099440
	140671366077984 [label=ViewBackward0]
	140671366020784 -> 140671366077984
	140671352805552 [label="stages.2.blocks.0.ls1.gamma
 (224)" fillcolor=lightblue]
	140671352805552 -> 140671366020784
	140671366020784 [label=AccumulateGrad]
	140671366099584 -> 140671366099680
	140671366099584 [label=MulBackward0]
	140671366078368 -> 140671366099584
	140671366078368 [label=NativeBatchNormBackward0]
	140671366017808 -> 140671366078368
	140671366017808 [label=ConvolutionBackward0]
	140671365995968 -> 140671366017808
	140671365995968 [label=GeluBackward0]
	140671365995680 -> 140671365995968
	140671365995680 [label=NativeBatchNormBackward0]
	140671365995488 -> 140671365995680
	140671365995488 [label=ConvolutionBackward0]
	140671366099536 -> 140671365995488
	140671365995104 -> 140671365995488
	140671352805792 [label="stages.2.blocks.0.mlp.fc1.weight
 (896, 224, 1, 1)" fillcolor=lightblue]
	140671352805792 -> 140671365995104
	140671365995104 [label=AccumulateGrad]
	140671365995248 -> 140671365995488
	140671352805872 [label="stages.2.blocks.0.mlp.fc1.bias
 (896)" fillcolor=lightblue]
	140671352805872 -> 140671365995248
	140671365995248 [label=AccumulateGrad]
	140671365995632 -> 140671365995680
	140671352805952 [label="stages.2.blocks.0.mlp.norm1.weight
 (896)" fillcolor=lightblue]
	140671352805952 -> 140671365995632
	140671365995632 [label=AccumulateGrad]
	140671365995776 -> 140671365995680
	140671352806032 [label="stages.2.blocks.0.mlp.norm1.bias
 (896)" fillcolor=lightblue]
	140671352806032 -> 140671365995776
	140671365995776 [label=AccumulateGrad]
	140671365996112 -> 140671366017808
	140671352806432 [label="stages.2.blocks.0.mlp.fc2.weight
 (224, 896, 1, 1)" fillcolor=lightblue]
	140671352806432 -> 140671365996112
	140671365996112 [label=AccumulateGrad]
	140671365996160 -> 140671366017808
	140671352806512 [label="stages.2.blocks.0.mlp.fc2.bias
 (224)" fillcolor=lightblue]
	140671352806512 -> 140671365996160
	140671365996160 [label=AccumulateGrad]
	140671365996400 -> 140671366078368
	140671352806592 [label="stages.2.blocks.0.mlp.norm2.weight
 (224)" fillcolor=lightblue]
	140671352806592 -> 140671365996400
	140671365996400 [label=AccumulateGrad]
	140671365996256 -> 140671366078368
	140671352806672 [label="stages.2.blocks.0.mlp.norm2.bias
 (224)" fillcolor=lightblue]
	140671352806672 -> 140671365996256
	140671365996256 [label=AccumulateGrad]
	140671366099488 -> 140671366099584
	140671366099488 [label=ViewBackward0]
	140671365995728 -> 140671366099488
	140671352806912 [label="stages.2.blocks.0.ls2.gamma
 (224)" fillcolor=lightblue]
	140671352806912 -> 140671365995728
	140671365995728 [label=AccumulateGrad]
	140671366099824 -> 140671366100016
	140671366099824 [label=MulBackward0]
	140671366018096 -> 140671366099824
	140671366018096 [label=SubBackward0]
	140671365996208 -> 140671366018096
	140671365996208 [label=AvgPool2DBackward0]
	140671366099680 -> 140671365996208
	140671366099680 -> 140671366018096
	140671366099632 -> 140671366099824
	140671366099632 [label=ViewBackward0]
	140671365995056 -> 140671366099632
	140671352806992 [label="stages.2.blocks.1.ls1.gamma
 (224)" fillcolor=lightblue]
	140671352806992 -> 140671365995056
	140671365995056 [label=AccumulateGrad]
	140671366100064 -> 140671366100256
	140671366100064 [label=MulBackward0]
	140671366099872 -> 140671366100064
	140671366099872 [label=NativeBatchNormBackward0]
	140671365995440 -> 140671366099872
	140671365995440 [label=ConvolutionBackward0]
	140671365994576 -> 140671365995440
	140671365994576 [label=GeluBackward0]
	140671365994288 -> 140671365994576
	140671365994288 [label=NativeBatchNormBackward0]
	140671365994096 -> 140671365994288
	140671365994096 [label=ConvolutionBackward0]
	140671366100016 -> 140671365994096
	140671365993712 -> 140671365994096
	140671352807232 [label="stages.2.blocks.1.mlp.fc1.weight
 (896, 224, 1, 1)" fillcolor=lightblue]
	140671352807232 -> 140671365993712
	140671365993712 [label=AccumulateGrad]
	140671365993760 -> 140671365994096
	140671352807312 [label="stages.2.blocks.1.mlp.fc1.bias
 (896)" fillcolor=lightblue]
	140671352807312 -> 140671365993760
	140671365993760 [label=AccumulateGrad]
	140671365994144 -> 140671365994288
	140671353843776 [label="stages.2.blocks.1.mlp.norm1.weight
 (896)" fillcolor=lightblue]
	140671353843776 -> 140671365994144
	140671365994144 [label=AccumulateGrad]
	140671365994480 -> 140671365994288
	140671353843856 [label="stages.2.blocks.1.mlp.norm1.bias
 (896)" fillcolor=lightblue]
	140671353843856 -> 140671365994480
	140671365994480 [label=AccumulateGrad]
	140671365994624 -> 140671365995440
	140671353844256 [label="stages.2.blocks.1.mlp.fc2.weight
 (224, 896, 1, 1)" fillcolor=lightblue]
	140671353844256 -> 140671365994624
	140671365994624 [label=AccumulateGrad]
	140671365994672 -> 140671365995440
	140671353844336 [label="stages.2.blocks.1.mlp.fc2.bias
 (224)" fillcolor=lightblue]
	140671353844336 -> 140671365994672
	140671365994672 [label=AccumulateGrad]
	140671365995296 -> 140671366099872
	140671353844416 [label="stages.2.blocks.1.mlp.norm2.weight
 (224)" fillcolor=lightblue]
	140671353844416 -> 140671365995296
	140671365995296 [label=AccumulateGrad]
	140671365994864 -> 140671366099872
	140671353844496 [label="stages.2.blocks.1.mlp.norm2.bias
 (224)" fillcolor=lightblue]
	140671353844496 -> 140671365994864
	140671365994864 [label=AccumulateGrad]
	140671365994912 -> 140671366100064
	140671365994912 [label=ViewBackward0]
	140671365994336 -> 140671365994912
	140671353844736 [label="stages.2.blocks.1.ls2.gamma
 (224)" fillcolor=lightblue]
	140671353844736 -> 140671365994336
	140671365994336 [label=AccumulateGrad]
	140671366100400 -> 140671366100592
	140671366100400 [label=MulBackward0]
	140671366100208 -> 140671366100400
	140671366100208 [label=SubBackward0]
	140671365994720 -> 140671366100208
	140671365994720 [label=AvgPool2DBackward0]
	140671366100256 -> 140671365994720
	140671366100256 -> 140671366100208
	140671365994528 -> 140671366100400
	140671365994528 [label=ViewBackward0]
	140671365993568 -> 140671365994528
	140671353844816 [label="stages.2.blocks.2.ls1.gamma
 (224)" fillcolor=lightblue]
	140671353844816 -> 140671365993568
	140671365993568 [label=AccumulateGrad]
	140671366100640 -> 140671366100736
	140671366100640 [label=MulBackward0]
	140671366100448 -> 140671366100640
	140671366100448 [label=NativeBatchNormBackward0]
	140671365993952 -> 140671366100448
	140671365993952 [label=ConvolutionBackward0]
	140671365993088 -> 140671365993952
	140671365993088 [label=GeluBackward0]
	140671365992800 -> 140671365993088
	140671365992800 [label=NativeBatchNormBackward0]
	140671365992608 -> 140671365992800
	140671365992608 [label=ConvolutionBackward0]
	140671366100592 -> 140671365992608
	140671365971680 -> 140671365992608
	140671353845056 [label="stages.2.blocks.2.mlp.fc1.weight
 (896, 224, 1, 1)" fillcolor=lightblue]
	140671353845056 -> 140671365971680
	140671365971680 [label=AccumulateGrad]
	140671365971824 -> 140671365992608
	140671353845136 [label="stages.2.blocks.2.mlp.fc1.bias
 (896)" fillcolor=lightblue]
	140671353845136 -> 140671365971824
	140671365971824 [label=AccumulateGrad]
	140671365992752 -> 140671365992800
	140671353845216 [label="stages.2.blocks.2.mlp.norm1.weight
 (896)" fillcolor=lightblue]
	140671353845216 -> 140671365992752
	140671365992752 [label=AccumulateGrad]
	140671365992992 -> 140671365992800
	140671353845296 [label="stages.2.blocks.2.mlp.norm1.bias
 (896)" fillcolor=lightblue]
	140671353845296 -> 140671365992992
	140671365992992 [label=AccumulateGrad]
	140671365993232 -> 140671365993952
	140671353845696 [label="stages.2.blocks.2.mlp.fc2.weight
 (224, 896, 1, 1)" fillcolor=lightblue]
	140671353845696 -> 140671365993232
	140671365993232 [label=AccumulateGrad]
	140671365993280 -> 140671365993952
	140671353845776 [label="stages.2.blocks.2.mlp.fc2.bias
 (224)" fillcolor=lightblue]
	140671353845776 -> 140671365993280
	140671365993280 [label=AccumulateGrad]
	140671365993904 -> 140671366100448
	140671353845856 [label="stages.2.blocks.2.mlp.norm2.weight
 (224)" fillcolor=lightblue]
	140671353845856 -> 140671365993904
	140671365993904 [label=AccumulateGrad]
	140671365993472 -> 140671366100448
	140671353845936 [label="stages.2.blocks.2.mlp.norm2.bias
 (224)" fillcolor=lightblue]
	140671353845936 -> 140671365993472
	140671365993472 [label=AccumulateGrad]
	140671365993520 -> 140671366100640
	140671365993520 [label=ViewBackward0]
	140671365992944 -> 140671365993520
	140671353846176 [label="stages.2.blocks.2.ls2.gamma
 (224)" fillcolor=lightblue]
	140671353846176 -> 140671365992944
	140671365992944 [label=AccumulateGrad]
	140671366100880 -> 140671366101072
	140671366100880 [label=MulBackward0]
	140671366100688 -> 140671366100880
	140671366100688 [label=SubBackward0]
	140671365993424 -> 140671366100688
	140671365993424 [label=AvgPool2DBackward0]
	140671366100736 -> 140671365993424
	140671366100736 -> 140671366100688
	140671365993040 -> 140671366100880
	140671365993040 [label=ViewBackward0]
	140671365992560 -> 140671365993040
	140671353846256 [label="stages.2.blocks.3.ls1.gamma
 (224)" fillcolor=lightblue]
	140671353846256 -> 140671365992560
	140671365992560 [label=AccumulateGrad]
	140671366101120 -> 140671366101216
	140671366101120 [label=MulBackward0]
	140671365995920 -> 140671366101120
	140671365995920 [label=NativeBatchNormBackward0]
	140671365971488 -> 140671365995920
	140671365971488 [label=ConvolutionBackward0]
	140671365971248 -> 140671365971488
	140671365971248 [label=GeluBackward0]
	140671365970864 -> 140671365971248
	140671365970864 [label=NativeBatchNormBackward0]
	140671365970672 -> 140671365970864
	140671365970672 [label=ConvolutionBackward0]
	140671366101072 -> 140671365970672
	140671365970288 -> 140671365970672
	140671353846496 [label="stages.2.blocks.3.mlp.fc1.weight
 (896, 224, 1, 1)" fillcolor=lightblue]
	140671353846496 -> 140671365970288
	140671365970288 [label=AccumulateGrad]
	140671365970336 -> 140671365970672
	140671353846576 [label="stages.2.blocks.3.mlp.fc1.bias
 (896)" fillcolor=lightblue]
	140671353846576 -> 140671365970336
	140671365970336 [label=AccumulateGrad]
	140671365970720 -> 140671365970864
	140671353846656 [label="stages.2.blocks.3.mlp.norm1.weight
 (896)" fillcolor=lightblue]
	140671353846656 -> 140671365970720
	140671365970720 [label=AccumulateGrad]
	140671365971056 -> 140671365970864
	140671353846736 [label="stages.2.blocks.3.mlp.norm1.bias
 (896)" fillcolor=lightblue]
	140671353846736 -> 140671365971056
	140671365971056 [label=AccumulateGrad]
	140671365971296 -> 140671365971488
	140671353847136 [label="stages.2.blocks.3.mlp.fc2.weight
 (224, 896, 1, 1)" fillcolor=lightblue]
	140671353847136 -> 140671365971296
	140671365971296 [label=AccumulateGrad]
	140671365971344 -> 140671365971488
	140671353847216 [label="stages.2.blocks.3.mlp.fc2.bias
 (224)" fillcolor=lightblue]
	140671353847216 -> 140671365971344
	140671365971344 [label=AccumulateGrad]
	140671365971872 -> 140671365995920
	140671353847296 [label="stages.2.blocks.3.mlp.norm2.weight
 (224)" fillcolor=lightblue]
	140671353847296 -> 140671365971872
	140671365971872 [label=AccumulateGrad]
	140671365971440 -> 140671365995920
	140671353847376 [label="stages.2.blocks.3.mlp.norm2.bias
 (224)" fillcolor=lightblue]
	140671353847376 -> 140671365971440
	140671365971440 [label=AccumulateGrad]
	140671366100928 -> 140671366101120
	140671366100928 [label=ViewBackward0]
	140671365970912 -> 140671366100928
	140671353847616 [label="stages.2.blocks.3.ls2.gamma
 (224)" fillcolor=lightblue]
	140671353847616 -> 140671365970912
	140671365970912 [label=AccumulateGrad]
	140671366101360 -> 140671366101552
	140671366101360 [label=MulBackward0]
	140671366101168 -> 140671366101360
	140671366101168 [label=SubBackward0]
	140671365971392 -> 140671366101168
	140671365971392 [label=AvgPool2DBackward0]
	140671366101216 -> 140671365971392
	140671366101216 -> 140671366101168
	140671365971104 -> 140671366101360
	140671365971104 [label=ViewBackward0]
	140671365970240 -> 140671365971104
	140671353847696 [label="stages.2.blocks.4.ls1.gamma
 (224)" fillcolor=lightblue]
	140671353847696 -> 140671365970240
	140671365970240 [label=AccumulateGrad]
	140671366101600 -> 140671366101792
	140671366101600 [label=MulBackward0]
	140671366101408 -> 140671366101600
	140671366101408 [label=NativeBatchNormBackward0]
	140671365970528 -> 140671366101408
	140671365970528 [label=ConvolutionBackward0]
	140671365969760 -> 140671365970528
	140671365969760 [label=GeluBackward0]
	140671365969376 -> 140671365969760
	140671365969376 [label=NativeBatchNormBackward0]
	140671365969184 -> 140671365969376
	140671365969184 [label=ConvolutionBackward0]
	140671366101552 -> 140671365969184
	140671365968896 -> 140671365969184
	140672403349728 [label="stages.2.blocks.4.mlp.fc1.weight
 (896, 224, 1, 1)" fillcolor=lightblue]
	140672403349728 -> 140671365968896
	140671365968896 [label=AccumulateGrad]
	140671365968944 -> 140671365969184
	140672403349808 [label="stages.2.blocks.4.mlp.fc1.bias
 (896)" fillcolor=lightblue]
	140672403349808 -> 140671365968944
	140671365968944 [label=AccumulateGrad]
	140671365969328 -> 140671365969376
	140672403349888 [label="stages.2.blocks.4.mlp.norm1.weight
 (896)" fillcolor=lightblue]
	140672403349888 -> 140671365969328
	140671365969328 [label=AccumulateGrad]
	140671365969568 -> 140671365969376
	140672403349968 [label="stages.2.blocks.4.mlp.norm1.bias
 (896)" fillcolor=lightblue]
	140672403349968 -> 140671365969568
	140671365969568 [label=AccumulateGrad]
	140671365969904 -> 140671365970528
	140672403350368 [label="stages.2.blocks.4.mlp.fc2.weight
 (224, 896, 1, 1)" fillcolor=lightblue]
	140672403350368 -> 140671365969904
	140671365969904 [label=AccumulateGrad]
	140671365969952 -> 140671365970528
	140672403350448 [label="stages.2.blocks.4.mlp.fc2.bias
 (224)" fillcolor=lightblue]
	140672403350448 -> 140671365969952
	140671365969952 [label=AccumulateGrad]
	140671365970480 -> 140671366101408
	140672403350528 [label="stages.2.blocks.4.mlp.norm2.weight
 (224)" fillcolor=lightblue]
	140672403350528 -> 140671365970480
	140671365970480 [label=AccumulateGrad]
	140671365970048 -> 140671366101408
	140672403350608 [label="stages.2.blocks.4.mlp.norm2.bias
 (224)" fillcolor=lightblue]
	140672403350608 -> 140671365970048
	140671365970048 [label=AccumulateGrad]
	140671365970192 -> 140671366101600
	140671365970192 [label=ViewBackward0]
	140671365969520 -> 140671365970192
	140672403350848 [label="stages.2.blocks.4.ls2.gamma
 (224)" fillcolor=lightblue]
	140672403350848 -> 140671365969520
	140671365969520 [label=AccumulateGrad]
	140671366101936 -> 140671366102128
	140671366101936 [label=MulBackward0]
	140671366101744 -> 140671366101936
	140671366101744 [label=SubBackward0]
	140671365970000 -> 140671366101744
	140671365970000 [label=AvgPool2DBackward0]
	140671366101792 -> 140671365970000
	140671366101792 -> 140671366101744
	140671365969712 -> 140671366101936
	140671365969712 [label=ViewBackward0]
	140671365968848 -> 140671365969712
	140672403350928 [label="stages.2.blocks.5.ls1.gamma
 (224)" fillcolor=lightblue]
	140672403350928 -> 140671365968848
	140671365968848 [label=AccumulateGrad]
	140671366102176 -> 140671366102320
	140671366102176 [label=MulBackward0]
	140671366101984 -> 140671366102176
	140671366101984 [label=NativeBatchNormBackward0]
	140671365969136 -> 140671366101984
	140671365969136 [label=ConvolutionBackward0]
	140671365968368 -> 140671365969136
	140671365968368 [label=GeluBackward0]
	140671365967984 -> 140671365968368
	140671365967984 [label=NativeBatchNormBackward0]
	140671365968176 -> 140671365967984
	140671365968176 [label=ConvolutionBackward0]
	140671366102128 -> 140671365968176
	140671365938768 -> 140671365968176
	140672403351168 [label="stages.2.blocks.5.mlp.fc1.weight
 (896, 224, 1, 1)" fillcolor=lightblue]
	140672403351168 -> 140671365938768
	140671365938768 [label=AccumulateGrad]
	140671365938816 -> 140671365968176
	140672403351248 [label="stages.2.blocks.5.mlp.fc1.bias
 (896)" fillcolor=lightblue]
	140672403351248 -> 140671365938816
	140671365938816 [label=AccumulateGrad]
	140671365939056 -> 140671365967984
	140672403351328 [label="stages.2.blocks.5.mlp.norm1.weight
 (896)" fillcolor=lightblue]
	140672403351328 -> 140671365939056
	140671365939056 [label=AccumulateGrad]
	140671365939104 -> 140671365967984
	140672403351408 [label="stages.2.blocks.5.mlp.norm1.bias
 (896)" fillcolor=lightblue]
	140672403351408 -> 140671365939104
	140671365939104 [label=AccumulateGrad]
	140671365968416 -> 140671365969136
	140672403351808 [label="stages.2.blocks.5.mlp.fc2.weight
 (224, 896, 1, 1)" fillcolor=lightblue]
	140672403351808 -> 140671365968416
	140671365968416 [label=AccumulateGrad]
	140671365968560 -> 140671365969136
	140672403351888 [label="stages.2.blocks.5.mlp.fc2.bias
 (224)" fillcolor=lightblue]
	140672403351888 -> 140671365968560
	140671365968560 [label=AccumulateGrad]
	140671365968992 -> 140671366101984
	140672403351968 [label="stages.2.blocks.5.mlp.norm2.weight
 (224)" fillcolor=lightblue]
	140672403351968 -> 140671365968992
	140671365968992 [label=AccumulateGrad]
	140671365968656 -> 140671366101984
	140672403352048 [label="stages.2.blocks.5.mlp.norm2.bias
 (224)" fillcolor=lightblue]
	140672403352048 -> 140671365968656
	140671365968656 [label=AccumulateGrad]
	140671365968704 -> 140671366102176
	140671365968704 [label=ViewBackward0]
	140671365968032 -> 140671365968704
	140672403352288 [label="stages.2.blocks.5.ls2.gamma
 (224)" fillcolor=lightblue]
	140672403352288 -> 140671365968032
	140671365968032 [label=AccumulateGrad]
	140671366102368 -> 140671366102704
	140672403352448 [label="stages.3.downsample.conv.weight
 (448, 224, 3, 3)" fillcolor=lightblue]
	140672403352448 -> 140671366102368
	140671366102368 [label=AccumulateGrad]
	140671366102512 -> 140671366102704
	140672403352528 [label="stages.3.downsample.conv.bias
 (448)" fillcolor=lightblue]
	140672403352528 -> 140671366102512
	140671366102512 [label=AccumulateGrad]
	140671366102752 -> 140671366127728
	140672403352608 [label="stages.3.downsample.norm.weight
 (448)" fillcolor=lightblue]
	140672403352608 -> 140671366102752
	140671366102752 [label=AccumulateGrad]
	140671366102896 -> 140671366127728
	140672403352688 [label="stages.3.downsample.norm.bias
 (448)" fillcolor=lightblue]
	140672403352688 -> 140671366102896
	140671366102896 [label=AccumulateGrad]
	140671366127776 -> 140671366127920
	140671366127776 [label=MulBackward0]
	140671366102224 -> 140671366127776
	140671366102224 [label=SubBackward0]
	140671365968608 -> 140671366102224
	140671365968608 [label=AvgPool2DBackward0]
	140671366127728 -> 140671365968608
	140671366127728 -> 140671366102224
	140671366102272 -> 140671366127776
	140671366102272 [label=ViewBackward0]
	140671365971632 -> 140671366102272
	140672403353008 [label="stages.3.blocks.0.ls1.gamma
 (448)" fillcolor=lightblue]
	140672403353008 -> 140671365971632
	140671365971632 [label=AccumulateGrad]
	140671366127968 -> 140671366128064
	140671366127968 [label=MulBackward0]
	140671365968224 -> 140671366127968
	140671365968224 [label=NativeBatchNormBackward0]
	140671365938912 -> 140671365968224
	140671365938912 [label=ConvolutionBackward0]
	140671365938336 -> 140671365938912
	140671365938336 [label=GeluBackward0]
	140671365937952 -> 140671365938336
	140671365937952 [label=NativeBatchNormBackward0]
	140671365937760 -> 140671365937952
	140671365937760 [label=ConvolutionBackward0]
	140671366127920 -> 140671365937760
	140671365937472 -> 140671365937760
	140672403353248 [label="stages.3.blocks.0.mlp.fc1.weight
 (1792, 448, 1, 1)" fillcolor=lightblue]
	140672403353248 -> 140671365937472
	140671365937472 [label=AccumulateGrad]
	140671365937616 -> 140671365937760
	140672403353328 [label="stages.3.blocks.0.mlp.fc1.bias
 (1792)" fillcolor=lightblue]
	140672403353328 -> 140671365937616
	140671365937616 [label=AccumulateGrad]
	140671365937904 -> 140671365937952
	140672403353408 [label="stages.3.blocks.0.mlp.norm1.weight
 (1792)" fillcolor=lightblue]
	140672403353408 -> 140671365937904
	140671365937904 [label=AccumulateGrad]
	140671365938144 -> 140671365937952
	140672403353488 [label="stages.3.blocks.0.mlp.norm1.bias
 (1792)" fillcolor=lightblue]
	140672403353488 -> 140671365938144
	140671365938144 [label=AccumulateGrad]
	140671365938480 -> 140671365938912
	140672403472768 [label="stages.3.blocks.0.mlp.fc2.weight
 (448, 1792, 1, 1)" fillcolor=lightblue]
	140672403472768 -> 140671365938480
	140671365938480 [label=AccumulateGrad]
	140671365938528 -> 140671365938912
	140672403472848 [label="stages.3.blocks.0.mlp.fc2.bias
 (448)" fillcolor=lightblue]
	140672403472848 -> 140671365938528
	140671365938528 [label=AccumulateGrad]
	140671365938864 -> 140671365968224
	140672403472928 [label="stages.3.blocks.0.mlp.norm2.weight
 (448)" fillcolor=lightblue]
	140672403472928 -> 140671365938864
	140671365938864 [label=AccumulateGrad]
	140671365938720 -> 140671365968224
	140672403473008 [label="stages.3.blocks.0.mlp.norm2.bias
 (448)" fillcolor=lightblue]
	140672403473008 -> 140671365938720
	140671365938720 [label=AccumulateGrad]
	140671366102560 -> 140671366127968
	140671366102560 [label=ViewBackward0]
	140671365938096 -> 140671366102560
	140672403473248 [label="stages.3.blocks.0.ls2.gamma
 (448)" fillcolor=lightblue]
	140672403473248 -> 140671365938096
	140671365938096 [label=AccumulateGrad]
	140671366128208 -> 140671366128400
	140671366128208 [label=MulBackward0]
	140671366102944 -> 140671366128208
	140671366102944 [label=SubBackward0]
	140671365938672 -> 140671366102944
	140671365938672 [label=AvgPool2DBackward0]
	140671366128064 -> 140671365938672
	140671366128064 -> 140671366102944
	140671366128016 -> 140671366128208
	140671366128016 [label=ViewBackward0]
	140671365937424 -> 140671366128016
	140672403473328 [label="stages.3.blocks.1.ls1.gamma
 (448)" fillcolor=lightblue]
	140672403473328 -> 140671365937424
	140671365937424 [label=AccumulateGrad]
	140671366128448 -> 140671366128544
	140671366128448 [label=MulBackward0]
	140671366128256 -> 140671366128448
	140671366128256 [label=NativeBatchNormBackward0]
	140671365937712 -> 140671366128256
	140671365937712 [label=ConvolutionBackward0]
	140671365936944 -> 140671365937712
	140671365936944 [label=GeluBackward0]
	140671365936560 -> 140671365936944
	140671365936560 [label=NativeBatchNormBackward0]
	140671365936368 -> 140671365936560
	140671365936368 [label=ConvolutionBackward0]
	140671366128400 -> 140671365936368
	140671365936032 -> 140671365936368
	140672403473568 [label="stages.3.blocks.1.mlp.fc1.weight
 (1792, 448, 1, 1)" fillcolor=lightblue]
	140672403473568 -> 140671365936032
	140671365936032 [label=AccumulateGrad]
	140671365936080 -> 140671365936368
	140672403473648 [label="stages.3.blocks.1.mlp.fc1.bias
 (1792)" fillcolor=lightblue]
	140672403473648 -> 140671365936080
	140671365936080 [label=AccumulateGrad]
	140671365936416 -> 140671365936560
	140672403473728 [label="stages.3.blocks.1.mlp.norm1.weight
 (1792)" fillcolor=lightblue]
	140672403473728 -> 140671365936416
	140671365936416 [label=AccumulateGrad]
	140671365936752 -> 140671365936560
	140672403473808 [label="stages.3.blocks.1.mlp.norm1.bias
 (1792)" fillcolor=lightblue]
	140672403473808 -> 140671365936752
	140671365936752 [label=AccumulateGrad]
	140671365936992 -> 140671365937712
	140672403474208 [label="stages.3.blocks.1.mlp.fc2.weight
 (448, 1792, 1, 1)" fillcolor=lightblue]
	140672403474208 -> 140671365936992
	140671365936992 [label=AccumulateGrad]
	140671365937136 -> 140671365937712
	140672403474288 [label="stages.3.blocks.1.mlp.fc2.bias
 (448)" fillcolor=lightblue]
	140672403474288 -> 140671365937136
	140671365937136 [label=AccumulateGrad]
	140671365937664 -> 140671366128256
	140672403474368 [label="stages.3.blocks.1.mlp.norm2.weight
 (448)" fillcolor=lightblue]
	140672403474368 -> 140671365937664
	140671365937664 [label=AccumulateGrad]
	140671365937232 -> 140671366128256
	140672403474448 [label="stages.3.blocks.1.mlp.norm2.bias
 (448)" fillcolor=lightblue]
	140672403474448 -> 140671365937232
	140671365937232 [label=AccumulateGrad]
	140671365937280 -> 140671366128448
	140671365937280 [label=ViewBackward0]
	140671365936608 -> 140671365937280
	140672403474688 [label="stages.3.blocks.1.ls2.gamma
 (448)" fillcolor=lightblue]
	140672403474688 -> 140671365936608
	140671365936608 [label=AccumulateGrad]
	140671366128688 -> 140671366128880
	140671366128688 [label=MulBackward0]
	140671366128496 -> 140671366128688
	140671366128496 [label=SubBackward0]
	140671365937184 -> 140671366128496
	140671365937184 [label=AvgPool2DBackward0]
	140671366128544 -> 140671365937184
	140671366128544 -> 140671366128496
	140671365936800 -> 140671366128688
	140671365936800 [label=ViewBackward0]
	140671365935984 -> 140671365936800
	140672403474768 [label="stages.3.blocks.2.ls1.gamma
 (448)" fillcolor=lightblue]
	140672403474768 -> 140671365935984
	140671365935984 [label=AccumulateGrad]
	140671366128928 -> 140671366129072
	140671366128928 [label=MulBackward0]
	140671366128736 -> 140671366128928
	140671366128736 [label=NativeBatchNormBackward0]
	140671365936224 -> 140671366128736
	140671365936224 [label=ConvolutionBackward0]
	140671365935408 -> 140671365936224
	140671365935408 [label=GeluBackward0]
	140671365935216 -> 140671365935408
	140671365935216 [label=NativeBatchNormBackward0]
	140671365918480 -> 140671365935216
	140671365918480 [label=ConvolutionBackward0]
	140671366128880 -> 140671365918480
	140671365918192 -> 140671365918480
	140672403475008 [label="stages.3.blocks.2.mlp.fc1.weight
 (1792, 448, 1, 1)" fillcolor=lightblue]
	140672403475008 -> 140671365918192
	140671365918192 [label=AccumulateGrad]
	140671365918240 -> 140671365918480
	140672403475088 [label="stages.3.blocks.2.mlp.fc1.bias
 (1792)" fillcolor=lightblue]
	140672403475088 -> 140671365918240
	140671365918240 [label=AccumulateGrad]
	140671365918528 -> 140671365935216
	140672403475168 [label="stages.3.blocks.2.mlp.norm1.weight
 (1792)" fillcolor=lightblue]
	140672403475168 -> 140671365918528
	140671365918528 [label=AccumulateGrad]
	140671365918624 -> 140671365935216
	140672403475248 [label="stages.3.blocks.2.mlp.norm1.bias
 (1792)" fillcolor=lightblue]
	140672403475248 -> 140671365918624
	140671365918624 [label=AccumulateGrad]
	140671365935456 -> 140671365936224
	140672403475648 [label="stages.3.blocks.2.mlp.fc2.weight
 (448, 1792, 1, 1)" fillcolor=lightblue]
	140672403475648 -> 140671365935456
	140671365935456 [label=AccumulateGrad]
	140671365935600 -> 140671365936224
	140672403475728 [label="stages.3.blocks.2.mlp.fc2.bias
 (448)" fillcolor=lightblue]
	140672403475728 -> 140671365935600
	140671365935600 [label=AccumulateGrad]
	140671365936128 -> 140671366128736
	140672403475808 [label="stages.3.blocks.2.mlp.norm2.weight
 (448)" fillcolor=lightblue]
	140672403475808 -> 140671365936128
	140671365936128 [label=AccumulateGrad]
	140671365935792 -> 140671366128736
	140672403475888 [label="stages.3.blocks.2.mlp.norm2.bias
 (448)" fillcolor=lightblue]
	140672403475888 -> 140671365935792
	140671365935792 [label=AccumulateGrad]
	140671365935840 -> 140671366128928
	140671365935840 [label=ViewBackward0]
	140671365935264 -> 140671365935840
	140672403476128 [label="stages.3.blocks.2.ls2.gamma
 (448)" fillcolor=lightblue]
	140672403476128 -> 140671365935264
	140671365935264 [label=AccumulateGrad]
	140671366129552 -> 140671366129648
	140671366129552 [label=MulBackward0]
	140671366129456 -> 140671366129552
	140671366129456 [label=ViewBackward0]
	140671365935648 -> 140671366129456
	140671365935648 [label=AddmmBackward0]
	140671365918576 -> 140671365935648
	140672403476288 [label="stages.3.blocks.4.token_mixer.proj.bias
 (448)" fillcolor=lightblue]
	140672403476288 -> 140671365918576
	140671365918576 [label=AccumulateGrad]
	140671365918336 -> 140671365935648
	140671365918336 [label=ViewBackward0]
	140671365918000 -> 140671365918336
	140671365918000 [label=UnsafeViewBackward0]
	140671365917616 -> 140671365918000
	140671365917616 [label=CloneBackward0]
	140671365917424 -> 140671365917616
	140671365917424 [label=TransposeBackward0]
	140671365917232 -> 140671365917424
	140671365917232 [label=UnsafeViewBackward0]
	140671365917136 -> 140671365917232
	140671365917136 [label=BmmBackward0]
	140671365916944 -> 140671365917136
	140671365916944 [label=ReshapeAliasBackward0]
	140671365916704 -> 140671365916944
	140671365916704 [label=ExpandBackward0]
	140671365916512 -> 140671365916704
	140671365916512 [label=SoftmaxBackward0]
	140671365916320 -> 140671365916512
	140671365916320 [label=AddBackward0]
	140671365916128 -> 140671365916320
	140671365916128 [label=MulBackward0]
	140671365915888 -> 140671365916128
	140671365915888 [label=UnsafeViewBackward0]
	140671365915792 -> 140671365915888
	140671365915792 [label=BmmBackward0]
	140671365915696 -> 140671365915792
	140671365915696 [label=ReshapeAliasBackward0]
	140671365915360 -> 140671365915696
	140671365915360 [label=ExpandBackward0]
	140671365915168 -> 140671365915360
	140671365915168 [label=SplitWithSizesBackward0]
	140671365914976 -> 140671365915168
	140671365914976 [label=PermuteBackward0]
	140671365914784 -> 140671365914976
	140671365914784 [label=ReshapeAliasBackward0]
	140671365914688 -> 140671365914784
	140671365914688 [label=ViewBackward0]
	140671365915552 -> 140671365914688
	140671365915552 [label=AddmmBackward0]
	140671360741328 -> 140671365915552
	140671881598912 [label="stages.3.blocks.4.token_mixer.qkv.bias
 (1536)" fillcolor=lightblue]
	140671881598912 -> 140671360741328
	140671360741328 [label=AccumulateGrad]
	140671360739552 -> 140671365915552
	140671360739552 [label=ViewBackward0]
	140670513991632 -> 140671360739552
	140670513991632 [label=NativeLayerNormBackward0]
	140671366129504 -> 140670513991632
	140670513991248 -> 140670513991632
	140671856193680 [label="stages.3.blocks.4.norm1.weight
 (448)" fillcolor=lightblue]
	140671856193680 -> 140670513991248
	140670513991248 [label=AccumulateGrad]
	140670513991296 -> 140670513991632
	140671328126272 [label="stages.3.blocks.4.norm1.bias
 (448)" fillcolor=lightblue]
	140671328126272 -> 140670513991296
	140670513991296 [label=AccumulateGrad]
	140671360739360 -> 140671365915552
	140671360739360 [label=TBackward0]
	140670513991200 -> 140671360739360
	140672403352928 [label="stages.3.blocks.4.token_mixer.qkv.weight
 (1536, 448)" fillcolor=lightblue]
	140672403352928 -> 140670513991200
	140670513991200 [label=AccumulateGrad]
	140671365915744 -> 140671365915792
	140671365915744 [label=ReshapeAliasBackward0]
	140671365915120 -> 140671365915744
	140671365915120 [label=ExpandBackward0]
	140671365914736 -> 140671365915120
	140671365914736 [label=TransposeBackward0]
	140671365915168 -> 140671365914736
	140671365916272 -> 140671365916320
	140671365916272 [label=IndexBackward0]
	140671360738784 -> 140671365916272
	140671360738784 [label=SliceBackward0]
	140671365916080 -> 140671360738784
	140671360618640 [label="stages.3.blocks.4.token_mixer.attention_biases
 (8, 49)" fillcolor=lightblue]
	140671360618640 -> 140671365916080
	140671365916080 [label=AccumulateGrad]
	140671365916992 -> 140671365917136
	140671365916992 [label=ReshapeAliasBackward0]
	140671360738688 -> 140671365916992
	140671360738688 [label=ExpandBackward0]
	140671365915168 -> 140671360738688
	140671365918288 -> 140671365935648
	140671365918288 [label=TBackward0]
	140671365917472 -> 140671365918288
	140672403476208 [label="stages.3.blocks.4.token_mixer.proj.weight
 (448, 1024)" fillcolor=lightblue]
	140672403476208 -> 140671365917472
	140671365917472 [label=AccumulateGrad]
	140671366129120 -> 140671366129552
	140671360648112 [label="stages.3.blocks.4.ls1.gamma
 (448)" fillcolor=lightblue]
	140671360648112 -> 140671366129120
	140671366129120 [label=AccumulateGrad]
	140671366129696 -> 140671366130032
	140671366129696 [label=MulBackward0]
	140671365938288 -> 140671366129696
	140671365938288 [label=ViewBackward0]
	140671365917184 -> 140671365938288
	140671365917184 [label=AddmmBackward0]
	140671365917856 -> 140671365917184
	140671360648432 [label="stages.3.blocks.4.mlp.fc2.bias
 (448)" fillcolor=lightblue]
	140671360648432 -> 140671365917856
	140671365917856 [label=AccumulateGrad]
	140671365917280 -> 140671365917184
	140671365917280 [label=ViewBackward0]
	140671365916656 -> 140671365917280
	140671365916656 [label=GeluBackward0]
	140671365915840 -> 140671365916656
	140671365915840 [label=ViewBackward0]
	140671365915312 -> 140671365915840
	140671365915312 [label=AddmmBackward0]
	140671365915504 -> 140671365915312
	140671360648272 [label="stages.3.blocks.4.mlp.fc1.bias
 (1792)" fillcolor=lightblue]
	140671360648272 -> 140671365915504
	140671365915504 [label=AccumulateGrad]
	140671365915936 -> 140671365915312
	140671365915936 [label=ViewBackward0]
	140670513991008 -> 140671365915936
	140670513991008 [label=NativeLayerNormBackward0]
	140671366129648 -> 140670513991008
	140670513990816 -> 140670513991008
	140671360618720 [label="stages.3.blocks.4.norm2.weight
 (448)" fillcolor=lightblue]
	140671360618720 -> 140670513990816
	140670513990816 [label=AccumulateGrad]
	140670513990960 -> 140670513991008
	140671360618560 [label="stages.3.blocks.4.norm2.bias
 (448)" fillcolor=lightblue]
	140671360618560 -> 140670513990960
	140670513990960 [label=AccumulateGrad]
	140671365916464 -> 140671365915312
	140671365916464 [label=TBackward0]
	140670513990768 -> 140671365916464
	140671360648192 [label="stages.3.blocks.4.mlp.fc1.weight
 (1792, 448)" fillcolor=lightblue]
	140671360648192 -> 140670513990768
	140670513990768 [label=AccumulateGrad]
	140671365917664 -> 140671365917184
	140671365917664 [label=TBackward0]
	140671365914928 -> 140671365917664
	140671360648352 [label="stages.3.blocks.4.mlp.fc2.weight
 (448, 1792)" fillcolor=lightblue]
	140671360648352 -> 140671365914928
	140671365914928 [label=AccumulateGrad]
	140671366129312 -> 140671366129696
	140671360648512 [label="stages.3.blocks.4.ls2.gamma
 (448)" fillcolor=lightblue]
	140671360648512 -> 140671366129312
	140671366129312 [label=AccumulateGrad]
	140671366130080 -> 140671366130416
	140671360648672 [label="norm.weight
 (448)" fillcolor=lightblue]
	140671360648672 -> 140671366130080
	140671366130080 [label=AccumulateGrad]
	140671366130224 -> 140671366130416
	140671360648752 [label="norm.bias
 (448)" fillcolor=lightblue]
	140671360648752 -> 140671366130224
	140671366130224 [label=AccumulateGrad]
	140671366131568 -> 140671366130944
	140671366131568 [label=TBackward0]
	140671366129840 -> 140671366131568
	140671360648832 [label="head.weight
 (1000, 448)" fillcolor=lightblue]
	140671360648832 -> 140671366129840
	140671366129840 [label=AccumulateGrad]
	140671366131376 -> 140671366130704
	140671366131376 [label=AddmmBackward0]
	140671366129600 -> 140671366131376
	140671360649072 [label="head_dist.bias
 (1000)" fillcolor=lightblue]
	140671360649072 -> 140671366129600
	140671366129600 [label=AccumulateGrad]
	140671366130464 -> 140671366131376
	140671366129888 -> 140671366131376
	140671366129888 [label=TBackward0]
	140671366130272 -> 140671366129888
	140671360648992 [label="head_dist.weight
 (1000, 448)" fillcolor=lightblue]
	140671360648992 -> 140671366130272
	140671366130272 [label=AccumulateGrad]
	140671366131088 -> 140671366002384
}
