digraph {
	graph [size="277.8,277.8"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	140191628029680 [label="
 (1, 1000)" fillcolor=darkolivegreen1]
	140191635291584 [label=AddmmBackward0]
	140191635291872 -> 140191635291584
	140190832788960 [label="classifier.bias
 (1000)" fillcolor=lightblue]
	140190832788960 -> 140191635291872
	140191635291872 [label=AccumulateGrad]
	140191635291680 -> 140191635291584
	140191635291680 [label=ReshapeAliasBackward0]
	140191635291488 -> 140191635291680
	140191635291488 [label=ReluBackward0]
	140191635291152 -> 140191635291488
	140191635291152 [label=ConvolutionBackward0]
	140191635291104 -> 140191635291152
	140191635291104 [label=MeanBackward1]
	140191635290672 -> 140191635291104
	140191635290672 [label=ReluBackward0]
	140191635290624 -> 140191635290672
	140191635290624 [label=NativeBatchNormBackward0]
	140191635290480 -> 140191635290624
	140191635290480 [label=ConvolutionBackward0]
	140191635290240 -> 140191635290480
	140191635290240 [label=AsStridedBackward0]
	140191635290096 -> 140191635290240
	140191635290096 [label=CopySlices]
	140191635289952 -> 140191635290096
	140191635289952 [label=CatBackward0]
	140191635289712 -> 140191635289952
	140191635289712 [label=NativeBatchNormBackward0]
	140191635289472 -> 140191635289712
	140191635289472 [label=ConvolutionBackward0]
	140191635289136 -> 140191635289472
	140191635289136 [label=MulBackward0]
	140191635288992 -> 140191635289136
	140191635288992 [label=SliceBackward0]
	140191635288848 -> 140191635288992
	140191635288848 [label=SliceBackward0]
	140191635288704 -> 140191635288848
	140191635288704 [label=SliceBackward0]
	140191635288416 -> 140191635288704
	140191635288416 [label=CatBackward0]
	140191635288368 -> 140191635288416
	140191635288368 [label=ReluBackward0]
	140191635288128 -> 140191635288368
	140191635288128 [label=NativeBatchNormBackward0]
	140191635288176 -> 140191635288128
	140191635288176 [label=ConvolutionBackward0]
	140191635289904 -> 140191635288176
	140191635289904 [label=AsStridedBackward0]
	140191635267056 -> 140191635289904
	140191635267056 [label=CopySlices]
	140191635266864 -> 140191635267056
	140191635266864 [label=CatBackward0]
	140191635266624 -> 140191635266864
	140191635266624 [label=NativeBatchNormBackward0]
	140191635266432 -> 140191635266624
	140191635266432 [label=ConvolutionBackward0]
	140191635266336 -> 140191635266432
	140191635266336 [label=SliceBackward0]
	140191635266096 -> 140191635266336
	140191635266096 [label=SliceBackward0]
	140191635265808 -> 140191635266096
	140191635265808 [label=SliceBackward0]
	140191635265760 -> 140191635265808
	140191635265760 [label=CatBackward0]
	140191635265616 -> 140191635265760
	140191635265616 [label=ReluBackward0]
	140191635265376 -> 140191635265616
	140191635265376 [label=NativeBatchNormBackward0]
	140191635265184 -> 140191635265376
	140191635265184 [label=ConvolutionBackward0]
	140191635266960 -> 140191635265184
	140191635266960 [label=AsStridedBackward0]
	140191635264800 -> 140191635266960
	140191635264800 [label=CopySlices]
	140191635264752 -> 140191635264800
	140191635264752 [label=CatBackward0]
	140191635264512 -> 140191635264752
	140191635264512 [label=NativeBatchNormBackward0]
	140191635264272 -> 140191635264512
	140191635264272 [label=ConvolutionBackward0]
	140191635263936 -> 140191635264272
	140191635263936 [label=MulBackward0]
	140191635263792 -> 140191635263936
	140191635263792 [label=SliceBackward0]
	140191635263552 -> 140191635263792
	140191635263552 [label=SliceBackward0]
	140191635263600 -> 140191635263552
	140191635263600 [label=SliceBackward0]
	140191628230464 -> 140191635263600
	140191628230464 [label=CatBackward0]
	140191628230176 -> 140191628230464
	140191628230176 [label=ReluBackward0]
	140191628229936 -> 140191628230176
	140191628229936 [label=NativeBatchNormBackward0]
	140191628229888 -> 140191628229936
	140191628229888 [label=ConvolutionBackward0]
	140191635264848 -> 140191628229888
	140191635264848 [label=AsStridedBackward0]
	140191628229600 -> 140191635264848
	140191628229600 [label=CopySlices]
	140191628229456 -> 140191628229600
	140191628229456 [label=CatBackward0]
	140191628229216 -> 140191628229456
	140191628229216 [label=NativeBatchNormBackward0]
	140191628228976 -> 140191628229216
	140191628228976 [label=ConvolutionBackward0]
	140191628228640 -> 140191628228976
	140191628228640 [label=SliceBackward0]
	140191628228496 -> 140191628228640
	140191628228496 [label=SliceBackward0]
	140191628228448 -> 140191628228496
	140191628228448 [label=SliceBackward0]
	140191628228160 -> 140191628228448
	140191628228160 [label=CatBackward0]
	140191628228112 -> 140191628228160
	140191628228112 [label=ReluBackward0]
	140191628227872 -> 140191628228112
	140191628227872 [label=NativeBatchNormBackward0]
	140191628227728 -> 140191628227872
	140191628227728 [label=ConvolutionBackward0]
	140191628229408 -> 140191628227728
	140191628229408 [label=AsStridedBackward0]
	140191628227344 -> 140191628229408
	140191628227344 [label=CopySlices]
	140191628227152 -> 140191628227344
	140191628227152 [label=CatBackward0]
	140191628226912 -> 140191628227152
	140191628226912 [label=NativeBatchNormBackward0]
	140191628226672 -> 140191628226912
	140191628226672 [label=ConvolutionBackward0]
	140191628226624 -> 140191628226672
	140191628226624 [label=MulBackward0]
	140191628213888 -> 140191628226624
	140191628213888 [label=NativeBatchNormBackward0]
	140191628213744 -> 140191628213888
	140191628213744 [label=ConvolutionBackward0]
	140191628213504 -> 140191628213744
	140191628213504 [label=SliceBackward0]
	140191628213264 -> 140191628213504
	140191628213264 [label=SliceBackward0]
	140191628213120 -> 140191628213264
	140191628213120 [label=SliceBackward0]
	140191628212832 -> 140191628213120
	140191628212832 [label=CatBackward0]
	140191628212784 -> 140191628212832
	140191628212784 [label=ReluBackward0]
	140191628212544 -> 140191628212784
	140191628212544 [label=NativeBatchNormBackward0]
	140191628212496 -> 140191628212544
	140191628212496 [label=ConvolutionBackward0]
	140191628212256 -> 140191628212496
	140191628212256 [label=AsStridedBackward0]
	140191628212016 -> 140191628212256
	140191628212016 [label=CopySlices]
	140191628211872 -> 140191628212016
	140191628211872 [label=CatBackward0]
	140191628211632 -> 140191628211872
	140191628211632 [label=NativeBatchNormBackward0]
	140191628211392 -> 140191628211632
	140191628211392 [label=ConvolutionBackward0]
	140191628211152 -> 140191628211392
	140191628211152 [label=MulBackward0]
	140191628210912 -> 140191628211152
	140191628210912 [label=SliceBackward0]
	140191628210768 -> 140191628210912
	140191628210768 [label=SliceBackward0]
	140191628210624 -> 140191628210768
	140191628210624 [label=SliceBackward0]
	140191628210432 -> 140191628210624
	140191628210432 [label=CatBackward0]
	140191628210384 -> 140191628210432
	140191628210384 [label=ReluBackward0]
	140191628210240 -> 140191628210384
	140191628210240 [label=NativeBatchNormBackward0]
	140191628210288 -> 140191628210240
	140191628210288 [label=ConvolutionBackward0]
	140191628211824 -> 140191628210288
	140191628211824 [label=AsStridedBackward0]
	140191628189072 -> 140191628211824
	140191628189072 [label=CopySlices]
	140191628188784 -> 140191628189072
	140191628188784 [label=CatBackward0]
	140191628188592 -> 140191628188784
	140191628188592 [label=NativeBatchNormBackward0]
	140191628188496 -> 140191628188592
	140191628188496 [label=ConvolutionBackward0]
	140191628188304 -> 140191628188496
	140191628188304 [label=MulBackward0]
	140191628188064 -> 140191628188304
	140191628188064 [label=SliceBackward0]
	140191628187824 -> 140191628188064
	140191628187824 [label=SliceBackward0]
	140191628187536 -> 140191628187824
	140191628187536 [label=SliceBackward0]
	140191628187488 -> 140191628187536
	140191628187488 [label=CatBackward0]
	140191628187440 -> 140191628187488
	140191628187440 [label=ReluBackward0]
	140191628187296 -> 140191628187440
	140191628187296 [label=NativeBatchNormBackward0]
	140191628187008 -> 140191628187296
	140191628187008 [label=ConvolutionBackward0]
	140191628186816 -> 140191628187008
	140191628186816 [label=AsStridedBackward0]
	140191628186576 -> 140191628186816
	140191628186576 [label=CopySlices]
	140191628186288 -> 140191628186576
	140191628186288 [label=CatBackward0]
	140191628186144 -> 140191628186288
	140191628186144 [label=NativeBatchNormBackward0]
	140191628186000 -> 140191628186144
	140191628186000 [label=ConvolutionBackward0]
	140191628185808 -> 140191628186000
	140191628185808 [label=SliceBackward0]
	140191628185712 -> 140191628185808
	140191628185712 [label=SliceBackward0]
	140191628164736 -> 140191628185712
	140191628164736 [label=SliceBackward0]
	140191628164688 -> 140191628164736
	140191628164688 [label=CatBackward0]
	140191628164544 -> 140191628164688
	140191628164544 [label=ReluBackward0]
	140191628164400 -> 140191628164544
	140191628164400 [label=NativeBatchNormBackward0]
	140191628164208 -> 140191628164400
	140191628164208 [label=ConvolutionBackward0]
	140191628186480 -> 140191628164208
	140191628186480 [label=AsStridedBackward0]
	140191628163728 -> 140191628186480
	140191628163728 [label=CopySlices]
	140191628163680 -> 140191628163728
	140191628163680 [label=CatBackward0]
	140191628163440 -> 140191628163680
	140191628163440 [label=NativeBatchNormBackward0]
	140191628163200 -> 140191628163440
	140191628163200 [label=ConvolutionBackward0]
	140191628162960 -> 140191628163200
	140191628162960 [label=SliceBackward0]
	140191628162720 -> 140191628162960
	140191628162720 [label=SliceBackward0]
	140191628162672 -> 140191628162720
	140191628162672 [label=SliceBackward0]
	140191628162528 -> 140191628162672
	140191628162528 [label=CatBackward0]
	140191628162240 -> 140191628162528
	140191628162240 [label=ReluBackward0]
	140191628162000 -> 140191628162240
	140191628162000 [label=NativeBatchNormBackward0]
	140191628161952 -> 140191628162000
	140191628161952 [label=ConvolutionBackward0]
	140191628163776 -> 140191628161952
	140191628163776 [label=AsStridedBackward0]
	140191628161664 -> 140191628163776
	140191628161664 [label=CopySlices]
	140191628161520 -> 140191628161664
	140191628161520 [label=CatBackward0]
	140191628161280 -> 140191628161520
	140191628161280 [label=NativeBatchNormBackward0]
	140191628161184 -> 140191628161280
	140191628161184 [label=ConvolutionBackward0]
	140191628136064 -> 140191628161184
	140191628136064 [label=SliceBackward0]
	140191628135920 -> 140191628136064
	140191628135920 [label=SliceBackward0]
	140191628135872 -> 140191628135920
	140191628135872 [label=SliceBackward0]
	140191628135584 -> 140191628135872
	140191628135584 [label=CatBackward0]
	140191628135536 -> 140191628135584
	140191628135536 [label=ReluBackward0]
	140191628135296 -> 140191628135536
	140191628135296 [label=NativeBatchNormBackward0]
	140191628135152 -> 140191628135296
	140191628135152 [label=ConvolutionBackward0]
	140191628161472 -> 140191628135152
	140191628161472 [label=AsStridedBackward0]
	140191628134768 -> 140191628161472
	140191628134768 [label=CopySlices]
	140191628134576 -> 140191628134768
	140191628134576 [label=CatBackward0]
	140191628134336 -> 140191628134576
	140191628134336 [label=NativeBatchNormBackward0]
	140191628134096 -> 140191628134336
	140191628134096 [label=ConvolutionBackward0]
	140191628133904 -> 140191628134096
	140191628133904 [label=NativeBatchNormBackward0]
	140191628133664 -> 140191628133904
	140191628133664 [label=ConvolutionBackward0]
	140191628133424 -> 140191628133664
	140191628133424 [label=SliceBackward0]
	140191628133280 -> 140191628133424
	140191628133280 [label=SliceBackward0]
	140191628133136 -> 140191628133280
	140191628133136 [label=SliceBackward0]
	140191628132848 -> 140191628133136
	140191628132848 [label=CatBackward0]
	140191628132800 -> 140191628132848
	140191628132800 [label=ReluBackward0]
	140191628132560 -> 140191628132800
	140191628132560 [label=NativeBatchNormBackward0]
	140191628132416 -> 140191628132560
	140191628132416 [label=ConvolutionBackward0]
	140191628111632 -> 140191628132416
	140191628111632 [label=AsStridedBackward0]
	140191628111392 -> 140191628111632
	140191628111392 [label=CopySlices]
	140191628111344 -> 140191628111392
	140191628111344 [label=CatBackward0]
	140191628111104 -> 140191628111344
	140191628111104 [label=NativeBatchNormBackward0]
	140191628110960 -> 140191628111104
	140191628110960 [label=ConvolutionBackward0]
	140191628110720 -> 140191628110960
	140191628110720 [label=MulBackward0]
	140191628110480 -> 140191628110720
	140191628110480 [label=SliceBackward0]
	140191628110240 -> 140191628110480
	140191628110240 [label=SliceBackward0]
	140191628110096 -> 140191628110240
	140191628110096 [label=SliceBackward0]
	140191628109808 -> 140191628110096
	140191628109808 [label=CatBackward0]
	140191628109760 -> 140191628109808
	140191628109760 [label=ReluBackward0]
	140191628109616 -> 140191628109760
	140191628109616 [label=NativeBatchNormBackward0]
	140191628109568 -> 140191628109616
	140191628109568 [label=ConvolutionBackward0]
	140191628111296 -> 140191628109568
	140191628111296 [label=AsStridedBackward0]
	140191628109088 -> 140191628111296
	140191628109088 [label=CopySlices]
	140191628108800 -> 140191628109088
	140191628108800 [label=CatBackward0]
	140191628108560 -> 140191628108800
	140191628108560 [label=NativeBatchNormBackward0]
	140191628108416 -> 140191628108560
	140191628108416 [label=ConvolutionBackward0]
	140191628108320 -> 140191628108416
	140191628108320 [label=MulBackward0]
	140191628108080 -> 140191628108320
	140191628108080 [label=NativeBatchNormBackward0]
	140191628107840 -> 140191628108080
	140191628107840 [label=ConvolutionBackward0]
	140191628082864 -> 140191628107840
	140191628082864 [label=SliceBackward0]
	140191628082624 -> 140191628082864
	140191628082624 [label=SliceBackward0]
	140191628082576 -> 140191628082624
	140191628082576 [label=SliceBackward0]
	140191628082384 -> 140191628082576
	140191628082384 [label=CatBackward0]
	140191628082336 -> 140191628082384
	140191628082336 [label=ReluBackward0]
	140191628082096 -> 140191628082336
	140191628082096 [label=NativeBatchNormBackward0]
	140191628081952 -> 140191628082096
	140191628081952 [label=ConvolutionBackward0]
	140191628081616 -> 140191628081952
	140191628081616 [label=AsStridedBackward0]
	140191628081376 -> 140191628081616
	140191628081376 [label=CopySlices]
	140191628081328 -> 140191628081376
	140191628081328 [label=CatBackward0]
	140191628081184 -> 140191628081328
	140191628081184 [label=NativeBatchNormBackward0]
	140191628080944 -> 140191628081184
	140191628080944 [label=ConvolutionBackward0]
	140191628080608 -> 140191628080944
	140191628080608 [label=SliceBackward0]
	140191628080368 -> 140191628080608
	140191628080368 [label=SliceBackward0]
	140191628080224 -> 140191628080368
	140191628080224 [label=SliceBackward0]
	140191628080032 -> 140191628080224
	140191628080032 [label=CatBackward0]
	140191628079984 -> 140191628080032
	140191628079984 [label=ReluBackward0]
	140191628079744 -> 140191628079984
	140191628079744 [label=NativeBatchNormBackward0]
	140191628079696 -> 140191628079744
	140191628079696 [label=ConvolutionBackward0]
	140191628081280 -> 140191628079696
	140191628081280 [label=AsStridedBackward0]
	140191628079312 -> 140191628081280
	140191628079312 [label=CopySlices]
	140191628079168 -> 140191628079312
	140191628079168 [label=CatBackward0]
	140191628066528 -> 140191628079168
	140191628066528 [label=NativeBatchNormBackward0]
	140191628066288 -> 140191628066528
	140191628066288 [label=ConvolutionBackward0]
	140191628066096 -> 140191628066288
	140191628066096 [label=NativeBatchNormBackward0]
	140191628065856 -> 140191628066096
	140191628065856 [label=ConvolutionBackward0]
	140191628065616 -> 140191628065856
	140191628065616 [label=SliceBackward0]
	140191628065472 -> 140191628065616
	140191628065472 [label=SliceBackward0]
	140191628065328 -> 140191628065472
	140191628065328 [label=SliceBackward0]
	140191628065040 -> 140191628065328
	140191628065040 [label=CatBackward0]
	140191628064992 -> 140191628065040
	140191628064992 [label=ReluBackward0]
	140191628064752 -> 140191628064992
	140191628064752 [label=NativeBatchNormBackward0]
	140191628064608 -> 140191628064752
	140191628064608 [label=ConvolutionBackward0]
	140191628064368 -> 140191628064608
	140191628064368 [label=AsStridedBackward0]
	140191628064224 -> 140191628064368
	140191628064224 [label=CopySlices]
	140191628064080 -> 140191628064224
	140191628064080 [label=CatBackward0]
	140191628063840 -> 140191628064080
	140191628063840 [label=NativeBatchNormBackward0]
	140191628063600 -> 140191628063840
	140191628063600 [label=ConvolutionBackward0]
	140191628063264 -> 140191628063600
	140191628063264 [label=SliceBackward0]
	140191628063120 -> 140191628063264
	140191628063120 [label=SliceBackward0]
	140191628063072 -> 140191628063120
	140191628063072 [label=SliceBackward0]
	140191628062784 -> 140191628063072
	140191628062784 [label=CatBackward0]
	140191628062832 -> 140191628062784
	140191628062832 [label=ReluBackward0]
	140191628041952 -> 140191628062832
	140191628041952 [label=NativeBatchNormBackward0]
	140191628041808 -> 140191628041952
	140191628041808 [label=ConvolutionBackward0]
	140191628064032 -> 140191628041808
	140191628064032 [label=ReluBackward0]
	140191628041424 -> 140191628064032
	140191628041424 [label=NativeBatchNormBackward0]
	140191628041232 -> 140191628041424
	140191628041232 [label=ConvolutionBackward0]
	140191628041040 -> 140191628041232
	140191627223824 [label="conv_stem.weight
 (16, 3, 3, 3)" fillcolor=lightblue]
	140191627223824 -> 140191628041040
	140191628041040 [label=AccumulateGrad]
	140191628041328 -> 140191628041424
	140191627223904 [label="bn1.weight
 (16)" fillcolor=lightblue]
	140191627223904 -> 140191628041328
	140191628041328 [label=AccumulateGrad]
	140191628041520 -> 140191628041424
	140191627223984 [label="bn1.bias
 (16)" fillcolor=lightblue]
	140191627223984 -> 140191628041520
	140191628041520 [label=AccumulateGrad]
	140191628041472 -> 140191628041808
	140191627224464 [label="blocks.0.0.ghost1.primary_conv.0.weight
 (8, 16, 1, 1)" fillcolor=lightblue]
	140191627224464 -> 140191628041472
	140191628041472 [label=AccumulateGrad]
	140191628041760 -> 140191628041952
	140191627224544 [label="blocks.0.0.ghost1.primary_conv.1.weight
 (8)" fillcolor=lightblue]
	140191627224544 -> 140191628041760
	140191628041760 [label=AccumulateGrad]
	140191628042000 -> 140191628041952
	140191627224624 [label="blocks.0.0.ghost1.primary_conv.1.bias
 (8)" fillcolor=lightblue]
	140191627224624 -> 140191628042000
	140191628042000 [label=AccumulateGrad]
	140191628063168 -> 140191628062784
	140191628063168 [label=ReluBackward0]
	140191628041568 -> 140191628063168
	140191628041568 [label=NativeBatchNormBackward0]
	140191628041280 -> 140191628041568
	140191628041280 [label=ConvolutionBackward0]
	140191628062832 -> 140191628041280
	140191628041184 -> 140191628041280
	140191627225024 [label="blocks.0.0.ghost1.cheap_operation.0.weight
 (8, 1, 3, 3)" fillcolor=lightblue]
	140191627225024 -> 140191628041184
	140191628041184 [label=AccumulateGrad]
	140191628040992 -> 140191628041568
	140191627225104 [label="blocks.0.0.ghost1.cheap_operation.1.weight
 (8)" fillcolor=lightblue]
	140191627225104 -> 140191628040992
	140191628040992 [label=AccumulateGrad]
	140191628042048 -> 140191628041568
	140191627225184 [label="blocks.0.0.ghost1.cheap_operation.1.bias
 (8)" fillcolor=lightblue]
	140191627225184 -> 140191628042048
	140191628042048 [label=AccumulateGrad]
	140191628063360 -> 140191628063600
	140191627225584 [label="blocks.0.0.ghost2.primary_conv.0.weight
 (8, 16, 1, 1)" fillcolor=lightblue]
	140191627225584 -> 140191628063360
	140191628063360 [label=AccumulateGrad]
	140191628063552 -> 140191628063840
	140191627225664 [label="blocks.0.0.ghost2.primary_conv.1.weight
 (8)" fillcolor=lightblue]
	140191627225664 -> 140191628063552
	140191628063552 [label=AccumulateGrad]
	140191628063744 -> 140191628063840
	140191627225744 [label="blocks.0.0.ghost2.primary_conv.1.bias
 (8)" fillcolor=lightblue]
	140191627225744 -> 140191628063744
	140191628063744 [label=AccumulateGrad]
	140191628063792 -> 140191628064080
	140191628063792 [label=NativeBatchNormBackward0]
	140191628062976 -> 140191628063792
	140191628062976 [label=ConvolutionBackward0]
	140191628063840 -> 140191628062976
	140191628041376 -> 140191628062976
	140191627226144 [label="blocks.0.0.ghost2.cheap_operation.0.weight
 (8, 1, 3, 3)" fillcolor=lightblue]
	140191627226144 -> 140191628041376
	140191628041376 [label=AccumulateGrad]
	140191628063312 -> 140191628063792
	140191627226224 [label="blocks.0.0.ghost2.cheap_operation.1.weight
 (8)" fillcolor=lightblue]
	140191627226224 -> 140191628063312
	140191628063312 [label=AccumulateGrad]
	140191628063504 -> 140191628063792
	140191627226304 [label="blocks.0.0.ghost2.cheap_operation.1.bias
 (8)" fillcolor=lightblue]
	140191627226304 -> 140191628063504
	140191628063504 [label=AccumulateGrad]
	140191628064032 -> 140191628064224
	140191628064464 -> 140191628064608
	140191627226704 [label="blocks.1.0.ghost1.primary_conv.0.weight
 (24, 16, 1, 1)" fillcolor=lightblue]
	140191627226704 -> 140191628064464
	140191628064464 [label=AccumulateGrad]
	140191628064560 -> 140191628064752
	140191627226784 [label="blocks.1.0.ghost1.primary_conv.1.weight
 (24)" fillcolor=lightblue]
	140191627226784 -> 140191628064560
	140191628064560 [label=AccumulateGrad]
	140191628064800 -> 140191628064752
	140191627226864 [label="blocks.1.0.ghost1.primary_conv.1.bias
 (24)" fillcolor=lightblue]
	140191627226864 -> 140191628064800
	140191628064800 [label=AccumulateGrad]
	140191628065088 -> 140191628065040
	140191628065088 [label=ReluBackward0]
	140191628064416 -> 140191628065088
	140191628064416 [label=NativeBatchNormBackward0]
	140191628064272 -> 140191628064416
	140191628064272 [label=ConvolutionBackward0]
	140191628064992 -> 140191628064272
	140191628063024 -> 140191628064272
	140190562459872 [label="blocks.1.0.ghost1.cheap_operation.0.weight
 (24, 1, 3, 3)" fillcolor=lightblue]
	140190562459872 -> 140191628063024
	140191628063024 [label=AccumulateGrad]
	140191628063984 -> 140191628064416
	140190562459952 [label="blocks.1.0.ghost1.cheap_operation.1.weight
 (24)" fillcolor=lightblue]
	140190562459952 -> 140191628063984
	140191628063984 [label=AccumulateGrad]
	140191628064848 -> 140191628064416
	140190562460032 [label="blocks.1.0.ghost1.cheap_operation.1.bias
 (24)" fillcolor=lightblue]
	140190562460032 -> 140191628064848
	140191628064848 [label=AccumulateGrad]
	140191628065712 -> 140191628065856
	140190562460512 [label="blocks.1.0.conv_dw.weight
 (48, 1, 3, 3)" fillcolor=lightblue]
	140190562460512 -> 140191628065712
	140191628065712 [label=AccumulateGrad]
	140191628065808 -> 140191628066096
	140190562460592 [label="blocks.1.0.bn_dw.weight
 (48)" fillcolor=lightblue]
	140190562460592 -> 140191628065808
	140191628065808 [label=AccumulateGrad]
	140191628066000 -> 140191628066096
	140190562460672 [label="blocks.1.0.bn_dw.bias
 (48)" fillcolor=lightblue]
	140190562460672 -> 140191628066000
	140191628066000 [label=AccumulateGrad]
	140191628066048 -> 140191628066288
	140190562461072 [label="blocks.1.0.ghost2.primary_conv.0.weight
 (12, 48, 1, 1)" fillcolor=lightblue]
	140190562461072 -> 140191628066048
	140191628066048 [label=AccumulateGrad]
	140191628066480 -> 140191628066528
	140190562461152 [label="blocks.1.0.ghost2.primary_conv.1.weight
 (12)" fillcolor=lightblue]
	140190562461152 -> 140191628066480
	140191628066480 [label=AccumulateGrad]
	140191628066576 -> 140191628066528
	140190562461232 [label="blocks.1.0.ghost2.primary_conv.1.bias
 (12)" fillcolor=lightblue]
	140190562461232 -> 140191628066576
	140191628066576 [label=AccumulateGrad]
	140191628066720 -> 140191628079168
	140191628066720 [label=NativeBatchNormBackward0]
	140191628065232 -> 140191628066720
	140191628065232 [label=ConvolutionBackward0]
	140191628066528 -> 140191628065232
	140191628065568 -> 140191628065232
	140190562461632 [label="blocks.1.0.ghost2.cheap_operation.0.weight
 (12, 1, 3, 3)" fillcolor=lightblue]
	140190562461632 -> 140191628065568
	140191628065568 [label=AccumulateGrad]
	140191628066240 -> 140191628066720
	140190562461712 [label="blocks.1.0.ghost2.cheap_operation.1.weight
 (12)" fillcolor=lightblue]
	140190562461712 -> 140191628066240
	140191628066240 [label=AccumulateGrad]
	140191628066336 -> 140191628066720
	140190562461792 [label="blocks.1.0.ghost2.cheap_operation.1.bias
 (12)" fillcolor=lightblue]
	140190562461792 -> 140191628066336
	140191628066336 [label=AccumulateGrad]
	140191628079216 -> 140191628079312
	140191628079216 [label=NativeBatchNormBackward0]
	140191628065280 -> 140191628079216
	140191628065280 [label=ConvolutionBackward0]
	140191628064320 -> 140191628065280
	140191628064320 [label=NativeBatchNormBackward0]
	140191628040944 -> 140191628064320
	140191628040944 [label=ConvolutionBackward0]
	140191628064368 -> 140191628040944
	140191628040512 -> 140191628040944
	140190562462192 [label="blocks.1.0.shortcut.0.weight
 (16, 1, 3, 3)" fillcolor=lightblue]
	140190562462192 -> 140191628040512
	140191628040512 [label=AccumulateGrad]
	140191628042192 -> 140191628064320
	140190562462272 [label="blocks.1.0.shortcut.1.weight
 (16)" fillcolor=lightblue]
	140190562462272 -> 140191628042192
	140191628042192 [label=AccumulateGrad]
	140191628040704 -> 140191628064320
	140190562462352 [label="blocks.1.0.shortcut.1.bias
 (16)" fillcolor=lightblue]
	140190562462352 -> 140191628040704
	140191628040704 [label=AccumulateGrad]
	140191628065760 -> 140191628065280
	140190562462752 [label="blocks.1.0.shortcut.2.weight
 (24, 16, 1, 1)" fillcolor=lightblue]
	140190562462752 -> 140191628065760
	140191628065760 [label=AccumulateGrad]
	140191628065664 -> 140191628079216
	140190562462832 [label="blocks.1.0.shortcut.3.weight
 (24)" fillcolor=lightblue]
	140190562462832 -> 140191628065664
	140191628065664 [label=AccumulateGrad]
	140191628066768 -> 140191628079216
	140190562462912 [label="blocks.1.0.shortcut.3.bias
 (24)" fillcolor=lightblue]
	140190562462912 -> 140191628066768
	140191628066768 [label=AccumulateGrad]
	140191628079408 -> 140191628079696
	140190562463312 [label="blocks.2.0.ghost1.primary_conv.0.weight
 (36, 24, 1, 1)" fillcolor=lightblue]
	140190562463312 -> 140191628079408
	140191628079408 [label=AccumulateGrad]
	140191628079648 -> 140191628079744
	140190562463392 [label="blocks.2.0.ghost1.primary_conv.1.weight
 (36)" fillcolor=lightblue]
	140190562463392 -> 140191628079648
	140191628079648 [label=AccumulateGrad]
	140191628079792 -> 140191628079744
	140190562463472 [label="blocks.2.0.ghost1.primary_conv.1.bias
 (36)" fillcolor=lightblue]
	140190562463472 -> 140191628079792
	140191628079792 [label=AccumulateGrad]
	140191628080080 -> 140191628080032
	140191628080080 [label=ReluBackward0]
	140191628079456 -> 140191628080080
	140191628079456 [label=NativeBatchNormBackward0]
	140191628079264 -> 140191628079456
	140191628079264 [label=ConvolutionBackward0]
	140191628079984 -> 140191628079264
	140191628065520 -> 140191628079264
	140190562558176 [label="blocks.2.0.ghost1.cheap_operation.0.weight
 (36, 1, 3, 3)" fillcolor=lightblue]
	140190562558176 -> 140191628065520
	140191628065520 [label=AccumulateGrad]
	140191628079360 -> 140191628079456
	140190562558256 [label="blocks.2.0.ghost1.cheap_operation.1.weight
 (36)" fillcolor=lightblue]
	140190562558256 -> 140191628079360
	140191628079360 [label=AccumulateGrad]
	140191628079840 -> 140191628079456
	140190562558336 [label="blocks.2.0.ghost1.cheap_operation.1.bias
 (36)" fillcolor=lightblue]
	140190562558336 -> 140191628079840
	140191628079840 [label=AccumulateGrad]
	140191628080704 -> 140191628080944
	140190562558736 [label="blocks.2.0.ghost2.primary_conv.0.weight
 (12, 72, 1, 1)" fillcolor=lightblue]
	140190562558736 -> 140191628080704
	140191628080704 [label=AccumulateGrad]
	140191628080896 -> 140191628081184
	140190562558816 [label="blocks.2.0.ghost2.primary_conv.1.weight
 (12)" fillcolor=lightblue]
	140190562558816 -> 140191628080896
	140191628080896 [label=AccumulateGrad]
	140191628081088 -> 140191628081184
	140190562558896 [label="blocks.2.0.ghost2.primary_conv.1.bias
 (12)" fillcolor=lightblue]
	140190562558896 -> 140191628081088
	140191628081088 [label=AccumulateGrad]
	140191628081136 -> 140191628081328
	140191628081136 [label=NativeBatchNormBackward0]
	140191628080128 -> 140191628081136
	140191628080128 [label=ConvolutionBackward0]
	140191628081184 -> 140191628080128
	140191628079600 -> 140191628080128
	140190562559296 [label="blocks.2.0.ghost2.cheap_operation.0.weight
 (12, 1, 3, 3)" fillcolor=lightblue]
	140190562559296 -> 140191628079600
	140191628079600 [label=AccumulateGrad]
	140191628080656 -> 140191628081136
	140190562559376 [label="blocks.2.0.ghost2.cheap_operation.1.weight
 (12)" fillcolor=lightblue]
	140190562559376 -> 140191628080656
	140191628080656 [label=AccumulateGrad]
	140191628080848 -> 140191628081136
	140190562559456 [label="blocks.2.0.ghost2.cheap_operation.1.bias
 (12)" fillcolor=lightblue]
	140190562559456 -> 140191628080848
	140191628080848 [label=AccumulateGrad]
	140191628081280 -> 140191628081376
	140191628081712 -> 140191628081952
	140190562559856 [label="blocks.3.0.ghost1.primary_conv.0.weight
 (36, 24, 1, 1)" fillcolor=lightblue]
	140190562559856 -> 140191628081712
	140191628081712 [label=AccumulateGrad]
	140191628081904 -> 140191628082096
	140190562559936 [label="blocks.3.0.ghost1.primary_conv.1.weight
 (36)" fillcolor=lightblue]
	140190562559936 -> 140191628081904
	140191628081904 [label=AccumulateGrad]
	140191628082144 -> 140191628082096
	140190562560016 [label="blocks.3.0.ghost1.primary_conv.1.bias
 (36)" fillcolor=lightblue]
	140190562560016 -> 140191628082144
	140191628082144 [label=AccumulateGrad]
	140191628082432 -> 140191628082384
	140191628082432 [label=ReluBackward0]
	140191628064512 -> 140191628082432
	140191628064512 [label=NativeBatchNormBackward0]
	140191628081424 -> 140191628064512
	140191628081424 [label=ConvolutionBackward0]
	140191628082336 -> 140191628081424
	140191628080416 -> 140191628081424
	140190562560416 [label="blocks.3.0.ghost1.cheap_operation.0.weight
 (36, 1, 3, 3)" fillcolor=lightblue]
	140190562560416 -> 140191628080416
	140191628080416 [label=AccumulateGrad]
	140191628081664 -> 140191628064512
	140190562560496 [label="blocks.3.0.ghost1.cheap_operation.1.weight
 (36)" fillcolor=lightblue]
	140190562560496 -> 140191628081664
	140191628081664 [label=AccumulateGrad]
	140191628082192 -> 140191628064512
	140190562560576 [label="blocks.3.0.ghost1.cheap_operation.1.bias
 (36)" fillcolor=lightblue]
	140190562560576 -> 140191628082192
	140191628082192 [label=AccumulateGrad]
	140191628082960 -> 140191628107840
	140190562560976 [label="blocks.3.0.conv_dw.weight
 (72, 1, 5, 5)" fillcolor=lightblue]
	140190562560976 -> 140191628082960
	140191628082960 [label=AccumulateGrad]
	140191628107984 -> 140191628108080
	140190562561056 [label="blocks.3.0.bn_dw.weight
 (72)" fillcolor=lightblue]
	140190562561056 -> 140191628107984
	140191628107984 [label=AccumulateGrad]
	140191628083152 -> 140191628108080
	140190562561136 [label="blocks.3.0.bn_dw.bias
 (72)" fillcolor=lightblue]
	140190562561136 -> 140191628083152
	140191628083152 [label=AccumulateGrad]
	140191628108032 -> 140191628108320
	140191628108032 [label=HardsigmoidBackward0]
	140191628082480 -> 140191628108032
	140191628082480 [label=ConvolutionBackward0]
	140191628082720 -> 140191628082480
	140191628082720 [label=ReluBackward0]
	140191628080176 -> 140191628082720
	140191628080176 [label=ConvolutionBackward0]
	140191628081232 -> 140191628080176
	140191628081232 [label=MeanBackward1]
	140191628108080 -> 140191628081232
	140191628080464 -> 140191628080176
	140190562561536 [label="blocks.3.0.se.conv_reduce.weight
 (20, 72, 1, 1)" fillcolor=lightblue]
	140190562561536 -> 140191628080464
	140191628080464 [label=AccumulateGrad]
	140191628081856 -> 140191628080176
	140190562561616 [label="blocks.3.0.se.conv_reduce.bias
 (20)" fillcolor=lightblue]
	140190562561616 -> 140191628081856
	140191628081856 [label=AccumulateGrad]
	140191628082528 -> 140191628082480
	140190562561776 [label="blocks.3.0.se.conv_expand.weight
 (72, 20, 1, 1)" fillcolor=lightblue]
	140190562561776 -> 140191628082528
	140191628082528 [label=AccumulateGrad]
	140191628083104 -> 140191628082480
	140190562561856 [label="blocks.3.0.se.conv_expand.bias
 (72)" fillcolor=lightblue]
	140190562561856 -> 140191628083104
	140191628083104 [label=AccumulateGrad]
	140191628108272 -> 140191628108416
	140191382106176 [label="blocks.3.0.ghost2.primary_conv.0.weight
 (20, 72, 1, 1)" fillcolor=lightblue]
	140191382106176 -> 140191628108272
	140191628108272 [label=AccumulateGrad]
	140191628108512 -> 140191628108560
	140191382106256 [label="blocks.3.0.ghost2.primary_conv.1.weight
 (20)" fillcolor=lightblue]
	140191382106256 -> 140191628108512
	140191628108512 [label=AccumulateGrad]
	140191628108608 -> 140191628108560
	140191382106336 [label="blocks.3.0.ghost2.primary_conv.1.bias
 (20)" fillcolor=lightblue]
	140191382106336 -> 140191628108608
	140191628108608 [label=AccumulateGrad]
	140191628108752 -> 140191628108800
	140191628108752 [label=NativeBatchNormBackward0]
	140191628108224 -> 140191628108752
	140191628108224 [label=ConvolutionBackward0]
	140191628108560 -> 140191628108224
	140191628081472 -> 140191628108224
	140191382106736 [label="blocks.3.0.ghost2.cheap_operation.0.weight
 (20, 1, 3, 3)" fillcolor=lightblue]
	140191382106736 -> 140191628081472
	140191628081472 [label=AccumulateGrad]
	140191628108368 -> 140191628108752
	140191382106816 [label="blocks.3.0.ghost2.cheap_operation.1.weight
 (20)" fillcolor=lightblue]
	140191382106816 -> 140191628108368
	140191628108368 [label=AccumulateGrad]
	140191628108464 -> 140191628108752
	140191382106896 [label="blocks.3.0.ghost2.cheap_operation.1.bias
 (20)" fillcolor=lightblue]
	140191382106896 -> 140191628108464
	140191628108464 [label=AccumulateGrad]
	140191628108992 -> 140191628109088
	140191628108992 [label=NativeBatchNormBackward0]
	140191628082912 -> 140191628108992
	140191628082912 [label=ConvolutionBackward0]
	140191628040560 -> 140191628082912
	140191628040560 [label=NativeBatchNormBackward0]
	140191628040320 -> 140191628040560
	140191628040320 [label=ConvolutionBackward0]
	140191628081616 -> 140191628040320
	140191628040080 -> 140191628040320
	140191382107296 [label="blocks.3.0.shortcut.0.weight
 (24, 1, 5, 5)" fillcolor=lightblue]
	140191382107296 -> 140191628040080
	140191628040080 [label=AccumulateGrad]
	140191628040272 -> 140191628040560
	140191382107376 [label="blocks.3.0.shortcut.1.weight
 (24)" fillcolor=lightblue]
	140191382107376 -> 140191628040272
	140191628040272 [label=AccumulateGrad]
	140191628040800 -> 140191628040560
	140191382107456 [label="blocks.3.0.shortcut.1.bias
 (24)" fillcolor=lightblue]
	140191382107456 -> 140191628040800
	140191628040800 [label=AccumulateGrad]
	140191628040752 -> 140191628082912
	140191382107856 [label="blocks.3.0.shortcut.2.weight
 (40, 24, 1, 1)" fillcolor=lightblue]
	140191382107856 -> 140191628040752
	140191628040752 [label=AccumulateGrad]
	140191628082672 -> 140191628108992
	140191382107936 [label="blocks.3.0.shortcut.3.weight
 (40)" fillcolor=lightblue]
	140191382107936 -> 140191628082672
	140191628082672 [label=AccumulateGrad]
	140191628108848 -> 140191628108992
	140191382108016 [label="blocks.3.0.shortcut.3.bias
 (40)" fillcolor=lightblue]
	140191382108016 -> 140191628108848
	140191628108848 [label=AccumulateGrad]
	140191628109232 -> 140191628109568
	140191382108416 [label="blocks.4.0.ghost1.primary_conv.0.weight
 (60, 40, 1, 1)" fillcolor=lightblue]
	140191382108416 -> 140191628109232
	140191628109232 [label=AccumulateGrad]
	140191628109520 -> 140191628109616
	140191382108496 [label="blocks.4.0.ghost1.primary_conv.1.weight
 (60)" fillcolor=lightblue]
	140191382108496 -> 140191628109520
	140191628109520 [label=AccumulateGrad]
	140191628109664 -> 140191628109616
	140191382108576 [label="blocks.4.0.ghost1.primary_conv.1.bias
 (60)" fillcolor=lightblue]
	140191382108576 -> 140191628109664
	140191628109664 [label=AccumulateGrad]
	140191628109856 -> 140191628109808
	140191628109856 [label=ReluBackward0]
	140191628109328 -> 140191628109856
	140191628109328 [label=NativeBatchNormBackward0]
	140191628109040 -> 140191628109328
	140191628109040 [label=ConvolutionBackward0]
	140191628109760 -> 140191628109040
	140191628040176 -> 140191628109040
	140191382108976 [label="blocks.4.0.ghost1.cheap_operation.0.weight
 (60, 1, 3, 3)" fillcolor=lightblue]
	140191382108976 -> 140191628040176
	140191628040176 [label=AccumulateGrad]
	140191628109280 -> 140191628109328
	140191382109056 [label="blocks.4.0.ghost1.cheap_operation.1.weight
 (60)" fillcolor=lightblue]
	140191382109056 -> 140191628109280
	140191628109280 [label=AccumulateGrad]
	140191628109712 -> 140191628109328
	140191382109136 [label="blocks.4.0.ghost1.cheap_operation.1.bias
 (60)" fillcolor=lightblue]
	140191382109136 -> 140191628109712
	140191628109712 [label=AccumulateGrad]
	140191628110576 -> 140191628110720
	140191628110576 [label=HardsigmoidBackward0]
	140191628110000 -> 140191628110576
	140191628110000 [label=ConvolutionBackward0]
	140191628109472 -> 140191628110000
	140191628109472 [label=ReluBackward0]
	140191628040224 -> 140191628109472
	140191628040224 [label=ConvolutionBackward0]
	140191628040128 -> 140191628040224
	140191628040128 [label=MeanBackward1]
	140191628110480 -> 140191628040128
	140191628040464 -> 140191628040224
	140191382109536 [label="blocks.4.0.se.conv_reduce.weight
 (32, 120, 1, 1)" fillcolor=lightblue]
	140191382109536 -> 140191628040464
	140191628040464 [label=AccumulateGrad]
	140191628040032 -> 140191628040224
	140191382109616 [label="blocks.4.0.se.conv_reduce.bias
 (32)" fillcolor=lightblue]
	140191382109616 -> 140191628040032
	140191628040032 [label=AccumulateGrad]
	140191628110288 -> 140191628110000
	140191382109776 [label="blocks.4.0.se.conv_expand.weight
 (120, 32, 1, 1)" fillcolor=lightblue]
	140191382109776 -> 140191628110288
	140191628110288 [label=AccumulateGrad]
	140191628110336 -> 140191628110000
	140191382109856 [label="blocks.4.0.se.conv_expand.bias
 (120)" fillcolor=lightblue]
	140191382109856 -> 140191628110336
	140191628110336 [label=AccumulateGrad]
	140191628110816 -> 140191628110960
	140191382110016 [label="blocks.4.0.ghost2.primary_conv.0.weight
 (20, 120, 1, 1)" fillcolor=lightblue]
	140191382110016 -> 140191628110816
	140191628110816 [label=AccumulateGrad]
	140191628110912 -> 140191628111104
	140191382110096 [label="blocks.4.0.ghost2.primary_conv.1.weight
 (20)" fillcolor=lightblue]
	140191382110096 -> 140191628110912
	140191628110912 [label=AccumulateGrad]
	140191628111008 -> 140191628111104
	140191382224960 [label="blocks.4.0.ghost2.primary_conv.1.bias
 (20)" fillcolor=lightblue]
	140191382224960 -> 140191628111008
	140191628111008 [label=AccumulateGrad]
	140191628111056 -> 140191628111344
	140191628111056 [label=NativeBatchNormBackward0]
	140191628110048 -> 140191628111056
	140191628110048 [label=ConvolutionBackward0]
	140191628111104 -> 140191628110048
	140191628039936 -> 140191628110048
	140191382225360 [label="blocks.4.0.ghost2.cheap_operation.0.weight
 (20, 1, 3, 3)" fillcolor=lightblue]
	140191382225360 -> 140191628039936
	140191628039936 [label=AccumulateGrad]
	140191628110768 -> 140191628111056
	140191382225440 [label="blocks.4.0.ghost2.cheap_operation.1.weight
 (20)" fillcolor=lightblue]
	140191382225440 -> 140191628110768
	140191628110768 [label=AccumulateGrad]
	140191628110864 -> 140191628111056
	140191382225520 [label="blocks.4.0.ghost2.cheap_operation.1.bias
 (20)" fillcolor=lightblue]
	140191382225520 -> 140191628110864
	140191628110864 [label=AccumulateGrad]
	140191628111296 -> 140191628111392
	140191628111728 -> 140191628132416
	140191382225920 [label="blocks.5.0.ghost1.primary_conv.0.weight
 (120, 40, 1, 1)" fillcolor=lightblue]
	140191382225920 -> 140191628111728
	140191628111728 [label=AccumulateGrad]
	140191628132608 -> 140191628132560
	140191382226000 [label="blocks.5.0.ghost1.primary_conv.1.weight
 (120)" fillcolor=lightblue]
	140191382226000 -> 140191628132608
	140191628132608 [label=AccumulateGrad]
	140191628111824 -> 140191628132560
	140191382226080 [label="blocks.5.0.ghost1.primary_conv.1.bias
 (120)" fillcolor=lightblue]
	140191382226080 -> 140191628111824
	140191628111824 [label=AccumulateGrad]
	140191628132896 -> 140191628132848
	140191628132896 [label=ReluBackward0]
	140191628132656 -> 140191628132896
	140191628132656 [label=NativeBatchNormBackward0]
	140191628111440 -> 140191628132656
	140191628111440 [label=ConvolutionBackward0]
	140191628132800 -> 140191628111440
	140191628110528 -> 140191628111440
	140191382226480 [label="blocks.5.0.ghost1.cheap_operation.0.weight
 (120, 1, 3, 3)" fillcolor=lightblue]
	140191382226480 -> 140191628110528
	140191628110528 [label=AccumulateGrad]
	140191628111248 -> 140191628132656
	140191382226560 [label="blocks.5.0.ghost1.cheap_operation.1.weight
 (120)" fillcolor=lightblue]
	140191382226560 -> 140191628111248
	140191628111248 [label=AccumulateGrad]
	140191628111776 -> 140191628132656
	140191382226640 [label="blocks.5.0.ghost1.cheap_operation.1.bias
 (120)" fillcolor=lightblue]
	140191382226640 -> 140191628111776
	140191628111776 [label=AccumulateGrad]
	140191628133520 -> 140191628133664
	140191382227040 [label="blocks.5.0.conv_dw.weight
 (240, 1, 3, 3)" fillcolor=lightblue]
	140191382227040 -> 140191628133520
	140191628133520 [label=AccumulateGrad]
	140191628133616 -> 140191628133904
	140191382227120 [label="blocks.5.0.bn_dw.weight
 (240)" fillcolor=lightblue]
	140191382227120 -> 140191628133616
	140191628133616 [label=AccumulateGrad]
	140191628133808 -> 140191628133904
	140191382227200 [label="blocks.5.0.bn_dw.bias
 (240)" fillcolor=lightblue]
	140191382227200 -> 140191628133808
	140191628133808 [label=AccumulateGrad]
	140191628133856 -> 140191628134096
	140191382227600 [label="blocks.5.0.ghost2.primary_conv.0.weight
 (40, 240, 1, 1)" fillcolor=lightblue]
	140191382227600 -> 140191628133856
	140191628133856 [label=AccumulateGrad]
	140191628134288 -> 140191628134336
	140191382227680 [label="blocks.5.0.ghost2.primary_conv.1.weight
 (40)" fillcolor=lightblue]
	140191382227680 -> 140191628134288
	140191628134288 [label=AccumulateGrad]
	140191628134384 -> 140191628134336
	140191382227760 [label="blocks.5.0.ghost2.primary_conv.1.bias
 (40)" fillcolor=lightblue]
	140191382227760 -> 140191628134384
	140191628134384 [label=AccumulateGrad]
	140191628134528 -> 140191628134576
	140191628134528 [label=NativeBatchNormBackward0]
	140191628133040 -> 140191628134528
	140191628133040 [label=ConvolutionBackward0]
	140191628134336 -> 140191628133040
	140191628133376 -> 140191628133040
	140191382228160 [label="blocks.5.0.ghost2.cheap_operation.0.weight
 (40, 1, 3, 3)" fillcolor=lightblue]
	140191382228160 -> 140191628133376
	140191628133376 [label=AccumulateGrad]
	140191628134048 -> 140191628134528
	140191382228240 [label="blocks.5.0.ghost2.cheap_operation.1.weight
 (40)" fillcolor=lightblue]
	140191382228240 -> 140191628134048
	140191628134048 [label=AccumulateGrad]
	140191628134144 -> 140191628134528
	140191382228320 [label="blocks.5.0.ghost2.cheap_operation.1.bias
 (40)" fillcolor=lightblue]
	140191382228320 -> 140191628134144
	140191628134144 [label=AccumulateGrad]
	140191628134672 -> 140191628134768
	140191628134672 [label=NativeBatchNormBackward0]
	140191628133088 -> 140191628134672
	140191628133088 [label=ConvolutionBackward0]
	140191628111488 -> 140191628133088
	140191628111488 [label=NativeBatchNormBackward0]
	140191628039696 -> 140191628111488
	140191628039696 [label=ConvolutionBackward0]
	140191628111632 -> 140191628039696
	140191628039552 -> 140191628039696
	140191382228720 [label="blocks.5.0.shortcut.0.weight
 (40, 1, 3, 3)" fillcolor=lightblue]
	140191382228720 -> 140191628039552
	140191628039552 [label=AccumulateGrad]
	140191628041712 -> 140191628111488
	140191382228800 [label="blocks.5.0.shortcut.1.weight
 (40)" fillcolor=lightblue]
	140191382228800 -> 140191628041712
	140191628041712 [label=AccumulateGrad]
	140191628039792 -> 140191628111488
	140191382228880 [label="blocks.5.0.shortcut.1.bias
 (40)" fillcolor=lightblue]
	140191382228880 -> 140191628039792
	140191628039792 [label=AccumulateGrad]
	140191628111680 -> 140191628133088
	140190825345408 [label="blocks.5.0.shortcut.2.weight
 (80, 40, 1, 1)" fillcolor=lightblue]
	140190825345408 -> 140191628111680
	140191628111680 [label=AccumulateGrad]
	140191628133472 -> 140191628134672
	140190825345488 [label="blocks.5.0.shortcut.3.weight
 (80)" fillcolor=lightblue]
	140190825345488 -> 140191628133472
	140191628133472 [label=AccumulateGrad]
	140191628134624 -> 140191628134672
	140190825345568 [label="blocks.5.0.shortcut.3.bias
 (80)" fillcolor=lightblue]
	140190825345568 -> 140191628134624
	140191628134624 [label=AccumulateGrad]
	140191628134816 -> 140191628135152
	140190825345968 [label="blocks.6.0.ghost1.primary_conv.0.weight
 (100, 80, 1, 1)" fillcolor=lightblue]
	140190825345968 -> 140191628134816
	140191628134816 [label=AccumulateGrad]
	140191628135104 -> 140191628135296
	140190825346048 [label="blocks.6.0.ghost1.primary_conv.1.weight
 (100)" fillcolor=lightblue]
	140190825346048 -> 140191628135104
	140191628135104 [label=AccumulateGrad]
	140191628135344 -> 140191628135296
	140190825346128 [label="blocks.6.0.ghost1.primary_conv.1.bias
 (100)" fillcolor=lightblue]
	140190825346128 -> 140191628135344
	140191628135344 [label=AccumulateGrad]
	140191628135632 -> 140191628135584
	140191628135632 [label=ReluBackward0]
	140191628134912 -> 140191628135632
	140191628134912 [label=NativeBatchNormBackward0]
	140191628133328 -> 140191628134912
	140191628133328 [label=ConvolutionBackward0]
	140191628135536 -> 140191628133328
	140191628039984 -> 140191628133328
	140190825346528 [label="blocks.6.0.ghost1.cheap_operation.0.weight
 (100, 1, 3, 3)" fillcolor=lightblue]
	140190825346528 -> 140191628039984
	140191628039984 [label=AccumulateGrad]
	140191628133568 -> 140191628134912
	140190825346608 [label="blocks.6.0.ghost1.cheap_operation.1.weight
 (100)" fillcolor=lightblue]
	140190825346608 -> 140191628133568
	140191628133568 [label=AccumulateGrad]
	140191628135392 -> 140191628134912
	140190825346688 [label="blocks.6.0.ghost1.cheap_operation.1.bias
 (100)" fillcolor=lightblue]
	140190825346688 -> 140191628135392
	140191628135392 [label=AccumulateGrad]
	140191628136160 -> 140191628161184
	140190825347088 [label="blocks.6.0.ghost2.primary_conv.0.weight
 (40, 200, 1, 1)" fillcolor=lightblue]
	140190825347088 -> 140191628136160
	140191628136160 [label=AccumulateGrad]
	140191628136400 -> 140191628161280
	140190825347168 [label="blocks.6.0.ghost2.primary_conv.1.weight
 (40)" fillcolor=lightblue]
	140190825347168 -> 140191628136400
	140191628136400 [label=AccumulateGrad]
	140191628136352 -> 140191628161280
	140190825347248 [label="blocks.6.0.ghost2.primary_conv.1.bias
 (40)" fillcolor=lightblue]
	140190825347248 -> 140191628136352
	140191628136352 [label=AccumulateGrad]
	140191628161232 -> 140191628161520
	140191628161232 [label=NativeBatchNormBackward0]
	140191628135776 -> 140191628161232
	140191628135776 [label=ConvolutionBackward0]
	140191628161280 -> 140191628135776
	140191628135056 -> 140191628135776
	140190825347648 [label="blocks.6.0.ghost2.cheap_operation.0.weight
 (40, 1, 3, 3)" fillcolor=lightblue]
	140190825347648 -> 140191628135056
	140191628135056 [label=AccumulateGrad]
	140191628136112 -> 140191628161232
	140190825347728 [label="blocks.6.0.ghost2.cheap_operation.1.weight
 (40)" fillcolor=lightblue]
	140190825347728 -> 140191628136112
	140191628136112 [label=AccumulateGrad]
	140191628136304 -> 140191628161232
	140190825347808 [label="blocks.6.0.ghost2.cheap_operation.1.bias
 (40)" fillcolor=lightblue]
	140190825347808 -> 140191628136304
	140191628136304 [label=AccumulateGrad]
	140191628161472 -> 140191628161664
	140191628161712 -> 140191628161952
	140190825348208 [label="blocks.6.1.ghost1.primary_conv.0.weight
 (92, 80, 1, 1)" fillcolor=lightblue]
	140190825348208 -> 140191628161712
	140191628161712 [label=AccumulateGrad]
	140191628162048 -> 140191628162000
	140190825348288 [label="blocks.6.1.ghost1.primary_conv.1.weight
 (92)" fillcolor=lightblue]
	140190825348288 -> 140191628162048
	140191628162048 [label=AccumulateGrad]
	140191628162288 -> 140191628162000
	140190825348368 [label="blocks.6.1.ghost1.primary_conv.1.bias
 (92)" fillcolor=lightblue]
	140190825348368 -> 140191628162288
	140191628162288 [label=AccumulateGrad]
	140191628162432 -> 140191628162528
	140191628162432 [label=ReluBackward0]
	140191628161856 -> 140191628162432
	140191628161856 [label=NativeBatchNormBackward0]
	140191628161904 -> 140191628161856
	140191628161904 [label=ConvolutionBackward0]
	140191628162240 -> 140191628161904
	140191628134864 -> 140191628161904
	140190825348768 [label="blocks.6.1.ghost1.cheap_operation.0.weight
 (92, 1, 3, 3)" fillcolor=lightblue]
	140190825348768 -> 140191628134864
	140191628134864 [label=AccumulateGrad]
	140191628161424 -> 140191628161856
	140190825348848 [label="blocks.6.1.ghost1.cheap_operation.1.weight
 (92)" fillcolor=lightblue]
	140190825348848 -> 140191628161424
	140191628161424 [label=AccumulateGrad]
	140191628162192 -> 140191628161856
	140190825348928 [label="blocks.6.1.ghost1.cheap_operation.1.bias
 (92)" fillcolor=lightblue]
	140190825348928 -> 140191628162192
	140191628162192 [label=AccumulateGrad]
	140191628163056 -> 140191628163200
	140190825468208 [label="blocks.6.1.ghost2.primary_conv.0.weight
 (40, 184, 1, 1)" fillcolor=lightblue]
	140190825468208 -> 140191628163056
	140191628163056 [label=AccumulateGrad]
	140191628163296 -> 140191628163440
	140190825468288 [label="blocks.6.1.ghost2.primary_conv.1.weight
 (40)" fillcolor=lightblue]
	140190825468288 -> 140191628163296
	140191628163296 [label=AccumulateGrad]
	140191628163248 -> 140191628163440
	140190825468368 [label="blocks.6.1.ghost2.primary_conv.1.bias
 (40)" fillcolor=lightblue]
	140190825468368 -> 140191628163248
	140191628163248 [label=AccumulateGrad]
	140191628163536 -> 140191628163680
	140191628163536 [label=NativeBatchNormBackward0]
	140191628162480 -> 140191628163536
	140191628162480 [label=ConvolutionBackward0]
	140191628163440 -> 140191628162480
	140191628161808 -> 140191628162480
	140190825468768 [label="blocks.6.1.ghost2.cheap_operation.0.weight
 (40, 1, 3, 3)" fillcolor=lightblue]
	140190825468768 -> 140191628161808
	140191628161808 [label=AccumulateGrad]
	140191628163152 -> 140191628163536
	140190825468848 [label="blocks.6.1.ghost2.cheap_operation.1.weight
 (40)" fillcolor=lightblue]
	140190825468848 -> 140191628163152
	140191628163152 [label=AccumulateGrad]
	140191628163104 -> 140191628163536
	140190825468928 [label="blocks.6.1.ghost2.cheap_operation.1.bias
 (40)" fillcolor=lightblue]
	140190825468928 -> 140191628163104
	140191628163104 [label=AccumulateGrad]
	140191628163776 -> 140191628163728
	140191628164016 -> 140191628164208
	140190825469328 [label="blocks.6.2.ghost1.primary_conv.0.weight
 (92, 80, 1, 1)" fillcolor=lightblue]
	140190825469328 -> 140191628164016
	140191628164016 [label=AccumulateGrad]
	140191628164304 -> 140191628164400
	140190825469408 [label="blocks.6.2.ghost1.primary_conv.1.weight
 (92)" fillcolor=lightblue]
	140190825469408 -> 140191628164304
	140191628164304 [label=AccumulateGrad]
	140191628164448 -> 140191628164400
	140190825469488 [label="blocks.6.2.ghost1.primary_conv.1.bias
 (92)" fillcolor=lightblue]
	140190825469488 -> 140191628164448
	140191628164448 [label=AccumulateGrad]
	140191628164496 -> 140191628164688
	140191628164496 [label=ReluBackward0]
	140191628163968 -> 140191628164496
	140191628163968 [label=NativeBatchNormBackward0]
	140191628164160 -> 140191628163968
	140191628164160 [label=ConvolutionBackward0]
	140191628164544 -> 140191628164160
	140191628161760 -> 140191628164160
	140190825469888 [label="blocks.6.2.ghost1.cheap_operation.0.weight
 (92, 1, 3, 3)" fillcolor=lightblue]
	140190825469888 -> 140191628161760
	140191628161760 [label=AccumulateGrad]
	140191628163488 -> 140191628163968
	140190825469968 [label="blocks.6.2.ghost1.cheap_operation.1.weight
 (92)" fillcolor=lightblue]
	140190825469968 -> 140191628163488
	140191628163488 [label=AccumulateGrad]
	140191628164352 -> 140191628163968
	140190825470048 [label="blocks.6.2.ghost1.cheap_operation.1.bias
 (92)" fillcolor=lightblue]
	140190825470048 -> 140191628164352
	140191628164352 [label=AccumulateGrad]
	140191628185760 -> 140191628186000
	140190825470448 [label="blocks.6.2.ghost2.primary_conv.0.weight
 (40, 184, 1, 1)" fillcolor=lightblue]
	140190825470448 -> 140191628185760
	140191628185760 [label=AccumulateGrad]
	140191628186096 -> 140191628186144
	140190825470528 [label="blocks.6.2.ghost2.primary_conv.1.weight
 (40)" fillcolor=lightblue]
	140190825470528 -> 140191628186096
	140191628186096 [label=AccumulateGrad]
	140191628186192 -> 140191628186144
	140190825470608 [label="blocks.6.2.ghost2.primary_conv.1.bias
 (40)" fillcolor=lightblue]
	140190825470608 -> 140191628186192
	140191628186192 [label=AccumulateGrad]
	140191628186240 -> 140191628186288
	140191628186240 [label=NativeBatchNormBackward0]
	140191628185952 -> 140191628186240
	140191628185952 [label=ConvolutionBackward0]
	140191628186144 -> 140191628185952
	140191628164256 -> 140191628185952
	140190825471008 [label="blocks.6.2.ghost2.cheap_operation.0.weight
 (40, 1, 3, 3)" fillcolor=lightblue]
	140190825471008 -> 140191628164256
	140191628164256 [label=AccumulateGrad]
	140191628186048 -> 140191628186240
	140190825471088 [label="blocks.6.2.ghost2.cheap_operation.1.weight
 (40)" fillcolor=lightblue]
	140190825471088 -> 140191628186048
	140191628186048 [label=AccumulateGrad]
	140191628164784 -> 140191628186240
	140190825471168 [label="blocks.6.2.ghost2.cheap_operation.1.bias
 (40)" fillcolor=lightblue]
	140190825471168 -> 140191628164784
	140191628164784 [label=AccumulateGrad]
	140191628186480 -> 140191628186576
	140191628186768 -> 140191628187008
	140190825471568 [label="blocks.6.3.ghost1.primary_conv.0.weight
 (240, 80, 1, 1)" fillcolor=lightblue]
	140190825471568 -> 140191628186768
	140191628186768 [label=AccumulateGrad]
	140191628187200 -> 140191628187296
	140190825471648 [label="blocks.6.3.ghost1.primary_conv.1.weight
 (240)" fillcolor=lightblue]
	140190825471648 -> 140191628187200
	140191628187200 [label=AccumulateGrad]
	140191628187344 -> 140191628187296
	140190825471728 [label="blocks.6.3.ghost1.primary_conv.1.bias
 (240)" fillcolor=lightblue]
	140190825471728 -> 140191628187344
	140191628187344 [label=AccumulateGrad]
	140191628187392 -> 140191628187488
	140191628187392 [label=ReluBackward0]
	140191628186960 -> 140191628187392
	140191628186960 [label=NativeBatchNormBackward0]
	140191628186720 -> 140191628186960
	140191628186720 [label=ConvolutionBackward0]
	140191628187440 -> 140191628186720
	140191628163008 -> 140191628186720
	140190825611488 [label="blocks.6.3.ghost1.cheap_operation.0.weight
 (240, 1, 3, 3)" fillcolor=lightblue]
	140190825611488 -> 140191628163008
	140191628163008 [label=AccumulateGrad]
	140191628186336 -> 140191628186960
	140190825611568 [label="blocks.6.3.ghost1.cheap_operation.1.weight
 (240)" fillcolor=lightblue]
	140190825611568 -> 140191628186336
	140191628186336 [label=AccumulateGrad]
	140191628187248 -> 140191628186960
	140190825611648 [label="blocks.6.3.ghost1.cheap_operation.1.bias
 (240)" fillcolor=lightblue]
	140190825611648 -> 140191628187248
	140191628187248 [label=AccumulateGrad]
	140191628188016 -> 140191628188304
	140191628188016 [label=HardsigmoidBackward0]
	140191628187584 -> 140191628188016
	140191628187584 [label=ConvolutionBackward0]
	140191628187056 -> 140191628187584
	140191628187056 [label=ReluBackward0]
	140191628163920 -> 140191628187056
	140191628163920 [label=ConvolutionBackward0]
	140191628162912 -> 140191628163920
	140191628162912 [label=MeanBackward1]
	140191628188064 -> 140191628162912
	140191628164928 -> 140191628163920
	140190825612048 [label="blocks.6.3.se.conv_reduce.weight
 (120, 480, 1, 1)" fillcolor=lightblue]
	140190825612048 -> 140191628164928
	140191628164928 [label=AccumulateGrad]
	140191628164976 -> 140191628163920
	140190825612128 [label="blocks.6.3.se.conv_reduce.bias
 (120)" fillcolor=lightblue]
	140190825612128 -> 140191628164976
	140191628164976 [label=AccumulateGrad]
	140191628187968 -> 140191628187584
	140190825612288 [label="blocks.6.3.se.conv_expand.weight
 (480, 120, 1, 1)" fillcolor=lightblue]
	140190825612288 -> 140191628187968
	140191628187968 [label=AccumulateGrad]
	140191628187776 -> 140191628187584
	140190825612368 [label="blocks.6.3.se.conv_expand.bias
 (480)" fillcolor=lightblue]
	140190825612368 -> 140191628187776
	140191628187776 [label=AccumulateGrad]
	140191628188256 -> 140191628188496
	140190825612528 [label="blocks.6.3.ghost2.primary_conv.0.weight
 (56, 480, 1, 1)" fillcolor=lightblue]
	140190825612528 -> 140191628188256
	140191628188256 [label=AccumulateGrad]
	140191628188640 -> 140191628188592
	140190825612608 [label="blocks.6.3.ghost2.primary_conv.1.weight
 (56)" fillcolor=lightblue]
	140190825612608 -> 140191628188640
	140191628188640 [label=AccumulateGrad]
	140191628188688 -> 140191628188592
	140190825612688 [label="blocks.6.3.ghost2.primary_conv.1.bias
 (56)" fillcolor=lightblue]
	140190825612688 -> 140191628188688
	140191628188688 [label=AccumulateGrad]
	140191628188736 -> 140191628188784
	140191628188736 [label=NativeBatchNormBackward0]
	140191628186528 -> 140191628188736
	140191628186528 [label=ConvolutionBackward0]
	140191628188592 -> 140191628186528
	140191628162768 -> 140191628186528
	140190825613088 [label="blocks.6.3.ghost2.cheap_operation.0.weight
 (56, 1, 3, 3)" fillcolor=lightblue]
	140190825613088 -> 140191628162768
	140191628162768 [label=AccumulateGrad]
	140191628188448 -> 140191628188736
	140190825613168 [label="blocks.6.3.ghost2.cheap_operation.1.weight
 (56)" fillcolor=lightblue]
	140190825613168 -> 140191628188448
	140191628188448 [label=AccumulateGrad]
	140191628188544 -> 140191628188736
	140190825613248 [label="blocks.6.3.ghost2.cheap_operation.1.bias
 (56)" fillcolor=lightblue]
	140190825613248 -> 140191628188544
	140191628188544 [label=AccumulateGrad]
	140191628188976 -> 140191628189072
	140191628188976 [label=NativeBatchNormBackward0]
	140191628165024 -> 140191628188976
	140191628165024 [label=ConvolutionBackward0]
	140191628135968 -> 140191628165024
	140191628135968 [label=NativeBatchNormBackward0]
	140191628039456 -> 140191628135968
	140191628039456 [label=ConvolutionBackward0]
	140191628186816 -> 140191628039456
	140191628039024 -> 140191628039456
	140190825613648 [label="blocks.6.3.shortcut.0.weight
 (80, 1, 3, 3)" fillcolor=lightblue]
	140190825613648 -> 140191628039024
	140191628039024 [label=AccumulateGrad]
	140191628039744 -> 140191628135968
	140190825613728 [label="blocks.6.3.shortcut.1.weight
 (80)" fillcolor=lightblue]
	140190825613728 -> 140191628039744
	140191628039744 [label=AccumulateGrad]
	140191628039504 -> 140191628135968
	140190825613808 [label="blocks.6.3.shortcut.1.bias
 (80)" fillcolor=lightblue]
	140190825613808 -> 140191628039504
	140191628039504 [label=AccumulateGrad]
	140191628135824 -> 140191628165024
	140190825614208 [label="blocks.6.3.shortcut.2.weight
 (112, 80, 1, 1)" fillcolor=lightblue]
	140190825614208 -> 140191628135824
	140191628135824 [label=AccumulateGrad]
	140191628187728 -> 140191628188976
	140190825614288 [label="blocks.6.3.shortcut.3.weight
 (112)" fillcolor=lightblue]
	140190825614288 -> 140191628187728
	140191628187728 [label=AccumulateGrad]
	140191628188832 -> 140191628188976
	140190825614368 [label="blocks.6.3.shortcut.3.bias
 (112)" fillcolor=lightblue]
	140190825614368 -> 140191628188832
	140191628188832 [label=AccumulateGrad]
	140191628189216 -> 140191628210288
	140190825614768 [label="blocks.6.4.ghost1.primary_conv.0.weight
 (336, 112, 1, 1)" fillcolor=lightblue]
	140190825614768 -> 140191628189216
	140191628189216 [label=AccumulateGrad]
	140191628189552 -> 140191628210240
	140190825614848 [label="blocks.6.4.ghost1.primary_conv.1.weight
 (336)" fillcolor=lightblue]
	140190825614848 -> 140191628189552
	140191628189552 [label=AccumulateGrad]
	140191628189504 -> 140191628210240
	140190825614928 [label="blocks.6.4.ghost1.primary_conv.1.bias
 (336)" fillcolor=lightblue]
	140190825614928 -> 140191628189504
	140191628189504 [label=AccumulateGrad]
	140191628210480 -> 140191628210432
	140191628210480 [label=ReluBackward0]
	140191628210336 -> 140191628210480
	140191628210336 [label=NativeBatchNormBackward0]
	140191628189264 -> 140191628210336
	140191628189264 [label=ConvolutionBackward0]
	140191628210384 -> 140191628189264
	140191628136016 -> 140191628189264
	140190825721920 [label="blocks.6.4.ghost1.cheap_operation.0.weight
 (336, 1, 3, 3)" fillcolor=lightblue]
	140190825721920 -> 140191628136016
	140191628136016 [label=AccumulateGrad]
	140191628188208 -> 140191628210336
	140190825722000 [label="blocks.6.4.ghost1.cheap_operation.1.weight
 (336)" fillcolor=lightblue]
	140190825722000 -> 140191628188208
	140191628188208 [label=AccumulateGrad]
	140191628189456 -> 140191628210336
	140190825722080 [label="blocks.6.4.ghost1.cheap_operation.1.bias
 (336)" fillcolor=lightblue]
	140190825722080 -> 140191628189456
	140191628189456 [label=AccumulateGrad]
	140191628211008 -> 140191628211152
	140191628211008 [label=HardsigmoidBackward0]
	140191628210528 -> 140191628211008
	140191628210528 [label=ConvolutionBackward0]
	140191628210816 -> 140191628210528
	140191628210816 [label=ReluBackward0]
	140191628039264 -> 140191628210816
	140191628039264 [label=ConvolutionBackward0]
	140191628039312 -> 140191628039264
	140191628039312 [label=MeanBackward1]
	140191628210912 -> 140191628039312
	140191628038976 -> 140191628039264
	140190825722480 [label="blocks.6.4.se.conv_reduce.weight
 (168, 672, 1, 1)" fillcolor=lightblue]
	140190825722480 -> 140191628038976
	140191628038976 [label=AccumulateGrad]
	140191628039216 -> 140191628039264
	140190825722560 [label="blocks.6.4.se.conv_reduce.bias
 (168)" fillcolor=lightblue]
	140190825722560 -> 140191628039216
	140191628039216 [label=AccumulateGrad]
	140191628210864 -> 140191628210528
	140190825722720 [label="blocks.6.4.se.conv_expand.weight
 (672, 168, 1, 1)" fillcolor=lightblue]
	140190825722720 -> 140191628210864
	140191628210864 [label=AccumulateGrad]
	140191628189312 -> 140191628210528
	140190825722800 [label="blocks.6.4.se.conv_expand.bias
 (672)" fillcolor=lightblue]
	140190825722800 -> 140191628189312
	140191628189312 [label=AccumulateGrad]
	140191628211248 -> 140191628211392
	140190825722960 [label="blocks.6.4.ghost2.primary_conv.0.weight
 (56, 672, 1, 1)" fillcolor=lightblue]
	140190825722960 -> 140191628211248
	140191628211248 [label=AccumulateGrad]
	140191628211344 -> 140191628211632
	140190825723040 [label="blocks.6.4.ghost2.primary_conv.1.weight
 (56)" fillcolor=lightblue]
	140190825723040 -> 140191628211344
	140191628211344 [label=AccumulateGrad]
	140191628211536 -> 140191628211632
	140190825723120 [label="blocks.6.4.ghost2.primary_conv.1.bias
 (56)" fillcolor=lightblue]
	140190825723120 -> 140191628211536
	140191628211536 [label=AccumulateGrad]
	140191628211584 -> 140191628211872
	140191628211584 [label=NativeBatchNormBackward0]
	140191628189024 -> 140191628211584
	140191628189024 [label=ConvolutionBackward0]
	140191628211632 -> 140191628189024
	140191628210960 -> 140191628189024
	140190825723520 [label="blocks.6.4.ghost2.cheap_operation.0.weight
 (56, 1, 3, 3)" fillcolor=lightblue]
	140190825723520 -> 140191628210960
	140191628210960 [label=AccumulateGrad]
	140191628211200 -> 140191628211584
	140190825723600 [label="blocks.6.4.ghost2.cheap_operation.1.weight
 (56)" fillcolor=lightblue]
	140190825723600 -> 140191628211200
	140191628211200 [label=AccumulateGrad]
	140191628211296 -> 140191628211584
	140190825723680 [label="blocks.6.4.ghost2.cheap_operation.1.bias
 (56)" fillcolor=lightblue]
	140190825723680 -> 140191628211296
	140191628211296 [label=AccumulateGrad]
	140191628211824 -> 140191628212016
	140191628212352 -> 140191628212496
	140190825724080 [label="blocks.7.0.ghost1.primary_conv.0.weight
 (336, 112, 1, 1)" fillcolor=lightblue]
	140190825724080 -> 140191628212352
	140191628212352 [label=AccumulateGrad]
	140191628212448 -> 140191628212544
	140190825724160 [label="blocks.7.0.ghost1.primary_conv.1.weight
 (336)" fillcolor=lightblue]
	140190825724160 -> 140191628212448
	140191628212448 [label=AccumulateGrad]
	140191628212592 -> 140191628212544
	140190825724240 [label="blocks.7.0.ghost1.primary_conv.1.bias
 (336)" fillcolor=lightblue]
	140190825724240 -> 140191628212592
	140191628212592 [label=AccumulateGrad]
	140191628212880 -> 140191628212832
	140191628212880 [label=ReluBackward0]
	140191628212304 -> 140191628212880
	140191628212304 [label=NativeBatchNormBackward0]
	140191628212064 -> 140191628212304
	140191628212064 [label=ConvolutionBackward0]
	140191628212784 -> 140191628212064
	140191628210576 -> 140191628212064
	140190825724640 [label="blocks.7.0.ghost1.cheap_operation.0.weight
 (336, 1, 3, 3)" fillcolor=lightblue]
	140190825724640 -> 140191628210576
	140191628210576 [label=AccumulateGrad]
	140191628211776 -> 140191628212304
	140190825724720 [label="blocks.7.0.ghost1.cheap_operation.1.weight
 (336)" fillcolor=lightblue]
	140190825724720 -> 140191628211776
	140191628211776 [label=AccumulateGrad]
	140191628212640 -> 140191628212304
	140190825724800 [label="blocks.7.0.ghost1.cheap_operation.1.bias
 (336)" fillcolor=lightblue]
	140190825724800 -> 140191628212640
	140191628212640 [label=AccumulateGrad]
	140191628213600 -> 140191628213744
	140190825725280 [label="blocks.7.0.conv_dw.weight
 (672, 1, 5, 5)" fillcolor=lightblue]
	140190825725280 -> 140191628213600
	140191628213600 [label=AccumulateGrad]
	140191628213696 -> 140191628213888
	140190825725360 [label="blocks.7.0.bn_dw.weight
 (672)" fillcolor=lightblue]
	140190825725360 -> 140191628213696
	140191628213696 [label=AccumulateGrad]
	140191628213792 -> 140191628213888
	140190825725440 [label="blocks.7.0.bn_dw.bias
 (672)" fillcolor=lightblue]
	140190825725440 -> 140191628213792
	140191628213792 [label=AccumulateGrad]
	140191628213840 -> 140191628226624
	140191628213840 [label=HardsigmoidBackward0]
	140191628213024 -> 140191628213840
	140191628213024 [label=ConvolutionBackward0]
	140191628213360 -> 140191628213024
	140191628213360 [label=ReluBackward0]
	140191628212112 -> 140191628213360
	140191628212112 [label=ConvolutionBackward0]
	140191628038640 -> 140191628212112
	140191628038640 [label=MeanBackward1]
	140191628213888 -> 140191628038640
	140191628039072 -> 140191628212112
	140190825725840 [label="blocks.7.0.se.conv_reduce.weight
 (168, 672, 1, 1)" fillcolor=lightblue]
	140190825725840 -> 140191628039072
	140191628039072 [label=AccumulateGrad]
	140191628038832 -> 140191628212112
	140190827618368 [label="blocks.7.0.se.conv_reduce.bias
 (168)" fillcolor=lightblue]
	140190827618368 -> 140191628038832
	140191628038832 [label=AccumulateGrad]
	140191628213072 -> 140191628213024
	140190827618528 [label="blocks.7.0.se.conv_expand.weight
 (672, 168, 1, 1)" fillcolor=lightblue]
	140190827618528 -> 140191628213072
	140191628213072 [label=AccumulateGrad]
	140191628213648 -> 140191628213024
	140190827618608 [label="blocks.7.0.se.conv_expand.bias
 (672)" fillcolor=lightblue]
	140190827618608 -> 140191628213648
	140191628213648 [label=AccumulateGrad]
	140191628214128 -> 140191628226672
	140190827618768 [label="blocks.7.0.ghost2.primary_conv.0.weight
 (80, 672, 1, 1)" fillcolor=lightblue]
	140190827618768 -> 140191628214128
	140191628214128 [label=AccumulateGrad]
	140191628226864 -> 140191628226912
	140190827618848 [label="blocks.7.0.ghost2.primary_conv.1.weight
 (80)" fillcolor=lightblue]
	140190827618848 -> 140191628226864
	140191628226864 [label=AccumulateGrad]
	140191628226960 -> 140191628226912
	140190827618928 [label="blocks.7.0.ghost2.primary_conv.1.bias
 (80)" fillcolor=lightblue]
	140190827618928 -> 140191628226960
	140191628226960 [label=AccumulateGrad]
	140191628227104 -> 140191628227152
	140191628227104 [label=NativeBatchNormBackward0]
	140191628226720 -> 140191628227104
	140191628226720 [label=ConvolutionBackward0]
	140191628226912 -> 140191628226720
	140191628212400 -> 140191628226720
	140190827619328 [label="blocks.7.0.ghost2.cheap_operation.0.weight
 (80, 1, 3, 3)" fillcolor=lightblue]
	140190827619328 -> 140191628212400
	140191628212400 [label=AccumulateGrad]
	140191628213312 -> 140191628227104
	140190827619408 [label="blocks.7.0.ghost2.cheap_operation.1.weight
 (80)" fillcolor=lightblue]
	140190827619408 -> 140191628213312
	140191628213312 [label=AccumulateGrad]
	140191628214080 -> 140191628227104
	140190827619488 [label="blocks.7.0.ghost2.cheap_operation.1.bias
 (80)" fillcolor=lightblue]
	140190827619488 -> 140191628214080
	140191628214080 [label=AccumulateGrad]
	140191628227248 -> 140191628227344
	140191628227248 [label=NativeBatchNormBackward0]
	140191628214032 -> 140191628227248
	140191628214032 [label=ConvolutionBackward0]
	140191628038928 -> 140191628214032
	140191628038928 [label=NativeBatchNormBackward0]
	140191628038448 -> 140191628038928
	140191628038448 [label=ConvolutionBackward0]
	140191628212256 -> 140191628038448
	140191628038208 -> 140191628038448
	140190827619888 [label="blocks.7.0.shortcut.0.weight
 (112, 1, 5, 5)" fillcolor=lightblue]
	140190827619888 -> 140191628038208
	140191628038208 [label=AccumulateGrad]
	140191628038544 -> 140191628038928
	140190827619968 [label="blocks.7.0.shortcut.1.weight
 (112)" fillcolor=lightblue]
	140190827619968 -> 140191628038544
	140191628038544 [label=AccumulateGrad]
	140191628038496 -> 140191628038928
	140190827620048 [label="blocks.7.0.shortcut.1.bias
 (112)" fillcolor=lightblue]
	140190827620048 -> 140191628038496
	140191628038496 [label=AccumulateGrad]
	140191628038880 -> 140191628214032
	140190827620448 [label="blocks.7.0.shortcut.2.weight
 (160, 112, 1, 1)" fillcolor=lightblue]
	140190827620448 -> 140191628038880
	140191628038880 [label=AccumulateGrad]
	140191628213552 -> 140191628227248
	140190827620528 [label="blocks.7.0.shortcut.3.weight
 (160)" fillcolor=lightblue]
	140190827620528 -> 140191628213552
	140191628213552 [label=AccumulateGrad]
	140191628227200 -> 140191628227248
	140190827620608 [label="blocks.7.0.shortcut.3.bias
 (160)" fillcolor=lightblue]
	140190827620608 -> 140191628227200
	140191628227200 [label=AccumulateGrad]
	140191628227392 -> 140191628227728
	140190827621008 [label="blocks.8.0.ghost1.primary_conv.0.weight
 (480, 160, 1, 1)" fillcolor=lightblue]
	140190827621008 -> 140191628227392
	140191628227392 [label=AccumulateGrad]
	140191628227680 -> 140191628227872
	140190827621088 [label="blocks.8.0.ghost1.primary_conv.1.weight
 (480)" fillcolor=lightblue]
	140190827621088 -> 140191628227680
	140191628227680 [label=AccumulateGrad]
	140191628227920 -> 140191628227872
	140190827621168 [label="blocks.8.0.ghost1.primary_conv.1.bias
 (480)" fillcolor=lightblue]
	140190827621168 -> 140191628227920
	140191628227920 [label=AccumulateGrad]
	140191628228208 -> 140191628228160
	140191628228208 [label=ReluBackward0]
	140191628227488 -> 140191628228208
	140191628227488 [label=NativeBatchNormBackward0]
	140191628227296 -> 140191628227488
	140191628227296 [label=ConvolutionBackward0]
	140191628228112 -> 140191628227296
	140191628038256 -> 140191628227296
	140190827621568 [label="blocks.8.0.ghost1.cheap_operation.0.weight
 (480, 1, 3, 3)" fillcolor=lightblue]
	140190827621568 -> 140191628038256
	140191628038256 [label=AccumulateGrad]
	140191628227440 -> 140191628227488
	140190827621648 [label="blocks.8.0.ghost1.cheap_operation.1.weight
 (480)" fillcolor=lightblue]
	140190827621648 -> 140191628227440
	140191628227440 [label=AccumulateGrad]
	140191628227968 -> 140191628227488
	140190827621728 [label="blocks.8.0.ghost1.cheap_operation.1.bias
 (480)" fillcolor=lightblue]
	140190827621728 -> 140191628227968
	140191628227968 [label=AccumulateGrad]
	140191628228736 -> 140191628228976
	140190827622128 [label="blocks.8.0.ghost2.primary_conv.0.weight
 (80, 960, 1, 1)" fillcolor=lightblue]
	140190827622128 -> 140191628228736
	140191628228736 [label=AccumulateGrad]
	140191628228928 -> 140191628229216
	140190827622208 [label="blocks.8.0.ghost2.primary_conv.1.weight
 (80)" fillcolor=lightblue]
	140190827622208 -> 140191628228928
	140191628228928 [label=AccumulateGrad]
	140191628229120 -> 140191628229216
	140190827622288 [label="blocks.8.0.ghost2.primary_conv.1.bias
 (80)" fillcolor=lightblue]
	140190827622288 -> 140191628229120
	140191628229120 [label=AccumulateGrad]
	140191628229168 -> 140191628229456
	140191628229168 [label=NativeBatchNormBackward0]
	140191628228352 -> 140191628229168
	140191628228352 [label=ConvolutionBackward0]
	140191628229216 -> 140191628228352
	140191628227632 -> 140191628228352
	140190827733376 [label="blocks.8.0.ghost2.cheap_operation.0.weight
 (80, 1, 3, 3)" fillcolor=lightblue]
	140190827733376 -> 140191628227632
	140191628227632 [label=AccumulateGrad]
	140191628228688 -> 140191628229168
	140190827733456 [label="blocks.8.0.ghost2.cheap_operation.1.weight
 (80)" fillcolor=lightblue]
	140190827733456 -> 140191628228688
	140191628228688 [label=AccumulateGrad]
	140191628228880 -> 140191628229168
	140190827733536 [label="blocks.8.0.ghost2.cheap_operation.1.bias
 (80)" fillcolor=lightblue]
	140190827733536 -> 140191628228880
	140191628228880 [label=AccumulateGrad]
	140191628229408 -> 140191628229600
	140191628229648 -> 140191628229888
	140190827733936 [label="blocks.8.1.ghost1.primary_conv.0.weight
 (480, 160, 1, 1)" fillcolor=lightblue]
	140190827733936 -> 140191628229648
	140191628229648 [label=AccumulateGrad]
	140191628229984 -> 140191628229936
	140190827734016 [label="blocks.8.1.ghost1.primary_conv.1.weight
 (480)" fillcolor=lightblue]
	140190827734016 -> 140191628229984
	140191628229984 [label=AccumulateGrad]
	140191628230224 -> 140191628229936
	140190827734096 [label="blocks.8.1.ghost1.primary_conv.1.bias
 (480)" fillcolor=lightblue]
	140190827734096 -> 140191628230224
	140191628230224 [label=AccumulateGrad]
	140191628230368 -> 140191628230464
	140191628230368 [label=ReluBackward0]
	140191628229744 -> 140191628230368
	140191628229744 [label=NativeBatchNormBackward0]
	140191628229840 -> 140191628229744
	140191628229840 [label=ConvolutionBackward0]
	140191628230176 -> 140191628229840
	140191628228592 -> 140191628229840
	140190827734496 [label="blocks.8.1.ghost1.cheap_operation.0.weight
 (480, 1, 3, 3)" fillcolor=lightblue]
	140190827734496 -> 140191628228592
	140191628228592 [label=AccumulateGrad]
	140191628229360 -> 140191628229744
	140190827734576 [label="blocks.8.1.ghost1.cheap_operation.1.weight
 (480)" fillcolor=lightblue]
	140190827734576 -> 140191628229360
	140191628229360 [label=AccumulateGrad]
	140191628230128 -> 140191628229744
	140190827734656 [label="blocks.8.1.ghost1.cheap_operation.1.bias
 (480)" fillcolor=lightblue]
	140190827734656 -> 140191628230128
	140191628230128 [label=AccumulateGrad]
	140191635263888 -> 140191635263936
	140191635263888 [label=HardsigmoidBackward0]
	140191635263840 -> 140191635263888
	140191635263840 [label=ConvolutionBackward0]
	140191628229792 -> 140191635263840
	140191628229792 [label=ReluBackward0]
	140191628228400 -> 140191628229792
	140191628228400 [label=ConvolutionBackward0]
	140191628038592 -> 140191628228400
	140191628038592 [label=MeanBackward1]
	140191635263792 -> 140191628038592
	140191628038304 -> 140191628228400
	140190827735056 [label="blocks.8.1.se.conv_reduce.weight
 (240, 960, 1, 1)" fillcolor=lightblue]
	140190827735056 -> 140191628038304
	140191628038304 [label=AccumulateGrad]
	140191628038688 -> 140191628228400
	140190827735136 [label="blocks.8.1.se.conv_reduce.bias
 (240)" fillcolor=lightblue]
	140190827735136 -> 140191628038688
	140191628038688 [label=AccumulateGrad]
	140191628230608 -> 140191635263840
	140190827735296 [label="blocks.8.1.se.conv_expand.weight
 (960, 240, 1, 1)" fillcolor=lightblue]
	140190827735296 -> 140191628230608
	140191628230608 [label=AccumulateGrad]
	140191628230416 -> 140191635263840
	140190827735376 [label="blocks.8.1.se.conv_expand.bias
 (960)" fillcolor=lightblue]
	140190827735376 -> 140191628230416
	140191628230416 [label=AccumulateGrad]
	140191635264032 -> 140191635264272
	140190827735536 [label="blocks.8.1.ghost2.primary_conv.0.weight
 (80, 960, 1, 1)" fillcolor=lightblue]
	140190827735536 -> 140191635264032
	140191635264032 [label=AccumulateGrad]
	140191635264368 -> 140191635264512
	140190827735616 [label="blocks.8.1.ghost2.primary_conv.1.weight
 (80)" fillcolor=lightblue]
	140190827735616 -> 140191635264368
	140191635264368 [label=AccumulateGrad]
	140191635264320 -> 140191635264512
	140190827735696 [label="blocks.8.1.ghost2.primary_conv.1.bias
 (80)" fillcolor=lightblue]
	140190827735696 -> 140191635264320
	140191635264320 [label=AccumulateGrad]
	140191635264608 -> 140191635264752
	140191635264608 [label=NativeBatchNormBackward0]
	140191635263744 -> 140191635264608
	140191635263744 [label=ConvolutionBackward0]
	140191635264512 -> 140191635263744
	140191628228544 -> 140191635263744
	140190827736096 [label="blocks.8.1.ghost2.cheap_operation.0.weight
 (80, 1, 3, 3)" fillcolor=lightblue]
	140190827736096 -> 140191628228544
	140191628228544 [label=AccumulateGrad]
	140191635264128 -> 140191635264608
	140190827736176 [label="blocks.8.1.ghost2.cheap_operation.1.weight
 (80)" fillcolor=lightblue]
	140190827736176 -> 140191635264128
	140191635264128 [label=AccumulateGrad]
	140191635264080 -> 140191635264608
	140190827736256 [label="blocks.8.1.ghost2.cheap_operation.1.bias
 (80)" fillcolor=lightblue]
	140190827736256 -> 140191635264080
	140191635264080 [label=AccumulateGrad]
	140191635264848 -> 140191635264800
	140191635265088 -> 140191635265184
	140190827736656 [label="blocks.8.2.ghost1.primary_conv.0.weight
 (480, 160, 1, 1)" fillcolor=lightblue]
	140190827736656 -> 140191635265088
	140191635265088 [label=AccumulateGrad]
	140191635265280 -> 140191635265376
	140190827736736 [label="blocks.8.2.ghost1.primary_conv.1.weight
 (480)" fillcolor=lightblue]
	140190827736736 -> 140191635265280
	140191635265280 [label=AccumulateGrad]
	140191635265520 -> 140191635265376
	140190827736816 [label="blocks.8.2.ghost1.primary_conv.1.bias
 (480)" fillcolor=lightblue]
	140190827736816 -> 140191635265520
	140191635265520 [label=AccumulateGrad]
	140191635265568 -> 140191635265760
	140191635265568 [label=ReluBackward0]
	140191635265040 -> 140191635265568
	140191635265040 [label=NativeBatchNormBackward0]
	140191635265136 -> 140191635265040
	140191635265136 [label=ConvolutionBackward0]
	140191635265616 -> 140191635265136
	140191628229696 -> 140191635265136
	140190832660704 [label="blocks.8.2.ghost1.cheap_operation.0.weight
 (480, 1, 3, 3)" fillcolor=lightblue]
	140190832660704 -> 140191628229696
	140191628229696 [label=AccumulateGrad]
	140191635264560 -> 140191635265040
	140190832660784 [label="blocks.8.2.ghost1.cheap_operation.1.weight
 (480)" fillcolor=lightblue]
	140190832660784 -> 140191635264560
	140191635264560 [label=AccumulateGrad]
	140191635265328 -> 140191635265040
	140190832660864 [label="blocks.8.2.ghost1.cheap_operation.1.bias
 (480)" fillcolor=lightblue]
	140190832660864 -> 140191635265328
	140191635265328 [label=AccumulateGrad]
	140191635266288 -> 140191635266432
	140190832661264 [label="blocks.8.2.ghost2.primary_conv.0.weight
 (80, 960, 1, 1)" fillcolor=lightblue]
	140190832661264 -> 140191635266288
	140191635266288 [label=AccumulateGrad]
	140191635266576 -> 140191635266624
	140190832661344 [label="blocks.8.2.ghost2.primary_conv.1.weight
 (80)" fillcolor=lightblue]
	140190832661344 -> 140191635266576
	140191635266576 [label=AccumulateGrad]
	140191635266672 -> 140191635266624
	140190832661424 [label="blocks.8.2.ghost2.primary_conv.1.bias
 (80)" fillcolor=lightblue]
	140190832661424 -> 140191635266672
	140191635266672 [label=AccumulateGrad]
	140191635266816 -> 140191635266864
	140191635266816 [label=NativeBatchNormBackward0]
	140191635265856 -> 140191635266816
	140191635265856 [label=ConvolutionBackward0]
	140191635266624 -> 140191635265856
	140191635265232 -> 140191635265856
	140190832661824 [label="blocks.8.2.ghost2.cheap_operation.0.weight
 (80, 1, 3, 3)" fillcolor=lightblue]
	140190832661824 -> 140191635265232
	140191635265232 [label=AccumulateGrad]
	140191635266384 -> 140191635266816
	140190832661904 [label="blocks.8.2.ghost2.cheap_operation.1.weight
 (80)" fillcolor=lightblue]
	140190832661904 -> 140191635266384
	140191635266384 [label=AccumulateGrad]
	140191635266480 -> 140191635266816
	140190832661984 [label="blocks.8.2.ghost2.cheap_operation.1.bias
 (80)" fillcolor=lightblue]
	140190832661984 -> 140191635266480
	140191635266480 [label=AccumulateGrad]
	140191635266960 -> 140191635267056
	140191635267200 -> 140191635288176
	140190832662384 [label="blocks.8.3.ghost1.primary_conv.0.weight
 (480, 160, 1, 1)" fillcolor=lightblue]
	140190832662384 -> 140191635267200
	140191635267200 [label=AccumulateGrad]
	140191635267440 -> 140191635288128
	140190832662464 [label="blocks.8.3.ghost1.primary_conv.1.weight
 (480)" fillcolor=lightblue]
	140190832662464 -> 140191635267440
	140191635267440 [label=AccumulateGrad]
	140191635267392 -> 140191635288128
	140190832662544 [label="blocks.8.3.ghost1.primary_conv.1.bias
 (480)" fillcolor=lightblue]
	140190832662544 -> 140191635267392
	140191635267392 [label=AccumulateGrad]
	140191635288464 -> 140191635288416
	140191635288464 [label=ReluBackward0]
	140191635288224 -> 140191635288464
	140191635288224 [label=NativeBatchNormBackward0]
	140191635267248 -> 140191635288224
	140191635267248 [label=ConvolutionBackward0]
	140191635288368 -> 140191635267248
	140191635263984 -> 140191635267248
	140190832662944 [label="blocks.8.3.ghost1.cheap_operation.0.weight
 (480, 1, 3, 3)" fillcolor=lightblue]
	140190832662944 -> 140191635263984
	140191635263984 [label=AccumulateGrad]
	140191635266912 -> 140191635288224
	140190832663024 [label="blocks.8.3.ghost1.cheap_operation.1.weight
 (480)" fillcolor=lightblue]
	140190832663024 -> 140191635266912
	140191635266912 [label=AccumulateGrad]
	140191635267344 -> 140191635288224
	140190832663104 [label="blocks.8.3.ghost1.cheap_operation.1.bias
 (480)" fillcolor=lightblue]
	140190832663104 -> 140191635267344
	140191635267344 [label=AccumulateGrad]
	140191635289088 -> 140191635289136
	140191635289088 [label=HardsigmoidBackward0]
	140191635288608 -> 140191635289088
	140191635288608 [label=ConvolutionBackward0]
	140191635288896 -> 140191635288608
	140191635288896 [label=ReluBackward0]
	140191635264992 -> 140191635288896
	140191635264992 [label=ConvolutionBackward0]
	140191635266000 -> 140191635264992
	140191635266000 [label=MeanBackward1]
	140191635288992 -> 140191635266000
	140191635266240 -> 140191635264992
	140190832663504 [label="blocks.8.3.se.conv_reduce.weight
 (240, 960, 1, 1)" fillcolor=lightblue]
	140190832663504 -> 140191635266240
	140191635266240 [label=AccumulateGrad]
	140191628021712 -> 140191635264992
	140190832663584 [label="blocks.8.3.se.conv_reduce.bias
 (240)" fillcolor=lightblue]
	140190832663584 -> 140191628021712
	140191628021712 [label=AccumulateGrad]
	140191635288944 -> 140191635288608
	140190832663744 [label="blocks.8.3.se.conv_expand.weight
 (960, 240, 1, 1)" fillcolor=lightblue]
	140190832663744 -> 140191635288944
	140191635288944 [label=AccumulateGrad]
	140191635267296 -> 140191635288608
	140190832663824 [label="blocks.8.3.se.conv_expand.bias
 (960)" fillcolor=lightblue]
	140190832663824 -> 140191635267296
	140191635267296 [label=AccumulateGrad]
	140191635289232 -> 140191635289472
	140190832663984 [label="blocks.8.3.ghost2.primary_conv.0.weight
 (80, 960, 1, 1)" fillcolor=lightblue]
	140190832663984 -> 140191635289232
	140191635289232 [label=AccumulateGrad]
	140191635289424 -> 140191635289712
	140190832664064 [label="blocks.8.3.ghost2.primary_conv.1.weight
 (80)" fillcolor=lightblue]
	140190832664064 -> 140191635289424
	140191635289424 [label=AccumulateGrad]
	140191635289616 -> 140191635289712
	140190832664144 [label="blocks.8.3.ghost2.primary_conv.1.bias
 (80)" fillcolor=lightblue]
	140190832664144 -> 140191635289616
	140191635289616 [label=AccumulateGrad]
	140191635289664 -> 140191635289952
	140191635289664 [label=NativeBatchNormBackward0]
	140191635288656 -> 140191635289664
	140191635288656 [label=ConvolutionBackward0]
	140191635289712 -> 140191635288656
	140191635266048 -> 140191635288656
	140190832787520 [label="blocks.8.3.ghost2.cheap_operation.0.weight
 (80, 1, 3, 3)" fillcolor=lightblue]
	140190832787520 -> 140191635266048
	140191635266048 [label=AccumulateGrad]
	140191635289184 -> 140191635289664
	140190832787600 [label="blocks.8.3.ghost2.cheap_operation.1.weight
 (80)" fillcolor=lightblue]
	140190832787600 -> 140191635289184
	140191635289184 [label=AccumulateGrad]
	140191635289376 -> 140191635289664
	140190832787680 [label="blocks.8.3.ghost2.cheap_operation.1.bias
 (80)" fillcolor=lightblue]
	140190832787680 -> 140191635289376
	140191635289376 [label=AccumulateGrad]
	140191635289904 -> 140191635290096
	140191635290336 -> 140191635290480
	140190832788320 [label="blocks.9.0.conv.weight
 (960, 160, 1, 1)" fillcolor=lightblue]
	140190832788320 -> 140191635290336
	140191635290336 [label=AccumulateGrad]
	140191635290432 -> 140191635290624
	140190832788240 [label="blocks.9.0.bn1.weight
 (960)" fillcolor=lightblue]
	140190832788240 -> 140191635290432
	140191635290432 [label=AccumulateGrad]
	140191635290960 -> 140191635290624
	140190832788400 [label="blocks.9.0.bn1.bias
 (960)" fillcolor=lightblue]
	140190832788400 -> 140191635290960
	140191635290960 [label=AccumulateGrad]
	140191635291200 -> 140191635291152
	140190832788720 [label="conv_head.weight
 (1280, 960, 1, 1)" fillcolor=lightblue]
	140190832788720 -> 140191635291200
	140191635291200 [label=AccumulateGrad]
	140191635291392 -> 140191635291152
	140190832788800 [label="conv_head.bias
 (1280)" fillcolor=lightblue]
	140190832788800 -> 140191635291392
	140191635291392 [label=AccumulateGrad]
	140191635291536 -> 140191635291584
	140191635291536 [label=TBackward0]
	140191635290912 -> 140191635291536
	140190832788880 [label="classifier.weight
 (1000, 1280)" fillcolor=lightblue]
	140190832788880 -> 140191635290912
	140191635290912 [label=AccumulateGrad]
	140191635291584 -> 140191628029680
}
