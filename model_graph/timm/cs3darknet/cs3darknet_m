digraph {
	graph [size="125.25,125.25"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	140306436491120 [label="
 (1, 1000)" fillcolor=darkolivegreen1]
	140307328109056 [label=AddmmBackward0]
	140307328109680 -> 140307328109056
	140307327989376 [label="head.fc.bias
 (1000)" fillcolor=lightblue]
	140307327989376 -> 140307328109680
	140307328109680 [label=AccumulateGrad]
	140307328109872 -> 140307328109056
	140307328109872 [label=ReshapeAliasBackward0]
	140307328106608 -> 140307328109872
	140307328106608 [label=MeanBackward1]
	140307328106848 -> 140307328106608
	140307328106848 [label=SiluBackward0]
	140307328110112 -> 140307328106848
	140307328110112 [label=NativeBatchNormBackward0]
	140307328106896 -> 140307328110112
	140307328106896 [label=ConvolutionBackward0]
	140307328109968 -> 140307328106896
	140307328109968 [label=CatBackward0]
	140307328110256 -> 140307328109968
	140307328110256 [label=AddBackward0]
	140307328106800 -> 140307328110256
	140307328106800 [label=SiluBackward0]
	140307328110400 -> 140307328106800
	140307328110400 [label=NativeBatchNormBackward0]
	140307328110496 -> 140307328110400
	140307328110496 [label=ConvolutionBackward0]
	140306436559008 -> 140307328110496
	140306436559008 [label=SiluBackward0]
	140306436559152 -> 140306436559008
	140306436559152 [label=NativeBatchNormBackward0]
	140306436559248 -> 140306436559152
	140306436559248 [label=ConvolutionBackward0]
	140307328110016 -> 140306436559248
	140307328110016 [label=AddBackward0]
	140306436559536 -> 140307328110016
	140306436559536 [label=SiluBackward0]
	140306436559632 -> 140306436559536
	140306436559632 [label=NativeBatchNormBackward0]
	140306436559728 -> 140306436559632
	140306436559728 [label=ConvolutionBackward0]
	140306436559920 -> 140306436559728
	140306436559920 [label=SiluBackward0]
	140306436560064 -> 140306436559920
	140306436560064 [label=NativeBatchNormBackward0]
	140306436560160 -> 140306436560064
	140306436560160 [label=ConvolutionBackward0]
	140307328106656 -> 140306436560160
	140307328106656 [label=SplitBackward0]
	140306436560448 -> 140307328106656
	140306436560448 [label=SiluBackward0]
	140306436560544 -> 140306436560448
	140306436560544 [label=NativeBatchNormBackward0]
	140306436560640 -> 140306436560544
	140306436560640 [label=ConvolutionBackward0]
	140306436560832 -> 140306436560640
	140306436560832 [label=SiluBackward0]
	140306436560976 -> 140306436560832
	140306436560976 [label=NativeBatchNormBackward0]
	140306436561072 -> 140306436560976
	140306436561072 [label=ConvolutionBackward0]
	140306436561264 -> 140306436561072
	140306436561264 [label=SiluBackward0]
	140306436561408 -> 140306436561264
	140306436561408 [label=NativeBatchNormBackward0]
	140306436561504 -> 140306436561408
	140306436561504 [label=ConvolutionBackward0]
	140306436561696 -> 140306436561504
	140306436561696 [label=CatBackward0]
	140306436561840 -> 140306436561696
	140306436561840 [label=AddBackward0]
	140306436561984 -> 140306436561840
	140306436561984 [label=SiluBackward0]
	140306436562128 -> 140306436561984
	140306436562128 [label=NativeBatchNormBackward0]
	140306436562224 -> 140306436562128
	140306436562224 [label=ConvolutionBackward0]
	140306436562416 -> 140306436562224
	140306436562416 [label=SiluBackward0]
	140306436562560 -> 140306436562416
	140306436562560 [label=NativeBatchNormBackward0]
	140306436562656 -> 140306436562560
	140306436562656 [label=ConvolutionBackward0]
	140306436561936 -> 140306436562656
	140306436561936 [label=AddBackward0]
	140306436562896 -> 140306436561936
	140306436562896 [label=SiluBackward0]
	140306436628688 -> 140306436562896
	140306436628688 [label=NativeBatchNormBackward0]
	140306436628784 -> 140306436628688
	140306436628784 [label=ConvolutionBackward0]
	140306436628976 -> 140306436628784
	140306436628976 [label=SiluBackward0]
	140306436629120 -> 140306436628976
	140306436629120 [label=NativeBatchNormBackward0]
	140306436629216 -> 140306436629120
	140306436629216 [label=ConvolutionBackward0]
	140306436562752 -> 140306436629216
	140306436562752 [label=AddBackward0]
	140306436629504 -> 140306436562752
	140306436629504 [label=SiluBackward0]
	140306436629648 -> 140306436629504
	140306436629648 [label=NativeBatchNormBackward0]
	140306436629744 -> 140306436629648
	140306436629744 [label=ConvolutionBackward0]
	140306436629936 -> 140306436629744
	140306436629936 [label=SiluBackward0]
	140306436630080 -> 140306436629936
	140306436630080 [label=NativeBatchNormBackward0]
	140306436630176 -> 140306436630080
	140306436630176 [label=ConvolutionBackward0]
	140306436629456 -> 140306436630176
	140306436629456 [label=AddBackward0]
	140306436630464 -> 140306436629456
	140306436630464 [label=SiluBackward0]
	140306436630608 -> 140306436630464
	140306436630608 [label=NativeBatchNormBackward0]
	140306436630656 -> 140306436630608
	140306436630656 [label=ConvolutionBackward0]
	140306436630944 -> 140306436630656
	140306436630944 [label=SiluBackward0]
	140306436631088 -> 140306436630944
	140306436631088 [label=NativeBatchNormBackward0]
	140306436631136 -> 140306436631088
	140306436631136 [label=ConvolutionBackward0]
	140306436630416 -> 140306436631136
	140306436630416 [label=AddBackward0]
	140306436631520 -> 140306436630416
	140306436631520 [label=SiluBackward0]
	140306436631664 -> 140306436631520
	140306436631664 [label=NativeBatchNormBackward0]
	140306436631712 -> 140306436631664
	140306436631712 [label=ConvolutionBackward0]
	140306436632000 -> 140306436631712
	140306436632000 [label=SiluBackward0]
	140306436632144 -> 140306436632000
	140306436632144 [label=NativeBatchNormBackward0]
	140306436632192 -> 140306436632144
	140306436632192 [label=ConvolutionBackward0]
	140306436631472 -> 140306436632192
	140306436631472 [label=AddBackward0]
	140306436632528 -> 140306436631472
	140306436632528 [label=SiluBackward0]
	140306436599968 -> 140306436632528
	140306436599968 [label=NativeBatchNormBackward0]
	140306436600016 -> 140306436599968
	140306436600016 [label=ConvolutionBackward0]
	140306436600304 -> 140306436600016
	140306436600304 [label=SiluBackward0]
	140306436600448 -> 140306436600304
	140306436600448 [label=NativeBatchNormBackward0]
	140306436600496 -> 140306436600448
	140306436600496 [label=ConvolutionBackward0]
	140306436561792 -> 140306436600496
	140306436561792 [label=SplitBackward0]
	140306436600880 -> 140306436561792
	140306436600880 [label=SiluBackward0]
	140306436600928 -> 140306436600880
	140306436600928 [label=NativeBatchNormBackward0]
	140306436601072 -> 140306436600928
	140306436601072 [label=ConvolutionBackward0]
	140306436601360 -> 140306436601072
	140306436601360 [label=SiluBackward0]
	140306436601504 -> 140306436601360
	140306436601504 [label=NativeBatchNormBackward0]
	140306436601552 -> 140306436601504
	140306436601552 [label=ConvolutionBackward0]
	140306436601840 -> 140306436601552
	140306436601840 [label=SiluBackward0]
	140306436601984 -> 140306436601840
	140306436601984 [label=NativeBatchNormBackward0]
	140306436602032 -> 140306436601984
	140306436602032 [label=ConvolutionBackward0]
	140306436602320 -> 140306436602032
	140306436602320 [label=CatBackward0]
	140306436602464 -> 140306436602320
	140306436602464 [label=AddBackward0]
	140306436602608 -> 140306436602464
	140306436602608 [label=SiluBackward0]
	140306436602752 -> 140306436602608
	140306436602752 [label=NativeBatchNormBackward0]
	140306436602800 -> 140306436602752
	140306436602800 [label=ConvolutionBackward0]
	140306436603088 -> 140306436602800
	140306436603088 [label=SiluBackward0]
	140306436603232 -> 140306436603088
	140306436603232 [label=NativeBatchNormBackward0]
	140306436603280 -> 140306436603232
	140306436603280 [label=ConvolutionBackward0]
	140306436602560 -> 140306436603280
	140306436602560 [label=AddBackward0]
	140306436603664 -> 140306436602560
	140306436603664 [label=SiluBackward0]
	140306436603808 -> 140306436603664
	140306436603808 [label=NativeBatchNormBackward0]
	140306436603712 -> 140306436603808
	140306436603712 [label=ConvolutionBackward0]
	140306436645168 -> 140306436603712
	140306436645168 [label=SiluBackward0]
	140306436645312 -> 140306436645168
	140306436645312 [label=NativeBatchNormBackward0]
	140306436645360 -> 140306436645312
	140306436645360 [label=ConvolutionBackward0]
	140306436603616 -> 140306436645360
	140306436603616 [label=AddBackward0]
	140306436645744 -> 140306436603616
	140306436645744 [label=SiluBackward0]
	140306436645888 -> 140306436645744
	140306436645888 [label=NativeBatchNormBackward0]
	140306436645936 -> 140306436645888
	140306436645936 [label=ConvolutionBackward0]
	140306436646224 -> 140306436645936
	140306436646224 [label=SiluBackward0]
	140306436646368 -> 140306436646224
	140306436646368 [label=NativeBatchNormBackward0]
	140306436646416 -> 140306436646368
	140306436646416 [label=ConvolutionBackward0]
	140306436645696 -> 140306436646416
	140306436645696 [label=AddBackward0]
	140306436646800 -> 140306436645696
	140306436646800 [label=SiluBackward0]
	140306436646896 -> 140306436646800
	140306436646896 [label=NativeBatchNormBackward0]
	140306436646944 -> 140306436646896
	140306436646944 [label=ConvolutionBackward0]
	140306436647232 -> 140306436646944
	140306436647232 [label=SiluBackward0]
	140306436647376 -> 140306436647232
	140306436647376 [label=NativeBatchNormBackward0]
	140306436647424 -> 140306436647376
	140306436647424 [label=ConvolutionBackward0]
	140306436602416 -> 140306436647424
	140306436602416 [label=SplitBackward0]
	140306436647808 -> 140306436602416
	140306436647808 [label=SiluBackward0]
	140306436647856 -> 140306436647808
	140306436647856 [label=NativeBatchNormBackward0]
	140306436648000 -> 140306436647856
	140306436648000 [label=ConvolutionBackward0]
	140306436648288 -> 140306436648000
	140306436648288 [label=SiluBackward0]
	140306436648432 -> 140306436648288
	140306436648432 [label=NativeBatchNormBackward0]
	140306436648480 -> 140306436648432
	140306436648480 [label=ConvolutionBackward0]
	140306436648768 -> 140306436648480
	140306436648768 [label=SiluBackward0]
	140306436648912 -> 140306436648768
	140306436648912 [label=NativeBatchNormBackward0]
	140306454290544 -> 140306436648912
	140306454290544 [label=ConvolutionBackward0]
	140306454290784 -> 140306454290544
	140306454290784 [label=CatBackward0]
	140306454290928 -> 140306454290784
	140306454290928 [label=AddBackward0]
	140306454291072 -> 140306454290928
	140306454291072 [label=SiluBackward0]
	140306454291216 -> 140306454291072
	140306454291216 [label=NativeBatchNormBackward0]
	140306454291264 -> 140306454291216
	140306454291264 [label=ConvolutionBackward0]
	140306454291552 -> 140306454291264
	140306454291552 [label=SiluBackward0]
	140306454291696 -> 140306454291552
	140306454291696 [label=NativeBatchNormBackward0]
	140306454291744 -> 140306454291696
	140306454291744 [label=ConvolutionBackward0]
	140306454291024 -> 140306454291744
	140306454291024 [label=AddBackward0]
	140306454292128 -> 140306454291024
	140306454292128 [label=SiluBackward0]
	140306454292224 -> 140306454292128
	140306454292224 [label=NativeBatchNormBackward0]
	140306454292272 -> 140306454292224
	140306454292272 [label=ConvolutionBackward0]
	140306454292560 -> 140306454292272
	140306454292560 [label=SiluBackward0]
	140306454292704 -> 140306454292560
	140306454292704 [label=NativeBatchNormBackward0]
	140306454292752 -> 140306454292704
	140306454292752 [label=ConvolutionBackward0]
	140306454290880 -> 140306454292752
	140306454290880 [label=SplitBackward0]
	140306454293136 -> 140306454290880
	140306454293136 [label=SiluBackward0]
	140306454293184 -> 140306454293136
	140306454293184 [label=NativeBatchNormBackward0]
	140306454293328 -> 140306454293184
	140306454293328 [label=ConvolutionBackward0]
	140306454293616 -> 140306454293328
	140306454293616 [label=SiluBackward0]
	140306454293760 -> 140306454293616
	140306454293760 [label=NativeBatchNormBackward0]
	140306454293808 -> 140306454293760
	140306454293808 [label=ConvolutionBackward0]
	140306454294096 -> 140306454293808
	140306454294096 [label=SiluBackward0]
	140306454294240 -> 140306454294096
	140306454294240 [label=NativeBatchNormBackward0]
	140306454294288 -> 140306454294240
	140306454294288 [label=ConvolutionBackward0]
	140306454294480 -> 140306454294288
	140306454294480 [label=SiluBackward0]
	140306454302976 -> 140306454294480
	140306454302976 [label=NativeBatchNormBackward0]
	140306454303024 -> 140306454302976
	140306454303024 [label=ConvolutionBackward0]
	140306454303312 -> 140306454303024
	140307071423568 [label="stem.conv1.conv.weight
 (24, 3, 3, 3)" fillcolor=lightblue]
	140307071423568 -> 140306454303312
	140306454303312 [label=AccumulateGrad]
	140306454302880 -> 140306454302976
	140307071423648 [label="stem.conv1.bn.weight
 (24)" fillcolor=lightblue]
	140307071423648 -> 140306454302880
	140306454302880 [label=AccumulateGrad]
	140306454303120 -> 140306454302976
	140307071423728 [label="stem.conv1.bn.bias
 (24)" fillcolor=lightblue]
	140307071423728 -> 140306454303120
	140306454303120 [label=AccumulateGrad]
	140306454302832 -> 140306454294288
	140307071424208 [label="stem.conv2.conv.weight
 (48, 24, 3, 3)" fillcolor=lightblue]
	140307071424208 -> 140306454302832
	140306454302832 [label=AccumulateGrad]
	140306454294144 -> 140306454294240
	140307071424288 [label="stem.conv2.bn.weight
 (48)" fillcolor=lightblue]
	140307071424288 -> 140306454294144
	140306454294144 [label=AccumulateGrad]
	140306454294384 -> 140306454294240
	140307071424368 [label="stem.conv2.bn.bias
 (48)" fillcolor=lightblue]
	140307071424368 -> 140306454294384
	140306454294384 [label=AccumulateGrad]
	140306454294048 -> 140306454293808
	140307071425888 [label="stages.0.conv_down.conv.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140307071425888 -> 140306454294048
	140306454294048 [label=AccumulateGrad]
	140306454293664 -> 140306454293760
	140307071425968 [label="stages.0.conv_down.bn.weight
 (48)" fillcolor=lightblue]
	140307071425968 -> 140306454293664
	140306454293664 [label=AccumulateGrad]
	140306454293904 -> 140306454293760
	140307071426048 [label="stages.0.conv_down.bn.bias
 (48)" fillcolor=lightblue]
	140307071426048 -> 140306454293904
	140306454293904 [label=AccumulateGrad]
	140306454293568 -> 140306454293328
	140307071426368 [label="stages.0.conv_exp.conv.weight
 (96, 48, 1, 1)" fillcolor=lightblue]
	140307071426368 -> 140306454293568
	140306454293568 [label=AccumulateGrad]
	140306454293280 -> 140306454293184
	140307071426448 [label="stages.0.conv_exp.bn.weight
 (96)" fillcolor=lightblue]
	140307071426448 -> 140306454293280
	140306454293280 [label=AccumulateGrad]
	140306454293424 -> 140306454293184
	140306427023424 [label="stages.0.conv_exp.bn.bias
 (96)" fillcolor=lightblue]
	140306427023424 -> 140306454293424
	140306454293424 [label=AccumulateGrad]
	140306454293040 -> 140306454292752
	140306427023904 [label="stages.0.blocks.0.conv1.conv.weight
 (48, 48, 1, 1)" fillcolor=lightblue]
	140306427023904 -> 140306454293040
	140306454293040 [label=AccumulateGrad]
	140306454292608 -> 140306454292704
	140306427023984 [label="stages.0.blocks.0.conv1.bn.weight
 (48)" fillcolor=lightblue]
	140306427023984 -> 140306454292608
	140306454292608 [label=AccumulateGrad]
	140306454292848 -> 140306454292704
	140306427024064 [label="stages.0.blocks.0.conv1.bn.bias
 (48)" fillcolor=lightblue]
	140306427024064 -> 140306454292848
	140306454292848 [label=AccumulateGrad]
	140306454292512 -> 140306454292272
	140306427024544 [label="stages.0.blocks.0.conv2.conv.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140306427024544 -> 140306454292512
	140306454292512 [label=AccumulateGrad]
	140306454291936 -> 140306454292224
	140306427024624 [label="stages.0.blocks.0.conv2.bn.weight
 (48)" fillcolor=lightblue]
	140306427024624 -> 140306454291936
	140306454291936 [label=AccumulateGrad]
	140306454292368 -> 140306454292224
	140306427024704 [label="stages.0.blocks.0.conv2.bn.bias
 (48)" fillcolor=lightblue]
	140306427024704 -> 140306454292368
	140306454292368 [label=AccumulateGrad]
	140306454290880 -> 140306454291024
	140306454292032 -> 140306454291744
	140306427025024 [label="stages.0.blocks.1.conv1.conv.weight
 (48, 48, 1, 1)" fillcolor=lightblue]
	140306427025024 -> 140306454292032
	140306454292032 [label=AccumulateGrad]
	140306454291600 -> 140306454291696
	140306427025104 [label="stages.0.blocks.1.conv1.bn.weight
 (48)" fillcolor=lightblue]
	140306427025104 -> 140306454291600
	140306454291600 [label=AccumulateGrad]
	140306454291840 -> 140306454291696
	140306427025184 [label="stages.0.blocks.1.conv1.bn.bias
 (48)" fillcolor=lightblue]
	140306427025184 -> 140306454291840
	140306454291840 [label=AccumulateGrad]
	140306454291504 -> 140306454291264
	140306427025664 [label="stages.0.blocks.1.conv2.conv.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140306427025664 -> 140306454291504
	140306454291504 [label=AccumulateGrad]
	140306454291120 -> 140306454291216
	140306427025744 [label="stages.0.blocks.1.conv2.bn.weight
 (48)" fillcolor=lightblue]
	140306427025744 -> 140306454291120
	140306454291120 [label=AccumulateGrad]
	140306454291360 -> 140306454291216
	140306427025824 [label="stages.0.blocks.1.conv2.bn.bias
 (48)" fillcolor=lightblue]
	140306427025824 -> 140306454291360
	140306454291360 [label=AccumulateGrad]
	140306454291024 -> 140306454290928
	140306454290880 -> 140306454290784
	140306454290736 -> 140306454290544
	140306427026064 [label="stages.0.conv_transition.conv.weight
 (96, 96, 1, 1)" fillcolor=lightblue]
	140306427026064 -> 140306454290736
	140306454290736 [label=AccumulateGrad]
	140306454290496 -> 140306436648912
	140306427026144 [label="stages.0.conv_transition.bn.weight
 (96)" fillcolor=lightblue]
	140306427026144 -> 140306454290496
	140306454290496 [label=AccumulateGrad]
	140306454290592 -> 140306436648912
	140306427026224 [label="stages.0.conv_transition.bn.bias
 (96)" fillcolor=lightblue]
	140306427026224 -> 140306454290592
	140306454290592 [label=AccumulateGrad]
	140306436648720 -> 140306436648480
	140306427026704 [label="stages.1.conv_down.conv.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	140306427026704 -> 140306436648720
	140306436648720 [label=AccumulateGrad]
	140306436648336 -> 140306436648432
	140306427026784 [label="stages.1.conv_down.bn.weight
 (96)" fillcolor=lightblue]
	140306427026784 -> 140306436648336
	140306436648336 [label=AccumulateGrad]
	140306436648576 -> 140306436648432
	140306427026864 [label="stages.1.conv_down.bn.bias
 (96)" fillcolor=lightblue]
	140306427026864 -> 140306436648576
	140306436648576 [label=AccumulateGrad]
	140306436648240 -> 140306436648000
	140306427027184 [label="stages.1.conv_exp.conv.weight
 (192, 96, 1, 1)" fillcolor=lightblue]
	140306427027184 -> 140306436648240
	140306436648240 [label=AccumulateGrad]
	140306436647952 -> 140306436647856
	140306427027264 [label="stages.1.conv_exp.bn.weight
 (192)" fillcolor=lightblue]
	140306427027264 -> 140306436647952
	140306436647952 [label=AccumulateGrad]
	140306436648096 -> 140306436647856
	140306427027344 [label="stages.1.conv_exp.bn.bias
 (192)" fillcolor=lightblue]
	140306427027344 -> 140306436648096
	140306436648096 [label=AccumulateGrad]
	140306436647712 -> 140306436647424
	140306427179472 [label="stages.1.blocks.0.conv1.conv.weight
 (96, 96, 1, 1)" fillcolor=lightblue]
	140306427179472 -> 140306436647712
	140306436647712 [label=AccumulateGrad]
	140306436647280 -> 140306436647376
	140306427179552 [label="stages.1.blocks.0.conv1.bn.weight
 (96)" fillcolor=lightblue]
	140306427179552 -> 140306436647280
	140306436647280 [label=AccumulateGrad]
	140306436647520 -> 140306436647376
	140306427179632 [label="stages.1.blocks.0.conv1.bn.bias
 (96)" fillcolor=lightblue]
	140306427179632 -> 140306436647520
	140306436647520 [label=AccumulateGrad]
	140306436647184 -> 140306436646944
	140306427180112 [label="stages.1.blocks.0.conv2.conv.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	140306427180112 -> 140306436647184
	140306436647184 [label=AccumulateGrad]
	140306436646608 -> 140306436646896
	140306427180192 [label="stages.1.blocks.0.conv2.bn.weight
 (96)" fillcolor=lightblue]
	140306427180192 -> 140306436646608
	140306436646608 [label=AccumulateGrad]
	140306436647040 -> 140306436646896
	140306427180272 [label="stages.1.blocks.0.conv2.bn.bias
 (96)" fillcolor=lightblue]
	140306427180272 -> 140306436647040
	140306436647040 [label=AccumulateGrad]
	140306436602416 -> 140306436645696
	140306436646704 -> 140306436646416
	140306427180592 [label="stages.1.blocks.1.conv1.conv.weight
 (96, 96, 1, 1)" fillcolor=lightblue]
	140306427180592 -> 140306436646704
	140306436646704 [label=AccumulateGrad]
	140306436646272 -> 140306436646368
	140306427180672 [label="stages.1.blocks.1.conv1.bn.weight
 (96)" fillcolor=lightblue]
	140306427180672 -> 140306436646272
	140306436646272 [label=AccumulateGrad]
	140306436646512 -> 140306436646368
	140306427180752 [label="stages.1.blocks.1.conv1.bn.bias
 (96)" fillcolor=lightblue]
	140306427180752 -> 140306436646512
	140306436646512 [label=AccumulateGrad]
	140306436646176 -> 140306436645936
	140306427181232 [label="stages.1.blocks.1.conv2.conv.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	140306427181232 -> 140306436646176
	140306436646176 [label=AccumulateGrad]
	140306436645792 -> 140306436645888
	140306427181312 [label="stages.1.blocks.1.conv2.bn.weight
 (96)" fillcolor=lightblue]
	140306427181312 -> 140306436645792
	140306436645792 [label=AccumulateGrad]
	140306436646032 -> 140306436645888
	140306427181392 [label="stages.1.blocks.1.conv2.bn.bias
 (96)" fillcolor=lightblue]
	140306427181392 -> 140306436646032
	140306436646032 [label=AccumulateGrad]
	140306436645696 -> 140306436603616
	140306436645648 -> 140306436645360
	140306427181712 [label="stages.1.blocks.2.conv1.conv.weight
 (96, 96, 1, 1)" fillcolor=lightblue]
	140306427181712 -> 140306436645648
	140306436645648 [label=AccumulateGrad]
	140306436645216 -> 140306436645312
	140306427181792 [label="stages.1.blocks.2.conv1.bn.weight
 (96)" fillcolor=lightblue]
	140306427181792 -> 140306436645216
	140306436645216 [label=AccumulateGrad]
	140306436645456 -> 140306436645312
	140306427181872 [label="stages.1.blocks.2.conv1.bn.bias
 (96)" fillcolor=lightblue]
	140306427181872 -> 140306436645456
	140306436645456 [label=AccumulateGrad]
	140306436645120 -> 140306436603712
	140306427182352 [label="stages.1.blocks.2.conv2.conv.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	140306427182352 -> 140306436645120
	140306436645120 [label=AccumulateGrad]
	140306436644928 -> 140306436603808
	140306427182432 [label="stages.1.blocks.2.conv2.bn.weight
 (96)" fillcolor=lightblue]
	140306427182432 -> 140306436644928
	140306436644928 [label=AccumulateGrad]
	140306436644976 -> 140306436603808
	140306427182512 [label="stages.1.blocks.2.conv2.bn.bias
 (96)" fillcolor=lightblue]
	140306427182512 -> 140306436644976
	140306436644976 [label=AccumulateGrad]
	140306436603616 -> 140306436602560
	140306436603568 -> 140306436603280
	140306427182832 [label="stages.1.blocks.3.conv1.conv.weight
 (96, 96, 1, 1)" fillcolor=lightblue]
	140306427182832 -> 140306436603568
	140306436603568 [label=AccumulateGrad]
	140306436603136 -> 140306436603232
	140306427182912 [label="stages.1.blocks.3.conv1.bn.weight
 (96)" fillcolor=lightblue]
	140306427182912 -> 140306436603136
	140306436603136 [label=AccumulateGrad]
	140306436603376 -> 140306436603232
	140306427182992 [label="stages.1.blocks.3.conv1.bn.bias
 (96)" fillcolor=lightblue]
	140306427182992 -> 140306436603376
	140306436603376 [label=AccumulateGrad]
	140306436603040 -> 140306436602800
	140306428039632 [label="stages.1.blocks.3.conv2.conv.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	140306428039632 -> 140306436603040
	140306436603040 [label=AccumulateGrad]
	140306436602656 -> 140306436602752
	140306428039712 [label="stages.1.blocks.3.conv2.bn.weight
 (96)" fillcolor=lightblue]
	140306428039712 -> 140306436602656
	140306436602656 [label=AccumulateGrad]
	140306436602896 -> 140306436602752
	140306428039792 [label="stages.1.blocks.3.conv2.bn.bias
 (96)" fillcolor=lightblue]
	140306428039792 -> 140306436602896
	140306436602896 [label=AccumulateGrad]
	140306436602560 -> 140306436602464
	140306436602416 -> 140306436602320
	140306436602272 -> 140306436602032
	140306428040032 [label="stages.1.conv_transition.conv.weight
 (192, 192, 1, 1)" fillcolor=lightblue]
	140306428040032 -> 140306436602272
	140306436602272 [label=AccumulateGrad]
	140306436601888 -> 140306436601984
	140306428040112 [label="stages.1.conv_transition.bn.weight
 (192)" fillcolor=lightblue]
	140306428040112 -> 140306436601888
	140306436601888 [label=AccumulateGrad]
	140306436602128 -> 140306436601984
	140306428040192 [label="stages.1.conv_transition.bn.bias
 (192)" fillcolor=lightblue]
	140306428040192 -> 140306436602128
	140306436602128 [label=AccumulateGrad]
	140306436601792 -> 140306436601552
	140306428040672 [label="stages.2.conv_down.conv.weight
 (192, 192, 3, 3)" fillcolor=lightblue]
	140306428040672 -> 140306436601792
	140306436601792 [label=AccumulateGrad]
	140306436601408 -> 140306436601504
	140306428040752 [label="stages.2.conv_down.bn.weight
 (192)" fillcolor=lightblue]
	140306428040752 -> 140306436601408
	140306436601408 [label=AccumulateGrad]
	140306436601648 -> 140306436601504
	140306428040832 [label="stages.2.conv_down.bn.bias
 (192)" fillcolor=lightblue]
	140306428040832 -> 140306436601648
	140306436601648 [label=AccumulateGrad]
	140306436601312 -> 140306436601072
	140306428041152 [label="stages.2.conv_exp.conv.weight
 (384, 192, 1, 1)" fillcolor=lightblue]
	140306428041152 -> 140306436601312
	140306436601312 [label=AccumulateGrad]
	140306436601024 -> 140306436600928
	140306428041232 [label="stages.2.conv_exp.bn.weight
 (384)" fillcolor=lightblue]
	140306428041232 -> 140306436601024
	140306436601024 [label=AccumulateGrad]
	140306436601168 -> 140306436600928
	140306428041312 [label="stages.2.conv_exp.bn.bias
 (384)" fillcolor=lightblue]
	140306428041312 -> 140306436601168
	140306436601168 [label=AccumulateGrad]
	140306436600784 -> 140306436600496
	140306428041792 [label="stages.2.blocks.0.conv1.conv.weight
 (192, 192, 1, 1)" fillcolor=lightblue]
	140306428041792 -> 140306436600784
	140306436600784 [label=AccumulateGrad]
	140306436600352 -> 140306436600448
	140306428041872 [label="stages.2.blocks.0.conv1.bn.weight
 (192)" fillcolor=lightblue]
	140306428041872 -> 140306436600352
	140306436600352 [label=AccumulateGrad]
	140306436600592 -> 140306436600448
	140306428041952 [label="stages.2.blocks.0.conv1.bn.bias
 (192)" fillcolor=lightblue]
	140306428041952 -> 140306436600592
	140306436600592 [label=AccumulateGrad]
	140306436600256 -> 140306436600016
	140306428042432 [label="stages.2.blocks.0.conv2.conv.weight
 (192, 192, 3, 3)" fillcolor=lightblue]
	140306428042432 -> 140306436600256
	140306436600256 [label=AccumulateGrad]
	140306436599872 -> 140306436599968
	140306428042512 [label="stages.2.blocks.0.conv2.bn.weight
 (192)" fillcolor=lightblue]
	140306428042512 -> 140306436599872
	140306436599872 [label=AccumulateGrad]
	140306436600112 -> 140306436599968
	140306428042592 [label="stages.2.blocks.0.conv2.bn.bias
 (192)" fillcolor=lightblue]
	140306428042592 -> 140306436600112
	140306436600112 [label=AccumulateGrad]
	140306436561792 -> 140306436631472
	140306436632480 -> 140306436632192
	140306428042912 [label="stages.2.blocks.1.conv1.conv.weight
 (192, 192, 1, 1)" fillcolor=lightblue]
	140306428042912 -> 140306436632480
	140306436632480 [label=AccumulateGrad]
	140306436632048 -> 140306436632144
	140306428042992 [label="stages.2.blocks.1.conv1.bn.weight
 (192)" fillcolor=lightblue]
	140306428042992 -> 140306436632048
	140306436632048 [label=AccumulateGrad]
	140306436632288 -> 140306436632144
	140306428043072 [label="stages.2.blocks.1.conv1.bn.bias
 (192)" fillcolor=lightblue]
	140306428043072 -> 140306436632288
	140306436632288 [label=AccumulateGrad]
	140306436631952 -> 140306436631712
	140306433143168 [label="stages.2.blocks.1.conv2.conv.weight
 (192, 192, 3, 3)" fillcolor=lightblue]
	140306433143168 -> 140306436631952
	140306436631952 [label=AccumulateGrad]
	140306436631568 -> 140306436631664
	140306433143248 [label="stages.2.blocks.1.conv2.bn.weight
 (192)" fillcolor=lightblue]
	140306433143248 -> 140306436631568
	140306436631568 [label=AccumulateGrad]
	140306436631808 -> 140306436631664
	140306433143328 [label="stages.2.blocks.1.conv2.bn.bias
 (192)" fillcolor=lightblue]
	140306433143328 -> 140306436631808
	140306436631808 [label=AccumulateGrad]
	140306436631472 -> 140306436630416
	140306436631424 -> 140306436631136
	140306433143648 [label="stages.2.blocks.2.conv1.conv.weight
 (192, 192, 1, 1)" fillcolor=lightblue]
	140306433143648 -> 140306436631424
	140306436631424 [label=AccumulateGrad]
	140306436630992 -> 140306436631088
	140306433143728 [label="stages.2.blocks.2.conv1.bn.weight
 (192)" fillcolor=lightblue]
	140306433143728 -> 140306436630992
	140306436630992 [label=AccumulateGrad]
	140306436631232 -> 140306436631088
	140306433143808 [label="stages.2.blocks.2.conv1.bn.bias
 (192)" fillcolor=lightblue]
	140306433143808 -> 140306436631232
	140306436631232 [label=AccumulateGrad]
	140306436630896 -> 140306436630656
	140306433144288 [label="stages.2.blocks.2.conv2.conv.weight
 (192, 192, 3, 3)" fillcolor=lightblue]
	140306433144288 -> 140306436630896
	140306436630896 [label=AccumulateGrad]
	140306436630512 -> 140306436630608
	140306433144368 [label="stages.2.blocks.2.conv2.bn.weight
 (192)" fillcolor=lightblue]
	140306433144368 -> 140306436630512
	140306436630512 [label=AccumulateGrad]
	140306436630752 -> 140306436630608
	140306433144448 [label="stages.2.blocks.2.conv2.bn.bias
 (192)" fillcolor=lightblue]
	140306433144448 -> 140306436630752
	140306436630752 [label=AccumulateGrad]
	140306436630416 -> 140306436629456
	140306436630368 -> 140306436630176
	140306433144768 [label="stages.2.blocks.3.conv1.conv.weight
 (192, 192, 1, 1)" fillcolor=lightblue]
	140306433144768 -> 140306436630368
	140306436630368 [label=AccumulateGrad]
	140306436630128 -> 140306436630080
	140306433144848 [label="stages.2.blocks.3.conv1.bn.weight
 (192)" fillcolor=lightblue]
	140306433144848 -> 140306436630128
	140306436630128 [label=AccumulateGrad]
	140306436629984 -> 140306436630080
	140306433144928 [label="stages.2.blocks.3.conv1.bn.bias
 (192)" fillcolor=lightblue]
	140306433144928 -> 140306436629984
	140306436629984 [label=AccumulateGrad]
	140306436629888 -> 140306436629744
	140306433145408 [label="stages.2.blocks.3.conv2.conv.weight
 (192, 192, 3, 3)" fillcolor=lightblue]
	140306433145408 -> 140306436629888
	140306436629888 [label=AccumulateGrad]
	140306436629696 -> 140306436629648
	140306433145488 [label="stages.2.blocks.3.conv2.bn.weight
 (192)" fillcolor=lightblue]
	140306433145488 -> 140306436629696
	140306436629696 [label=AccumulateGrad]
	140306436629552 -> 140306436629648
	140306433145568 [label="stages.2.blocks.3.conv2.bn.bias
 (192)" fillcolor=lightblue]
	140306433145568 -> 140306436629552
	140306436629552 [label=AccumulateGrad]
	140306436629456 -> 140306436562752
	140306436629408 -> 140306436629216
	140306433145888 [label="stages.2.blocks.4.conv1.conv.weight
 (192, 192, 1, 1)" fillcolor=lightblue]
	140306433145888 -> 140306436629408
	140306436629408 [label=AccumulateGrad]
	140306436629168 -> 140306436629120
	140306433145968 [label="stages.2.blocks.4.conv1.bn.weight
 (192)" fillcolor=lightblue]
	140306433145968 -> 140306436629168
	140306436629168 [label=AccumulateGrad]
	140306436629024 -> 140306436629120
	140306433146048 [label="stages.2.blocks.4.conv1.bn.bias
 (192)" fillcolor=lightblue]
	140306433146048 -> 140306436629024
	140306436629024 [label=AccumulateGrad]
	140306436628928 -> 140306436628784
	140306433146528 [label="stages.2.blocks.4.conv2.conv.weight
 (192, 192, 3, 3)" fillcolor=lightblue]
	140306433146528 -> 140306436628928
	140306436628928 [label=AccumulateGrad]
	140306436628736 -> 140306436628688
	140306433146608 [label="stages.2.blocks.4.conv2.bn.weight
 (192)" fillcolor=lightblue]
	140306433146608 -> 140306436628736
	140306436628736 [label=AccumulateGrad]
	140306436628592 -> 140306436628688
	140306433146688 [label="stages.2.blocks.4.conv2.bn.bias
 (192)" fillcolor=lightblue]
	140306433146688 -> 140306436628592
	140306436628592 [label=AccumulateGrad]
	140306436562752 -> 140306436561936
	140306436562848 -> 140306436562656
	140306433306848 [label="stages.2.blocks.5.conv1.conv.weight
 (192, 192, 1, 1)" fillcolor=lightblue]
	140306433306848 -> 140306436562848
	140306436562848 [label=AccumulateGrad]
	140306436562608 -> 140306436562560
	140306433306928 [label="stages.2.blocks.5.conv1.bn.weight
 (192)" fillcolor=lightblue]
	140306433306928 -> 140306436562608
	140306436562608 [label=AccumulateGrad]
	140306436562464 -> 140306436562560
	140306433307008 [label="stages.2.blocks.5.conv1.bn.bias
 (192)" fillcolor=lightblue]
	140306433307008 -> 140306436562464
	140306436562464 [label=AccumulateGrad]
	140306436562368 -> 140306436562224
	140306433307488 [label="stages.2.blocks.5.conv2.conv.weight
 (192, 192, 3, 3)" fillcolor=lightblue]
	140306433307488 -> 140306436562368
	140306436562368 [label=AccumulateGrad]
	140306436562176 -> 140306436562128
	140306433307568 [label="stages.2.blocks.5.conv2.bn.weight
 (192)" fillcolor=lightblue]
	140306433307568 -> 140306436562176
	140306436562176 [label=AccumulateGrad]
	140306436562032 -> 140306436562128
	140306433307648 [label="stages.2.blocks.5.conv2.bn.bias
 (192)" fillcolor=lightblue]
	140306433307648 -> 140306436562032
	140306436562032 [label=AccumulateGrad]
	140306436561936 -> 140306436561840
	140306436561792 -> 140306436561696
	140306436561648 -> 140306436561504
	140306433307888 [label="stages.2.conv_transition.conv.weight
 (384, 384, 1, 1)" fillcolor=lightblue]
	140306433307888 -> 140306436561648
	140306436561648 [label=AccumulateGrad]
	140306436561456 -> 140306436561408
	140306433307968 [label="stages.2.conv_transition.bn.weight
 (384)" fillcolor=lightblue]
	140306433307968 -> 140306436561456
	140306436561456 [label=AccumulateGrad]
	140306436561312 -> 140306436561408
	140306433308048 [label="stages.2.conv_transition.bn.bias
 (384)" fillcolor=lightblue]
	140306433308048 -> 140306436561312
	140306436561312 [label=AccumulateGrad]
	140306436561216 -> 140306436561072
	140306433308528 [label="stages.3.conv_down.conv.weight
 (384, 384, 3, 3)" fillcolor=lightblue]
	140306433308528 -> 140306436561216
	140306436561216 [label=AccumulateGrad]
	140306436561024 -> 140306436560976
	140306433308608 [label="stages.3.conv_down.bn.weight
 (384)" fillcolor=lightblue]
	140306433308608 -> 140306436561024
	140306436561024 [label=AccumulateGrad]
	140306436560880 -> 140306436560976
	140306433308688 [label="stages.3.conv_down.bn.bias
 (384)" fillcolor=lightblue]
	140306433308688 -> 140306436560880
	140306436560880 [label=AccumulateGrad]
	140306436560784 -> 140306436560640
	140306433309008 [label="stages.3.conv_exp.conv.weight
 (768, 384, 1, 1)" fillcolor=lightblue]
	140306433309008 -> 140306436560784
	140306436560784 [label=AccumulateGrad]
	140306436560592 -> 140306436560544
	140306433309088 [label="stages.3.conv_exp.bn.weight
 (768)" fillcolor=lightblue]
	140306433309088 -> 140306436560592
	140306436560592 [label=AccumulateGrad]
	140306436560256 -> 140306436560544
	140306433309168 [label="stages.3.conv_exp.bn.bias
 (768)" fillcolor=lightblue]
	140306433309168 -> 140306436560256
	140306436560256 [label=AccumulateGrad]
	140306436560352 -> 140306436560160
	140306433309648 [label="stages.3.blocks.0.conv1.conv.weight
 (384, 384, 1, 1)" fillcolor=lightblue]
	140306433309648 -> 140306436560352
	140306436560352 [label=AccumulateGrad]
	140306436560112 -> 140306436560064
	140306433309728 [label="stages.3.blocks.0.conv1.bn.weight
 (384)" fillcolor=lightblue]
	140306433309728 -> 140306436560112
	140306436560112 [label=AccumulateGrad]
	140306436559968 -> 140306436560064
	140306433309808 [label="stages.3.blocks.0.conv1.bn.bias
 (384)" fillcolor=lightblue]
	140306433309808 -> 140306436559968
	140306436559968 [label=AccumulateGrad]
	140306436559872 -> 140306436559728
	140306433310288 [label="stages.3.blocks.0.conv2.conv.weight
 (384, 384, 3, 3)" fillcolor=lightblue]
	140306433310288 -> 140306436559872
	140306436559872 [label=AccumulateGrad]
	140306436559680 -> 140306436559632
	140306433310368 [label="stages.3.blocks.0.conv2.bn.weight
 (384)" fillcolor=lightblue]
	140306433310368 -> 140306436559680
	140306436559680 [label=AccumulateGrad]
	140306436559344 -> 140306436559632
	140306433310448 [label="stages.3.blocks.0.conv2.bn.bias
 (384)" fillcolor=lightblue]
	140306433310448 -> 140306436559344
	140306436559344 [label=AccumulateGrad]
	140307328106656 -> 140307328110016
	140306436559440 -> 140306436559248
	140307327987856 [label="stages.3.blocks.1.conv1.conv.weight
 (384, 384, 1, 1)" fillcolor=lightblue]
	140307327987856 -> 140306436559440
	140306436559440 [label=AccumulateGrad]
	140306436559200 -> 140306436559152
	140307327987936 [label="stages.3.blocks.1.conv1.bn.weight
 (384)" fillcolor=lightblue]
	140307327987936 -> 140306436559200
	140306436559200 [label=AccumulateGrad]
	140306436559056 -> 140306436559152
	140307327988016 [label="stages.3.blocks.1.conv1.bn.bias
 (384)" fillcolor=lightblue]
	140307327988016 -> 140306436559056
	140306436559056 [label=AccumulateGrad]
	140306436558960 -> 140307328110496
	140307327988496 [label="stages.3.blocks.1.conv2.conv.weight
 (384, 384, 3, 3)" fillcolor=lightblue]
	140307327988496 -> 140306436558960
	140306436558960 [label=AccumulateGrad]
	140307328110448 -> 140307328110400
	140307327988576 [label="stages.3.blocks.1.conv2.bn.weight
 (384)" fillcolor=lightblue]
	140307327988576 -> 140307328110448
	140307328110448 [label=AccumulateGrad]
	140307328110304 -> 140307328110400
	140307327988656 [label="stages.3.blocks.1.conv2.bn.bias
 (384)" fillcolor=lightblue]
	140307327988656 -> 140307328110304
	140307328110304 [label=AccumulateGrad]
	140307328110016 -> 140307328110256
	140307328106656 -> 140307328109968
	140307328109728 -> 140307328106896
	140307327988896 [label="stages.3.conv_transition.conv.weight
 (768, 768, 1, 1)" fillcolor=lightblue]
	140307327988896 -> 140307328109728
	140307328109728 [label=AccumulateGrad]
	140307328110064 -> 140307328110112
	140307327988976 [label="stages.3.conv_transition.bn.weight
 (768)" fillcolor=lightblue]
	140307327988976 -> 140307328110064
	140307328110064 [label=AccumulateGrad]
	140307328107904 -> 140307328110112
	140307327989056 [label="stages.3.conv_transition.bn.bias
 (768)" fillcolor=lightblue]
	140307327989056 -> 140307328107904
	140307328107904 [label=AccumulateGrad]
	140307328109776 -> 140307328109056
	140307328109776 [label=TBackward0]
	140307328109584 -> 140307328109776
	140307327989296 [label="head.fc.weight
 (1000, 768)" fillcolor=lightblue]
	140307327989296 -> 140307328109584
	140307328109584 [label=AccumulateGrad]
	140307328109056 -> 140306436491120
}
